{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Novel Writer - Complete Pipeline & Training\n\nThis notebook runs **everything** end-to-end on Google Colab:\n\n1. Clone repo & install dependencies\n2. Upload your novels (or use built-in sample data)\n3. Run the full data processing pipeline\n4. Fine-tune your chosen model\n5. Generate sample text\n6. Download your trained model\n\n### Supported Models\n\n| Model | Params | Best For | Min GPU | Free Tier? |\n|-------|--------|----------|---------|------------|\n| Qwen3-4B | 4B | Chinese (lightweight) | 8GB (T4) | Yes |\n| Qwen3-8B | 8B | Chinese | 12GB (T4) | Yes |\n| Llama 3.1 8B | 8B | English | 12GB (T4) | Yes |\n| Gemma 2 9B | 9B | English | 12GB (T4) | Yes |\n| Mistral Nemo 12B | 12B | English creative writing | 12GB (T4) | Yes |\n| Phi-4 14B | 14B | English (reasoning + writing) | 24GB (L4/A10) | Kaggle 2xT4 |\n| Qwen3-14B | 14B | Chinese + English | 24GB (L4/A10) | Kaggle 2xT4 |\n| Qwen3-32B | 32B | Chinese + English (best quality) | 40GB (A100) | No |\n\n**Requirements:** Google Colab with T4 GPU (free tier works for 4B-12B models)\n\n---\n\n### How to use\n1. Pick your model in **Cell 1** below\n2. **Runtime > Run all**\n3. When prompted, upload your novel files (or skip to use sample data)\n4. Wait for training to complete (~1-3 hours depending on data size)\n5. Download your LoRA adapters at the end"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: Configuration\n",
    "\n",
    "**Change these settings before running!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#@title Configuration { display-mode: \"form\" }\n\n#@markdown ### Model Selection\n#@markdown > **Free tier (T4):** qwen3_4b, qwen3_8b, llama31_8b, gemma2_9b, mistral_nemo_12b\n#@markdown >\n#@markdown > **Paid/Kaggle (L4+):** phi4_14b, qwen3_14b\n#@markdown >\n#@markdown > **A100 only:** qwen3_32b\nMODEL_CHOICE = \"qwen3_8b\" #@param [\"qwen3_4b\", \"qwen3_8b\", \"llama31_8b\", \"gemma2_9b\", \"mistral_nemo_12b\", \"phi4_14b\", \"qwen3_14b\", \"qwen3_32b\"]\n\n#@markdown ### Data Upload Mode\n#@markdown > **upload_jsonl** = Upload a ready-made `train.jsonl` (skips pipeline, fastest)\n#@markdown >\n#@markdown > **upload_raw** = Upload raw novel files (.txt/.epub/etc), pipeline runs on Colab\n#@markdown >\n#@markdown > **sample_data** = Use built-in sample text (for testing)\nUPLOAD_MODE = \"upload_jsonl\" #@param [\"upload_jsonl\", \"upload_raw\", \"sample_data\"]\n\n#@markdown ### Training Settings\nNUM_EPOCHS = 2 #@param {type:\"slider\", min:1, max:5, step:1}\nLEARNING_RATE = 2e-4 #@param {type:\"number\"}\nMAX_SEQ_LENGTH = 4096 #@param [2048, 4096, 8192] {type:\"raw\"}\nLORA_RANK = 32 #@param [8, 16, 32, 64] {type:\"raw\"}\nBATCH_SIZE = 2 #@param [1, 2, 4] {type:\"raw\"}\nGRADIENT_ACCUMULATION = 4 #@param [2, 4, 8] {type:\"raw\"}\n\n#@markdown ### Advanced Training Settings\n#@markdown > **NEFTune** adds noise to embeddings during training — proven to significantly\n#@markdown > improve creative text generation quality (paper: NEFTune, 2023).\n#@markdown > Set to 0 to disable.\nNEFTUNE_ALPHA = 5 #@param {type:\"number\"}\nWEIGHT_DECAY = 0.01 #@param {type:\"number\"}\n\n#@markdown ### Pipeline Settings (only used with upload_raw)\nCHUNK_SIZE = 4000 #@param {type:\"integer\"}\nRUN_DEDUP = True #@param {type:\"boolean\"}\nRUN_QUALITY_FILTER = True #@param {type:\"boolean\"}\n\n#@markdown ---\n\n# ===== System prompts =====\n_ZH_SYSTEM = (\n    '你是一位经验丰富的中文小说作家，擅长构建沉浸式的叙事场景。'\n    '请根据给定的上下文续写故事，要求：\\n'\n    '1. 保持与原文一致的叙事视角和文风\\n'\n    '2. 通过具体的动作、对话和环境描写推动情节发展\\n'\n    '3. 角色的言行应符合其性格特征和当前情境\\n'\n    '4. 善用感官细节（视觉、听觉、触觉、嗅觉）营造氛围\\n'\n    '5. 对话要自然生动，符合角色身份和说话习惯\\n'\n    '6. 避免空洞的心理独白，用行动和细节展现人物内心'\n)\n\n_EN_SYSTEM = (\n    'You are an accomplished fiction author with a gift for immersive storytelling. '\n    'Continue the narrative following these principles:\\n'\n    '1. Maintain the established point of view, voice, and tonal register\\n'\n    '2. Advance the plot through concrete action, dialogue, and environmental detail\\n'\n    '3. Show character emotion through behavior, body language, and subtext — not exposition\\n'\n    '4. Engage multiple senses (sight, sound, touch, smell, taste) to ground scenes\\n'\n    '5. Write dialogue that reveals character, creates tension, and sounds natural\\n'\n    '6. Vary sentence rhythm — mix short punchy lines with longer flowing passages'\n)\n\n# ===== Diverse instruction pools for training data =====\n_ZH_INSTRUCTIONS = [\n    '续写这段叙事，保持原文的风格和节奏。',\n    '以相同的文风继续这个故事。',\n    '根据已有的情节和人物设定，续写下一段。',\n    '保持叙事视角不变，继续推进故事发展。',\n    '用生动的细节描写续写这个场景。',\n    '通过对话和动作描写推进下面的情节。',\n    '延续当前的叙事氛围，写出接下来发生的事。',\n    '以细腻的笔触续写这段文字。',\n    '按照原文的叙事节奏，写出故事的下一部分。',\n    '继续描绘这个场景中的人物和事件。',\n    '用符合原文风格的语言续写故事。',\n    '展开叙述，让故事自然地向前发展。',\n    '保持文风一致，续写接下来的情节。',\n    '以沉浸式的叙事方式继续这段故事。',\n    '描绘接下来的场景，注意环境和人物的刻画。',\n    '用简洁有力的文字续写这段叙事。',\n    '继续讲述这个故事，注意情感的表达。',\n    '以自然流畅的文笔续写下一段。',\n    '延续原文的基调，推进故事走向。',\n    '用丰富的感官描写续写这个场景。',\n]\n\n_EN_INSTRUCTIONS = [\n    'Continue the narrative in the established style.',\n    'Write the next passage, maintaining the existing voice and tone.',\n    'Advance the story using vivid sensory details.',\n    'Continue this scene with natural dialogue and action.',\n    'Extend the narrative, preserving the point of view and pacing.',\n    'Write what happens next, staying true to the characters.',\n    'Continue the story with concrete, immersive description.',\n    'Carry the narrative forward in the same literary register.',\n    'Write the next segment, matching the established rhythm.',\n    'Develop this scene further with authentic detail.',\n    'Push the story forward through action and dialogue.',\n    'Continue in the same voice, advancing the plot naturally.',\n    'Write the following passage in the style of the preceding text.',\n    'Extend this scene with attention to atmosphere and character.',\n    'Continue the narrative arc with engaging prose.',\n    'Write what comes next, maintaining tension and pacing.',\n    'Advance the story, weaving in environmental detail.',\n    'Continue with prose that matches the tone and texture of the original.',\n    'Develop the next beat of the story with precise language.',\n    'Carry the scene forward, balancing action with description.',\n]\n\n# ===== Test prompts =====\n# NOTE: Use single quotes for strings containing Chinese quotation marks\n_ZH_PROMPTS = [\n    '续写以下场景：\\n\\n暴雨如注，李明浑身湿透地站在破庙门口。庙里的火堆旁，一个蒙面人正用匕首削着木棍。两人目光相遇的瞬间，空气仿佛凝固了。\\n\\n请从李明的视角续写这个紧张的对峙场景，注意环境描写和人物心理。',\n    '以下是一段武侠小说的开头，请续写：\\n\\n月色如霜，照在悬崖边两道对峙的身影上。左边那人白衣胜雪，手中长剑微微颤动；右边那人一袭黑袍，双手背在身后，嘴角挂着一抹冷笑。\\n\\n\\u201c三年了，\\u201d白衣人开口，声音像是从牙缝里挤出来的，\\u201c你终于肯现身了。\\u201d\\n\\n续写这场决斗，要有招式描写和心理活动。',\n    '请用细腻的笔触描写以下场景：\\n\\n清晨的江南小镇刚刚苏醒。青石板路上还残留着昨夜的雨水，空气中弥漫着桂花和早点铺子里蒸笼的气息。一个背着书箱的年轻书生走过石桥，桥下有渔翁在收网。\\n\\n注意五感描写，营造宁静温暖的氛围。',\n]\n_EN_PROMPTS = [\n    'Continue this scene from the lighthouse keeper\\'s perspective:\\n\\nThe storm hit at midnight. Thomas pressed his face to the glass and watched the beam sweep across walls of black water. Then he saw it \\u2014 a flare, red and desperate, arcing up from somewhere beyond the reef.\\n\\nHe reached for the radio. Dead. The antenna had gone in the last gust.\\n\\nWrite the next 300 words. Focus on his decision-making, the physical environment, and building tension.',\n    'Continue this dialogue-driven scene:\\n\\nThe cafe was nearly empty. Rain streaked the windows, blurring the Paris streetlights into watercolor smears. Elena stirred her coffee for the third time without drinking it.\\n\\n\"You\\'re not here for the coffee,\" said the man across from her. He hadn\\'t touched his either.\\n\\n\"And you\\'re not here by accident,\" she replied.\\n\\nHe smiled \\u2014 not warmly. \"I know what you did in Lyon.\"\\n\\nContinue with tension-building dialogue. Reveal character through speech patterns and subtext, not exposition.',\n    'Write the next scene:\\n\\nAfter ten years of war, Commander Asha Renn walked through what remained of the village gate. The wooden arch was gone \\u2014 burned, she guessed, years ago. Where her mother\\'s garden had been, there was a blacksmith\\'s forge. A child she didn\\'t recognize stared at her scarred face with wide eyes.\\n\\n\"Are you a soldier?\" the child asked.\\n\\nContinue from Asha\\'s perspective. Balance external observation with internal emotion. Use specific sensory details to show how the village has changed.',\n]\n\nMODEL_CONFIGS = {\n    # ---- Free tier models (T4 / 12-16 GB VRAM) ----\n    'qwen3_4b': {\n        'model_name': 'unsloth/Qwen3-4B',\n        'output_name': 'qwen3_4b_novel_lora',\n        'system_prompt': _ZH_SYSTEM,\n        'test_prompts': _ZH_PROMPTS,\n        'lang': 'zh',\n        'min_vram_gb': 8,\n    },\n    'qwen3_8b': {\n        'model_name': 'unsloth/Qwen3-8B',\n        'output_name': 'qwen3_8b_novel_lora',\n        'system_prompt': _ZH_SYSTEM,\n        'test_prompts': _ZH_PROMPTS,\n        'lang': 'zh',\n        'min_vram_gb': 12,\n    },\n    'llama31_8b': {\n        'model_name': 'unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit',\n        'output_name': 'llama31_8b_novel_lora',\n        'system_prompt': _EN_SYSTEM,\n        'test_prompts': _EN_PROMPTS,\n        'lang': 'en',\n        'min_vram_gb': 12,\n    },\n    'gemma2_9b': {\n        'model_name': 'unsloth/gemma-2-9b-it-bnb-4bit',\n        'output_name': 'gemma2_9b_novel_lora',\n        'system_prompt': _EN_SYSTEM,\n        'test_prompts': _EN_PROMPTS,\n        'lang': 'en',\n        'min_vram_gb': 12,\n    },\n    'mistral_nemo_12b': {\n        'model_name': 'unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit',\n        'output_name': 'mistral_nemo_12b_novel_lora',\n        'system_prompt': _EN_SYSTEM,\n        'test_prompts': _EN_PROMPTS,\n        'lang': 'en',\n        'min_vram_gb': 12,\n    },\n    # ---- Larger models (L4/A10 / 24+ GB VRAM) ----\n    'phi4_14b': {\n        'model_name': 'unsloth/Phi-4-bnb-4bit',\n        'output_name': 'phi4_14b_novel_lora',\n        'system_prompt': _EN_SYSTEM,\n        'test_prompts': _EN_PROMPTS,\n        'lang': 'en',\n        'min_vram_gb': 24,\n    },\n    'qwen3_14b': {\n        'model_name': 'unsloth/Qwen3-14B',\n        'output_name': 'qwen3_14b_novel_lora',\n        'system_prompt': _ZH_SYSTEM,\n        'test_prompts': _ZH_PROMPTS,\n        'lang': 'zh',\n        'min_vram_gb': 24,\n    },\n    # ---- A100 models (40+ GB VRAM) ----\n    'qwen3_32b': {\n        'model_name': 'unsloth/Qwen3-32B',\n        'output_name': 'qwen3_32b_novel_lora',\n        'system_prompt': _ZH_SYSTEM,\n        'test_prompts': _ZH_PROMPTS,\n        'lang': 'zh',\n        'min_vram_gb': 40,\n    },\n}\n\nCFG = MODEL_CONFIGS[MODEL_CHOICE]\nprint(f\"Model: {CFG['model_name']}\")\nprint(f\"Language: {'Chinese' if CFG['lang'] == 'zh' else 'English'}\")\nprint(f\"Min VRAM: {CFG['min_vram_gb']} GB\")\nprint(f\"Upload mode: {UPLOAD_MODE}\")\nprint(f\"Output: {CFG['output_name']}\")\nprint(f\"Epochs: {NUM_EPOCHS}, LR: {LEARNING_RATE}, Seq len: {MAX_SEQ_LENGTH}\")\nprint(f\"LoRA rank: {LORA_RANK}, Batch: {BATCH_SIZE}, Grad accum: {GRADIENT_ACCUMULATION}\")\nprint(f\"NEFTune alpha: {NEFTUNE_ALPHA} {'(enabled)' if NEFTUNE_ALPHA > 0 else '(disabled)'}\")\nprint(f\"Weight decay: {WEIGHT_DECAY}\")\n\n# VRAM warning\nimport subprocess\ntry:\n    result = subprocess.run(['nvidia-smi', '--query-gpu=memory.total', '--format=csv,noheader,nounits'],\n                          capture_output=True, text=True)\n    gpu_vram = int(result.stdout.strip()) / 1024\n    if gpu_vram < CFG['min_vram_gb']:\n        print(f'\\nWARNING: Your GPU has ~{gpu_vram:.0f} GB VRAM but {MODEL_CHOICE} needs {CFG[\"min_vram_gb\"]} GB.')\n        print(f'   Consider using a smaller model or upgrading your Colab runtime.')\n    else:\n        print(f'\\nGPU VRAM: {gpu_vram:.0f} GB (requirement: {CFG[\"min_vram_gb\"]} GB)')\nexcept Exception:\n    pass"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Install Unsloth (2x faster training, 70% less VRAM)\n",
    "!pip install unsloth\n",
    "!pip install --force-reinstall --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git\n",
    "\n",
    "# Clone Novel Writer repo\n",
    "!rm -rf /content/Novel_Writer\n",
    "!git clone https://github.com/LL-LLLu/Novel_Writer.git /content/Novel_Writer\n",
    "\n",
    "# Install Novel Writer\n",
    "%cd /content/Novel_Writer\n",
    "!pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Verify installation\nimport torch\nprint(f\"GPU: {torch.cuda.get_device_name(0)}\")\nprint(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\nprint()\n\n!novel-writer --help | head -20\nprint(\"\\nSetup complete!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Step 3: Upload Data\n\n**Three modes** (set `UPLOAD_MODE` in Step 1):\n\n| Mode | What to upload | Pipeline runs? |\n|------|---------------|----------------|\n| `upload_jsonl` | Your `train.jsonl` from local pipeline | No (fastest) |\n| `upload_raw` | Raw novel files (.txt, .epub, .html, etc.) | Yes |\n| `sample_data` | Nothing - uses built-in sample text | Yes |"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os, json\nfrom pathlib import Path\n\n# Ensure CWD is valid (cell 4 rm -rf and re-clone can invalidate it)\nos.chdir('/content/Novel_Writer')\n\ndata_dir = Path('/content/Novel_Writer/data/raw')\nprocessed_dir = Path('/content/Novel_Writer/data/processed')\ndata_dir.mkdir(parents=True, exist_ok=True)\nprocessed_dir.mkdir(parents=True, exist_ok=True)\n\nSKIP_PIPELINE = False  # Will be set to True if user uploads train.jsonl directly\n\nif UPLOAD_MODE == 'upload_jsonl':\n    # ====== FASTEST: Upload pre-processed train.jsonl directly ======\n    from google.colab import files as colab_files\n    print('Upload your train.jsonl file:')\n    print('(This is the file from: data/processed/train.jsonl)')\n    print()\n\n    # Upload into the processed directory directly\n    os.chdir(str(processed_dir))\n    uploaded = colab_files.upload()\n    os.chdir('/content/Novel_Writer')\n\n    target = processed_dir / 'train.jsonl'\n    # If uploaded file has a different name, rename it\n    for name in uploaded:\n        src = processed_dir / name\n        if src != target and src.exists():\n            src.rename(target)\n\n    # Validate\n    with open(target, 'r', encoding='utf-8') as f:\n        lines = f.readlines()\n    sample = json.loads(lines[0])\n    print(f'\\nUploaded -> {target}')\n    print(f'  Entries: {len(lines)}')\n    print(f'  Keys: {list(sample.keys())}')\n    print(f'  Preview: {sample.get(\"output\", \"\")[:100]}...')\n\n    TRAIN_FILE = str(target)\n    SKIP_PIPELINE = True\n    print(f'\\nPipeline will be SKIPPED (data already processed)')\n\nelif UPLOAD_MODE == 'upload_raw':\n    # ====== Upload raw novel files, pipeline runs on Colab ======\n    from google.colab import files as colab_files\n    print('Upload your novel files (.txt, .pdf, .epub, .html, .md, .mobi):')\n    print('(Click \"Choose Files\" button below)')\n    print()\n\n    os.chdir(str(data_dir))\n    uploaded = colab_files.upload()\n    os.chdir('/content/Novel_Writer')\n\n    for name in uploaded:\n        size = (data_dir / name).stat().st_size\n        print(f'  Saved: {name} ({size:,} bytes)')\n\n    print(f'\\nTotal files: {len(list(data_dir.iterdir()))}')\n    print('Pipeline will process these into training data.')\n\nelif UPLOAD_MODE == 'sample_data':\n    # ====== Built-in sample data for testing ======\n    if CFG['lang'] == 'zh':\n        sample_text = (\n            '\\n\\u7b2c1\\u7ae0 \\u9ece\\u660e\\u4e4b\\u524d\\n\\n'\n            '\\u5929\\u8fd8\\u6ca1\\u6709\\u4eae\\uff0c\\u6574\\u4e2a\\u6751\\u5e84\\u90fd\\u7b3c\\u7f69\\u5728\\u4e00\\u7247\\u5bc2\\u9759\\u4e4b\\u4e2d\\u3002'\n            '\\u8fdc\\u5904\\u7684\\u5c71\\u5ce6\\u5728\\u8584\\u96fe\\u4e2d\\u82e5\\u9690\\u82e5\\u73b0\\uff0c\\u4eff\\u4f5b\\u4e00\\u5e45\\u6de1\\u58a8\\u5c71\\u6c34\\u753b\\u3002\\n'\n            '\\u674e\\u660e\\u7ad9\\u5728\\u9662\\u5b50\\u91cc\\uff0c\\u6df1\\u6df1\\u5730\\u5438\\u4e86\\u4e00\\u53e3\\u6e05\\u6668\\u7684\\u7a7a\\u6c14\\u3002'\n            '\\u4eca\\u5929\\u662f\\u4e2a\\u7279\\u522b\\u7684\\u65e5\\u5b50\\uff0c\\u4ed6\\u5df2\\u7ecf\\u7b49\\u4e86\\u6574\\u6574\\u4e09\\u5e74\\u3002\\n\\n'\n            '\\u201c\\u4f60\\u771f\\u7684\\u8981\\u8d70\\u5417\\uff1f\\u201d\\u8eab\\u540e\\u4f20\\u6765\\u6bcd\\u4eb2\\u82cd\\u8001\\u7684\\u58f0\\u97f3\\u3002\\n\\n'\n            '\\u674e\\u660e\\u6ca1\\u6709\\u56de\\u5934\\uff0c\\u4ed6\\u77e5\\u9053\\u5982\\u679c\\u56de\\u5934\\uff0c'\n            '\\u81ea\\u5df1\\u53ef\\u80fd\\u5c31\\u518d\\u4e5f\\u8d70\\u4e0d\\u4e86\\u4e86\\u3002'\n            '\\u201c\\u5988\\uff0c\\u6211\\u4f1a\\u56de\\u6765\\u7684\\u3002\\u201d\\n\\n'\n            '\\u4ed6\\u7684\\u58f0\\u97f3\\u5f88\\u8f7b\\uff0c\\u5374\\u5728\\u5bc2\\u9759\\u7684\\u6e05\\u6668\\u663e\\u5f97\\u683c\\u5916\\u6e05\\u6670\\u3002'\n            '\\u6bcd\\u4eb2\\u6ca1\\u6709\\u518d\\u8bf4\\u4ec0\\u4e48\\uff0c\\u53ea\\u662f\\u9ed8\\u9ed8\\u5730\\u5c06\\u4e00\\u4e2a\\u5305\\u88f1\\u9012\\u5230\\u4ed6\\u624b\\u4e2d\\u3002\\n'\n            '\\u5305\\u88f1\\u4e0d\\u91cd\\uff0c\\u4f46\\u674e\\u660e\\u77e5\\u9053\\u91cc\\u9762\\u88c5\\u7740\\u6bcd\\u4eb2\\u6240\\u6709\\u7684\\u5fc3\\u610f\\u2014\\u2014'\n            '\\u51e0\\u4ef6\\u6362\\u6d17\\u7684\\u8863\\u88f3\\uff0c\\u51e0\\u4e2a\\u70d9\\u997c\\uff0c'\n            '\\u8fd8\\u6709\\u7236\\u4eb2\\u7559\\u4e0b\\u7684\\u90a3\\u628a\\u77ed\\u5200\\u3002\\n\\n'\n            '\\u201c\\u8def\\u4e0a\\u5c0f\\u5fc3\\u3002\\u201d\\u6bcd\\u4eb2\\u7ec8\\u4e8e\\u5f00\\u53e3\\uff0c\\u58f0\\u97f3\\u6709\\u4e9b\\u98a4\\u6296\\u3002\\n\\n'\n            '\\u674e\\u660e\\u70b9\\u4e86\\u70b9\\u5934\\uff0c\\u80cc\\u8d77\\u5305\\u88f1\\uff0c\\u5411\\u6751\\u53e3\\u8d70\\u53bb\\u3002'\n            '\\u6668\\u96fe\\u6e10\\u6e10\\u6563\\u5f00\\uff0c\\u4e1c\\u65b9\\u7684\\u5929\\u9645\\u6cdb\\u8d77\\u4e86\\u4e00\\u62b9\\u9c7c\\u809a\\u767d\\u3002\\n'\n            '\\u4ed6\\u77e5\\u9053\\uff0c\\u4ece\\u8fd9\\u4e00\\u523b\\u8d77\\uff0c\\u4e00\\u5207\\u90fd\\u5c06\\u4e0d\\u540c\\u3002\\n'\n        )\n        for i in range(3):\n            names = ['\\u674e\\u660e', '\\u738b\\u521a', '\\u8d75\\u4e91']\n            (data_dir / f'sample_novel_{i+1}.txt').write_text(\n                sample_text.replace('\\u674e\\u660e', names[i]),\n                encoding='utf-8'\n            )\n    else:\n        sample_text = (\n            '\\nChapter 1: The Last Light\\n\\n'\n            'The old lighthouse stood at the edge of the world, or so it seemed to Thomas Gray.\\n'\n            'For forty years he had climbed these stairs each evening, lit the great lamp, and watched\\n'\n            'its beam sweep across the dark Atlantic waters.\\n\\n'\n            '\"One more night,\" he muttered to himself, a habit born of decades without anyone else\\n'\n            'to talk to. \"Just one more.\"\\n\\n'\n            'The lamp room at the top was warm despite the storm. Thomas had maintained the old\\n'\n            'Fresnel lens with religious devotion. The Coast Guard had wanted to automate the light\\n'\n            'years ago, replace him with sensors and timers. He had fought them tooth and nail.\\n\\n'\n            'Thomas struck the match and touched it to the wick. The flame caught, small at first,\\n'\n            'then growing as the oil drew upward. He watched the light bloom and multiply through\\n'\n            'the precision-cut prisms until it became something powerful, something that could reach\\n'\n            'across miles of angry ocean to tell a sailor: you are not alone.\\n\\n'\n            'He settled into his chair and opened his logbook. \"November 17th,\" he wrote. \"Wind\\n'\n            'northeast, 45 knots gusting to 60. Rain heavy. Visibility poor.\" He paused, pen\\n'\n            'hovering over the page. Then he added: \"Final entry.\"\\n\\n'\n            'Chapter 2: The Storm\\n\\n'\n            'Sarah Chen had not planned to be at sea tonight. The storm had come on fast, much\\n'\n            'faster than the forecast predicted. So she did what sailors do: she shortened sail,\\n'\n            'lashed everything down, clipped her harness to the jackline, and held on.\\n\\n'\n            'Through the rain, through the spray, through the chaos of wind and wave, she saw\\n'\n            'it \\u2014 a light. Sweeping across the water in a steady, ancient rhythm. The lighthouse.\\n\\n'\n            '\"Thank God,\" she breathed, and for the first time in hours, she knew where she was.\\n'\n        )\n        for i in range(3):\n            names = [('Thomas Gray', 'Sarah Chen'), ('James Walker', 'Maria Santos'), ('Robert Kim', 'Elena Volkov')]\n            text = sample_text.replace('Thomas Gray', names[i][0]).replace('Sarah Chen', names[i][1])\n            (data_dir / f'sample_novel_{i+1}.txt').write_text(text, encoding='utf-8')\n\n    print(f'Created sample data in {data_dir}:')\n    for f in sorted(data_dir.iterdir()):\n        if f.is_file():\n            print(f'  {f.name} ({f.stat().st_size:,} bytes)')\n\nprint(f'\\nSkip pipeline: {SKIP_PIPELINE}')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Step 4: Run Data Processing Pipeline\n\nRuns the full pipeline: **clean** > **format** (to JSONL) > **deduplicate** > **quality filter**\n\n*This step is automatically skipped if you uploaded `train.jsonl` directly.*"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if SKIP_PIPELINE:\n    print(\"✓ Skipping pipeline (train.jsonl was uploaded directly)\")\n    print(f\"  Using: {TRAIN_FILE}\")\nelse:\n    import yaml\n    from pathlib import Path\n\n    config = {\n        \"data\": {\n            \"input_dir\": \"data/raw\",\n            \"output_dir\": \"data/processed\",\n            \"temp_dir\": \"data/processed/temp_cleaned\",\n            \"chunk_size\": CHUNK_SIZE,\n            \"overlap\": 500,\n        },\n        \"log_level\": \"INFO\",\n    }\n\n    with open(\"/content/Novel_Writer/config.yaml\", \"w\") as f:\n        yaml.dump(config, f, default_flow_style=False)\n\n    print(\"Config written. Running pipeline...\\n\")\n\n    cmd = \"novel-writer -v pipeline --clean\"\n\n    ingest_exts = {\".epub\", \".html\", \".htm\", \".md\", \".mobi\"}\n    has_ingestable = any(f.suffix.lower() in ingest_exts for f in Path(\"data/raw\").iterdir() if f.is_file())\n    if has_ingestable:\n        cmd += \" --ingest\"\n    if RUN_DEDUP:\n        cmd += \" --deduplicate\"\n    if RUN_QUALITY_FILTER:\n        cmd += \" --filter\"\n\n    print(f\"Command: {cmd}\\n\")\n    !{cmd}"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import json\nfrom pathlib import Path\n\nif not SKIP_PIPELINE:\n    # Find the final JSONL file produced by pipeline\n    processed_dir = Path(\"data/processed\")\n    jsonl_files = sorted(processed_dir.glob(\"*.jsonl\"), key=lambda f: f.stat().st_mtime, reverse=True)\n\n    if not jsonl_files:\n        raise FileNotFoundError(\"No JSONL files produced! Check pipeline output above.\")\n\n    TRAIN_FILE = str(jsonl_files[0])\n\n# Validate the training file\nwith open(TRAIN_FILE, \"r\", encoding=\"utf-8\") as f:\n    lines = f.readlines()\n\nprint(f\"Training data: {TRAIN_FILE}\")\nprint(f\"Total entries: {len(lines)}\")\n\nif lines:\n    sample = json.loads(lines[0])\n    print(f\"Keys: {list(sample.keys())}\")\n    print(f\"Output preview: {sample.get('output', '')[:200]}...\")\nelse:\n    raise ValueError(\"Training file is empty!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: Load Model & Configure LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "print(f\"Loading model: {CFG['model_name']}\")\n",
    "print(f\"Max sequence length: {MAX_SEQ_LENGTH}\")\n",
    "print(f\"4-bit quantization: True\\n\")\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=CFG[\"model_name\"],\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    dtype=None,          # Auto-detect\n",
    "    load_in_4bit=True,   # QLoRA\n",
    ")\n",
    "\n",
    "print(f\"\\nGPU memory after loading: {torch.cuda.memory_allocated() / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=LORA_RANK,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha=LORA_RANK // 2,\n",
    "    lora_dropout=0,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=3407,\n",
    ")\n",
    "\n",
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Trainable parameters: {trainable:,} / {total:,} ({100 * trainable / total:.2f}%)\")\n",
    "print(f\"GPU memory with LoRA: {torch.cuda.memory_allocated() / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 6: Prepare Dataset for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import random\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"json\", data_files=TRAIN_FILE, split=\"train\")\n\n# Diversify instructions if they're all the same (common with pipeline-generated data)\nunique_instructions = set(dataset[\"instruction\"])\nif len(unique_instructions) <= 2:\n    print(f\"Found only {len(unique_instructions)} unique instruction(s) - diversifying...\")\n    instruction_pool = _ZH_INSTRUCTIONS if CFG[\"lang\"] == \"zh\" else _EN_INSTRUCTIONS\n\n    def diversify_instructions(examples):\n        new_instructions = [random.choice(instruction_pool) for _ in examples[\"instruction\"]]\n        return {\"instruction\": new_instructions}\n\n    dataset = dataset.map(diversify_instructions, batched=True)\n    new_unique = len(set(dataset[\"instruction\"]))\n    print(f\"  Diversified to {new_unique} unique instructions\")\nelse:\n    print(f\"Instructions already diverse ({len(unique_instructions)} unique)\")\n\n# Train/validation split\nif len(dataset) > 10:\n    split = dataset.train_test_split(test_size=0.1, seed=42)\n    train_dataset = split[\"train\"]\n    eval_dataset = split[\"test\"]\nelse:\n    train_dataset = dataset\n    eval_dataset = None\n    print(\"Dataset too small for validation split, training on all data.\")\n\nprint(f\"Training samples: {len(train_dataset)}\")\nif eval_dataset:\n    print(f\"Validation samples: {len(eval_dataset)}\")\n\n# Universal formatting using tokenizer's built-in chat template\n# This works for ALL models (Qwen, Llama, Mistral, Gemma, Phi, etc.)\ndef formatting_func(examples):\n    instructions = examples[\"instruction\"]\n    outputs = examples[\"output\"]\n    texts = []\n    for instruction, output in zip(instructions, outputs):\n        messages = [\n            {\"role\": \"system\", \"content\": CFG[\"system_prompt\"]},\n            {\"role\": \"user\", \"content\": instruction},\n            {\"role\": \"assistant\", \"content\": output},\n        ]\n        # apply_chat_template handles the correct format for each model family\n        try:\n            text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n        except Exception:\n            # Fallback for models without system role support\n            messages_no_sys = [\n                {\"role\": \"user\", \"content\": CFG[\"system_prompt\"] + \"\\n\\n\" + instruction},\n                {\"role\": \"assistant\", \"content\": output},\n            ]\n            text = tokenizer.apply_chat_template(messages_no_sys, tokenize=False, add_generation_prompt=False)\n        texts.append(text)\n    return {\"text\": texts}\n\ntrain_dataset = train_dataset.map(formatting_func, batched=True)\nif eval_dataset:\n    eval_dataset = eval_dataset.map(formatting_func, batched=True)\n\nprint(f\"\\n--- Sample formatted entry ---\")\nprint(train_dataset[0][\"text\"][:600])\nprint(\"...\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 7: Train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from trl import SFTTrainer\nfrom transformers import TrainingArguments, EarlyStoppingCallback\n\ntraining_args = TrainingArguments(\n    output_dir=f\"checkpoints_{CFG['output_name']}\",\n    num_train_epochs=NUM_EPOCHS,\n    per_device_train_batch_size=BATCH_SIZE,\n    gradient_accumulation_steps=GRADIENT_ACCUMULATION,\n    warmup_ratio=0.1,\n    learning_rate=LEARNING_RATE,\n    weight_decay=WEIGHT_DECAY,\n    lr_scheduler_type=\"cosine\",\n    fp16=not torch.cuda.is_bf16_supported(),\n    bf16=torch.cuda.is_bf16_supported(),\n    logging_steps=5,\n    save_strategy=\"steps\" if eval_dataset else \"epoch\",\n    save_steps=50 if eval_dataset else None,\n    save_total_limit=3,\n    seed=3407,\n)\n\n# Add eval settings if we have validation data\nif eval_dataset:\n    training_args.eval_strategy = \"steps\"\n    training_args.eval_steps = 50\n    training_args.load_best_model_at_end = True\n    training_args.metric_for_best_model = \"eval_loss\"\n    training_args.greater_is_better = False\n\ncallbacks = []\nif eval_dataset:\n    callbacks.append(EarlyStoppingCallback(early_stopping_patience=3))\n\ntrainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    dataset_text_field=\"text\",\n    max_seq_length=MAX_SEQ_LENGTH,\n    dataset_num_proc=2,\n    packing=False,\n    neftune_noise_alpha=NEFTUNE_ALPHA if NEFTUNE_ALPHA > 0 else None,\n    args=training_args,\n    callbacks=callbacks if callbacks else None,\n)\n\nnef_status = f\"NEFTune alpha={NEFTUNE_ALPHA}\" if NEFTUNE_ALPHA > 0 else \"NEFTune disabled\"\nprint(f\"Starting training: {NUM_EPOCHS} epochs, {len(train_dataset)} samples\")\nprint(f\"Effective batch size: {BATCH_SIZE * GRADIENT_ACCUMULATION}\")\nprint(f\"Estimated steps: {len(train_dataset) * NUM_EPOCHS // (BATCH_SIZE * GRADIENT_ACCUMULATION)}\")\nprint(f\"LR schedule: cosine, Weight decay: {WEIGHT_DECAY}, {nef_status}\")\nprint(\"=\"*60)\n\nstats = trainer.train()\n\nprint(\"=\"*60)\nprint(f\"Training complete!\")\nprint(f\"  Total steps: {stats.global_step}\")\nprint(f\"  Training loss: {stats.training_loss:.4f}\")\nprint(f\"  Runtime: {stats.metrics['train_runtime']:.0f} seconds\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 8: Test Generation\n",
    "\n",
    "Let's see what the fine-tuned model can do!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "FastLanguageModel.for_inference(model)\n\nprint(f\"Generating with {CFG['model_name']}...\\n\")\n\nfor i, prompt in enumerate(CFG[\"test_prompts\"]):\n    # Universal chat template approach - works for all models\n    messages = [\n        {\"role\": \"system\", \"content\": CFG[\"system_prompt\"]},\n        {\"role\": \"user\", \"content\": prompt},\n    ]\n    try:\n        inputs = tokenizer.apply_chat_template(\n            messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\"\n        ).to(\"cuda\")\n    except Exception:\n        # Fallback for models without system role\n        messages_no_sys = [\n            {\"role\": \"user\", \"content\": CFG[\"system_prompt\"] + \"\\n\\n\" + prompt},\n        ]\n        inputs = tokenizer.apply_chat_template(\n            messages_no_sys, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\"\n        ).to(\"cuda\")\n\n    input_len = inputs.shape[-1]\n\n    outputs = model.generate(\n        input_ids=inputs,\n        max_new_tokens=512,\n        temperature=0.8,\n        top_p=0.9,\n        top_k=50,\n        do_sample=True,\n        repetition_penalty=1.1,\n    )\n    response = tokenizer.decode(outputs[0][input_len:], skip_special_tokens=True)\n\n    print(f\"{'='*60}\")\n    print(f\"Prompt {i+1}: {prompt}\")\n    print(f\"{'='*60}\")\n    print(response)\n    print(f\"[{len(response)} chars]\\n\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 9: Save & Download Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_name = CFG[\"output_name\"]\n",
    "\n",
    "# Save LoRA adapters\n",
    "model.save_pretrained(output_name)\n",
    "tokenizer.save_pretrained(output_name)\n",
    "print(f\"Model saved to {output_name}/\")\n",
    "\n",
    "# Show saved files\n",
    "import os\n",
    "total_size = 0\n",
    "for f in sorted(Path(output_name).rglob(\"*\")):\n",
    "    if f.is_file():\n",
    "        size = f.stat().st_size\n",
    "        total_size += size\n",
    "        print(f\"  {f.name}: {size / 1024 / 1024:.1f} MB\")\n",
    "print(f\"\\nTotal size: {total_size / 1024 / 1024:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download as zip\n",
    "!zip -r {output_name}.zip {output_name}/\n",
    "\n",
    "from google.colab import files as colab_files\n",
    "colab_files.download(f\"{output_name}.zip\")\n",
    "print(f\"\\nDownloading {output_name}.zip ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 10 (Optional): Save to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment these lines to save to Google Drive\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount(\"/content/drive\")\n",
    "#\n",
    "# import shutil\n",
    "# drive_path = f\"/content/drive/MyDrive/{output_name}\"\n",
    "# shutil.copytree(output_name, drive_path, dirs_exist_ok=True)\n",
    "# print(f\"Saved to Google Drive: {drive_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 11 (Optional): Export to GGUF for Local Use\n",
    "\n",
    "Export your model to GGUF format for running locally with **Ollama** or **llama.cpp**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to export to GGUF (takes ~10-15 minutes)\n",
    "\n",
    "# gguf_name = f\"{output_name}_gguf\"\n",
    "# model.save_pretrained_gguf(\n",
    "#     gguf_name,\n",
    "#     tokenizer,\n",
    "#     quantization_method=\"q4_k_m\",  # Good balance of quality vs size\n",
    "# )\n",
    "#\n",
    "# from google.colab import files as colab_files\n",
    "# gguf_file = list(Path(gguf_name).glob(\"*.gguf\"))[0]\n",
    "# colab_files.download(str(gguf_file))\n",
    "# print(f\"GGUF exported! Run locally with:\")\n",
    "# print(f\"  ollama run ./{gguf_file.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Done!\n",
    "\n",
    "Your fine-tuned model has been saved. To use it locally with the Novel Writer CLI:\n",
    "\n",
    "```bash\n",
    "# Unzip your downloaded model\n",
    "unzip qwen3_chinese_novel_lora.zip  # or nemo_english_story_lora.zip\n",
    "\n",
    "# Generate text\n",
    "novel-writer generate --prompt \"Your prompt here...\" --model qwen3_chinese_novel_lora\n",
    "```\n",
    "\n",
    "Or start the API server:\n",
    "```bash\n",
    "python -m novel_writer.api\n",
    "# POST to http://localhost:8000/generate\n",
    "```"
   ]
  }
 ]
}