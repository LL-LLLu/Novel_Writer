{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Novel Writer - Complete Pipeline & Training\n\nThis notebook runs **everything** end-to-end on Google Colab:\n\n1. Clone repo & install dependencies\n2. Upload your novels (or use built-in sample data)\n3. Run the full data processing pipeline\n4. Fine-tune your chosen model\n5. Generate sample text\n6. Download your trained model\n\n### Supported Models\n\n| Model | Params | Best For | Min GPU | Free Tier? |\n|-------|--------|----------|---------|------------|\n| Qwen3-4B | 4B | Chinese (lightweight) | 8GB (T4) | Yes |\n| Qwen3-8B | 8B | Chinese | 12GB (T4) | Yes |\n| Llama 3.1 8B | 8B | English | 12GB (T4) | Yes |\n| Gemma 2 9B | 9B | English | 12GB (T4) | Yes |\n| Mistral Nemo 12B | 12B | English creative writing | 12GB (T4) | Yes |\n| Phi-4 14B | 14B | English (reasoning + writing) | 24GB (L4/A10) | Kaggle 2xT4 |\n| Qwen3-14B | 14B | Chinese + English | 24GB (L4/A10) | Kaggle 2xT4 |\n| Qwen3-32B | 32B | Chinese + English (best quality) | 40GB (A100) | No |\n\n**Requirements:** Google Colab with T4 GPU (free tier works for 4B-12B models)\n\n---\n\n### How to use\n1. Pick your model in **Cell 1** below\n2. **Runtime > Run all**\n3. When prompted, upload your novel files (or skip to use sample data)\n4. Wait for training to complete (~1-3 hours depending on data size)\n5. Download your LoRA adapters at the end"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: Configuration\n",
    "\n",
    "**Change these settings before running!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#@title Configuration { display-mode: \"form\" }\n\n#@markdown ### Model Selection\n#@markdown > **Free tier (T4):** qwen3_4b, qwen3_8b, llama31_8b, gemma2_9b, mistral_nemo_12b\n#@markdown >\n#@markdown > **Paid/Kaggle (L4+):** phi4_14b, qwen3_14b\n#@markdown >\n#@markdown > **A100 only:** qwen3_32b\nMODEL_CHOICE = \"qwen3_8b\" #@param [\"qwen3_4b\", \"qwen3_8b\", \"llama31_8b\", \"gemma2_9b\", \"mistral_nemo_12b\", \"phi4_14b\", \"qwen3_14b\", \"qwen3_32b\"]\n\n#@markdown ### Data Upload Mode\n#@markdown > **upload_jsonl** = Upload a ready-made `train.jsonl` (skips pipeline, fastest)\n#@markdown >\n#@markdown > **upload_raw** = Upload raw novel files (.txt/.epub/etc), pipeline runs on Colab\n#@markdown >\n#@markdown > **sample_data** = Use built-in sample text (for testing)\nUPLOAD_MODE = \"upload_jsonl\" #@param [\"upload_jsonl\", \"upload_raw\", \"sample_data\"]\n\n#@markdown ### Training Settings\nNUM_EPOCHS = 2 #@param {type:\"slider\", min:1, max:5, step:1}\nLEARNING_RATE = 2e-4 #@param {type:\"number\"}\nMAX_SEQ_LENGTH = 4096 #@param [2048, 4096, 8192] {type:\"raw\"}\nLORA_RANK = 32 #@param [8, 16, 32, 64] {type:\"raw\"}\nBATCH_SIZE = 2 #@param [1, 2, 4] {type:\"raw\"}\nGRADIENT_ACCUMULATION = 4 #@param [2, 4, 8] {type:\"raw\"}\n\n#@markdown ### Advanced Training Settings\n#@markdown > **NEFTune** adds noise to embeddings during training — proven to significantly\n#@markdown > improve creative text generation quality (paper: NEFTune, 2023).\n#@markdown > Set to 0 to disable.\nNEFTUNE_ALPHA = 5 #@param {type:\"number\"}\nWEIGHT_DECAY = 0.01 #@param {type:\"number\"}\n\n#@markdown ### Pipeline Settings (only used with upload_raw)\nCHUNK_SIZE = 4000 #@param {type:\"integer\"}\nRUN_DEDUP = True #@param {type:\"boolean\"}\nRUN_QUALITY_FILTER = True #@param {type:\"boolean\"}\n\n#@markdown ---\n\n# ===== System prompts =====\n_ZH_SYSTEM = (\n    '你是一位经验丰富的中文小说作家，擅长构建沉浸式的叙事场景。'\n    '请根据给定的上下文续写故事，要求：\\n'\n    '1. 保持与原文一致的叙事视角和文风\\n'\n    '2. 通过具体的动作、对话和环境描写推动情节发展\\n'\n    '3. 角色的言行应符合其性格特征和当前情境\\n'\n    '4. 善用感官细节（视觉、听觉、触觉、嗅觉）营造氛围\\n'\n    '5. 对话要自然生动，符合角色身份和说话习惯\\n'\n    '6. 避免空洞的心理独白，用行动和细节展现人物内心'\n)\n\n_EN_SYSTEM = (\n    'You are an accomplished fiction author with a gift for immersive storytelling. '\n    'Continue the narrative following these principles:\\n'\n    '1. Maintain the established point of view, voice, and tonal register\\n'\n    '2. Advance the plot through concrete action, dialogue, and environmental detail\\n'\n    '3. Show character emotion through behavior, body language, and subtext — not exposition\\n'\n    '4. Engage multiple senses (sight, sound, touch, smell, taste) to ground scenes\\n'\n    '5. Write dialogue that reveals character, creates tension, and sounds natural\\n'\n    '6. Vary sentence rhythm — mix short punchy lines with longer flowing passages'\n)\n\n# ===== Diverse instruction pools for training data =====\n_ZH_INSTRUCTIONS = [\n    '续写这段叙事，保持原文的风格和节奏。',\n    '以相同的文风继续这个故事。',\n    '根据已有的情节和人物设定，续写下一段。',\n    '保持叙事视角不变，继续推进故事发展。',\n    '用生动的细节描写续写这个场景。',\n    '通过对话和动作描写推进下面的情节。',\n    '延续当前的叙事氛围，写出接下来发生的事。',\n    '以细腻的笔触续写这段文字。',\n    '按照原文的叙事节奏，写出故事的下一部分。',\n    '继续描绘这个场景中的人物和事件。',\n    '用符合原文风格的语言续写故事。',\n    '展开叙述，让故事自然地向前发展。',\n    '保持文风一致，续写接下来的情节。',\n    '以沉浸式的叙事方式继续这段故事。',\n    '描绘接下来的场景，注意环境和人物的刻画。',\n    '用简洁有力的文字续写这段叙事。',\n    '继续讲述这个故事，注意情感的表达。',\n    '以自然流畅的文笔续写下一段。',\n    '延续原文的基调，推进故事走向。',\n    '用丰富的感官描写续写这个场景。',\n]\n\n_EN_INSTRUCTIONS = [\n    'Continue the narrative in the established style.',\n    'Write the next passage, maintaining the existing voice and tone.',\n    'Advance the story using vivid sensory details.',\n    'Continue this scene with natural dialogue and action.',\n    'Extend the narrative, preserving the point of view and pacing.',\n    'Write what happens next, staying true to the characters.',\n    'Continue the story with concrete, immersive description.',\n    'Carry the narrative forward in the same literary register.',\n    'Write the next segment, matching the established rhythm.',\n    'Develop this scene further with authentic detail.',\n    'Push the story forward through action and dialogue.',\n    'Continue in the same voice, advancing the plot naturally.',\n    'Write the following passage in the style of the preceding text.',\n    'Extend this scene with attention to atmosphere and character.',\n    'Continue the narrative arc with engaging prose.',\n    'Write what comes next, maintaining tension and pacing.',\n    'Advance the story, weaving in environmental detail.',\n    'Continue with prose that matches the tone and texture of the original.',\n    'Develop the next beat of the story with precise language.',\n    'Carry the scene forward, balancing action with description.',\n]\n\n# ===== Test prompts =====\n# NOTE: Use single quotes for strings containing Chinese quotation marks\n_ZH_PROMPTS = [\n    '续写以下场景：\\n\\n暴雨如注，李明浑身湿透地站在破庙门口。庙里的火堆旁，一个蒙面人正用匕首削着木棍。两人目光相遇的瞬间，空气仿佛凝固了。\\n\\n请从李明的视角续写这个紧张的对峙场景，注意环境描写和人物心理。',\n    '以下是一段武侠小说的开头，请续写：\\n\\n月色如霜，照在悬崖边两道对峙的身影上。左边那人白衣胜雪，手中长剑微微颤动；右边那人一袭黑袍，双手背在身后，嘴角挂着一抹冷笑。\\n\\n\\u201c三年了，\\u201d白衣人开口，声音像是从牙缝里挤出来的，\\u201c你终于肯现身了。\\u201d\\n\\n续写这场决斗，要有招式描写和心理活动。',\n    '请用细腻的笔触描写以下场景：\\n\\n清晨的江南小镇刚刚苏醒。青石板路上还残留着昨夜的雨水，空气中弥漫着桂花和早点铺子里蒸笼的气息。一个背着书箱的年轻书生走过石桥，桥下有渔翁在收网。\\n\\n注意五感描写，营造宁静温暖的氛围。',\n]\n_EN_PROMPTS = [\n    'Continue this scene from the lighthouse keeper\\'s perspective:\\n\\nThe storm hit at midnight. Thomas pressed his face to the glass and watched the beam sweep across walls of black water. Then he saw it \\u2014 a flare, red and desperate, arcing up from somewhere beyond the reef.\\n\\nHe reached for the radio. Dead. The antenna had gone in the last gust.\\n\\nWrite the next 300 words. Focus on his decision-making, the physical environment, and building tension.',\n    'Continue this dialogue-driven scene:\\n\\nThe cafe was nearly empty. Rain streaked the windows, blurring the Paris streetlights into watercolor smears. Elena stirred her coffee for the third time without drinking it.\\n\\n\"You\\'re not here for the coffee,\" said the man across from her. He hadn\\'t touched his either.\\n\\n\"And you\\'re not here by accident,\" she replied.\\n\\nHe smiled \\u2014 not warmly. \"I know what you did in Lyon.\"\\n\\nContinue with tension-building dialogue. Reveal character through speech patterns and subtext, not exposition.',\n    'Write the next scene:\\n\\nAfter ten years of war, Commander Asha Renn walked through what remained of the village gate. The wooden arch was gone \\u2014 burned, she guessed, years ago. Where her mother\\'s garden had been, there was a blacksmith\\'s forge. A child she didn\\'t recognize stared at her scarred face with wide eyes.\\n\\n\"Are you a soldier?\" the child asked.\\n\\nContinue from Asha\\'s perspective. Balance external observation with internal emotion. Use specific sensory details to show how the village has changed.',\n]\n\nMODEL_CONFIGS = {\n    # ---- Free tier models (T4 / 12-16 GB VRAM) ----\n    'qwen3_4b': {\n        'model_name': 'unsloth/Qwen3-4B',\n        'output_name': 'qwen3_4b_novel_lora',\n        'system_prompt': _ZH_SYSTEM,\n        'test_prompts': _ZH_PROMPTS,\n        'lang': 'zh',\n        'min_vram_gb': 8,\n    },\n    'qwen3_8b': {\n        'model_name': 'unsloth/Qwen3-8B',\n        'output_name': 'qwen3_8b_novel_lora',\n        'system_prompt': _ZH_SYSTEM,\n        'test_prompts': _ZH_PROMPTS,\n        'lang': 'zh',\n        'min_vram_gb': 12,\n    },\n    'llama31_8b': {\n        'model_name': 'unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit',\n        'output_name': 'llama31_8b_novel_lora',\n        'system_prompt': _EN_SYSTEM,\n        'test_prompts': _EN_PROMPTS,\n        'lang': 'en',\n        'min_vram_gb': 12,\n    },\n    'gemma2_9b': {\n        'model_name': 'unsloth/gemma-2-9b-it-bnb-4bit',\n        'output_name': 'gemma2_9b_novel_lora',\n        'system_prompt': _EN_SYSTEM,\n        'test_prompts': _EN_PROMPTS,\n        'lang': 'en',\n        'min_vram_gb': 12,\n    },\n    'mistral_nemo_12b': {\n        'model_name': 'unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit',\n        'output_name': 'mistral_nemo_12b_novel_lora',\n        'system_prompt': _EN_SYSTEM,\n        'test_prompts': _EN_PROMPTS,\n        'lang': 'en',\n        'min_vram_gb': 12,\n    },\n    # ---- Larger models (L4/A10 / 24+ GB VRAM) ----\n    'phi4_14b': {\n        'model_name': 'unsloth/Phi-4-bnb-4bit',\n        'output_name': 'phi4_14b_novel_lora',\n        'system_prompt': _EN_SYSTEM,\n        'test_prompts': _EN_PROMPTS,\n        'lang': 'en',\n        'min_vram_gb': 24,\n    },\n    'qwen3_14b': {\n        'model_name': 'unsloth/Qwen3-14B',\n        'output_name': 'qwen3_14b_novel_lora',\n        'system_prompt': _ZH_SYSTEM,\n        'test_prompts': _ZH_PROMPTS,\n        'lang': 'zh',\n        'min_vram_gb': 24,\n    },\n    # ---- A100 models (40+ GB VRAM) ----\n    'qwen3_32b': {\n        'model_name': 'unsloth/Qwen3-32B',\n        'output_name': 'qwen3_32b_novel_lora',\n        'system_prompt': _ZH_SYSTEM,\n        'test_prompts': _ZH_PROMPTS,\n        'lang': 'zh',\n        'min_vram_gb': 40,\n    },\n}\n\nCFG = MODEL_CONFIGS[MODEL_CHOICE]\nprint(f\"Model: {CFG['model_name']}\")\nprint(f\"Language: {'Chinese' if CFG['lang'] == 'zh' else 'English'}\")\nprint(f\"Min VRAM: {CFG['min_vram_gb']} GB\")\nprint(f\"Upload mode: {UPLOAD_MODE}\")\nprint(f\"Output: {CFG['output_name']}\")\nprint(f\"Epochs: {NUM_EPOCHS}, LR: {LEARNING_RATE}, Seq len: {MAX_SEQ_LENGTH}\")\nprint(f\"LoRA rank: {LORA_RANK}, Batch: {BATCH_SIZE}, Grad accum: {GRADIENT_ACCUMULATION}\")\nprint(f\"NEFTune alpha: {NEFTUNE_ALPHA} {'(enabled)' if NEFTUNE_ALPHA > 0 else '(disabled)'}\")\nprint(f\"Weight decay: {WEIGHT_DECAY}\")\n\n# VRAM warning\nimport subprocess\ntry:\n    result = subprocess.run(['nvidia-smi', '--query-gpu=memory.total', '--format=csv,noheader,nounits'],\n                          capture_output=True, text=True)\n    gpu_vram = int(result.stdout.strip()) / 1024\n    if gpu_vram < CFG['min_vram_gb']:\n        print(f'\\nWARNING: Your GPU has ~{gpu_vram:.0f} GB VRAM but {MODEL_CHOICE} needs {CFG[\"min_vram_gb\"]} GB.')\n        print(f'   Consider using a smaller model or upgrading your Colab runtime.')\n    else:\n        print(f'\\nGPU VRAM: {gpu_vram:.0f} GB (requirement: {CFG[\"min_vram_gb\"]} GB)')\nexcept Exception:\n    pass"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Install Unsloth (2x faster training, 70% less VRAM)\n",
    "!pip install unsloth\n",
    "!pip install --force-reinstall --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git\n",
    "\n",
    "# Clone Novel Writer repo\n",
    "!rm -rf /content/Novel_Writer\n",
    "!git clone https://github.com/LL-LLLu/Novel_Writer.git /content/Novel_Writer\n",
    "\n",
    "# Install Novel Writer\n",
    "%cd /content/Novel_Writer\n",
    "!pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify installation\n",
    "import torch\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"VRAM: {torch.cuda.get_device_properties(0).total_mem / 1024**3:.1f} GB\")\n",
    "print()\n",
    "\n",
    "!novel-writer --help | head -20\n",
    "print(\"\\nSetup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Step 3: Upload Data\n\n**Three modes** (set `UPLOAD_MODE` in Step 1):\n\n| Mode | What to upload | Pipeline runs? |\n|------|---------------|----------------|\n| `upload_jsonl` | Your `train.jsonl` from local pipeline | No (fastest) |\n| `upload_raw` | Raw novel files (.txt, .epub, .html, etc.) | Yes |\n| `sample_data` | Nothing - uses built-in sample text | Yes |"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os, json\nfrom pathlib import Path\n\ndata_dir = Path(\"/content/Novel_Writer/data/raw\")\nprocessed_dir = Path(\"/content/Novel_Writer/data/processed\")\ndata_dir.mkdir(parents=True, exist_ok=True)\nprocessed_dir.mkdir(parents=True, exist_ok=True)\n\nSKIP_PIPELINE = False  # Will be set to True if user uploads train.jsonl directly\n\nif UPLOAD_MODE == \"upload_jsonl\":\n    # ====== FASTEST: Upload pre-processed train.jsonl directly ======\n    from google.colab import files as colab_files\n    print(\"Upload your train.jsonl file:\")\n    print(\"(This is the file from: data/processed/train.jsonl)\")\n    print()\n    uploaded = colab_files.upload()\n\n    for name, content in uploaded.items():\n        target = processed_dir / \"train.jsonl\"\n        with open(target, \"wb\") as f:\n            f.write(content)\n        # Validate it's valid JSONL\n        with open(target, \"r\", encoding=\"utf-8\") as f:\n            lines = f.readlines()\n        sample = json.loads(lines[0])\n        print(f\"\\n✓ Uploaded {name} -> {target}\")\n        print(f\"  Entries: {len(lines)}\")\n        print(f\"  Keys: {list(sample.keys())}\")\n        print(f\"  Preview: {sample.get('output', '')[:100]}...\")\n\n    TRAIN_FILE = str(processed_dir / \"train.jsonl\")\n    SKIP_PIPELINE = True\n    print(f\"\\n✓ Pipeline will be SKIPPED (data already processed)\")\n\nelif UPLOAD_MODE == \"upload_raw\":\n    # ====== Upload raw novel files, pipeline runs on Colab ======\n    from google.colab import files as colab_files\n    print(\"Upload your novel files (.txt, .pdf, .epub, .html, .md, .mobi):\")\n    print(\"(Click 'Choose Files' button below)\")\n    print()\n    uploaded = colab_files.upload()\n\n    for name, content in uploaded.items():\n        target = data_dir / name\n        with open(target, \"wb\") as f:\n            f.write(content)\n        print(f\"  Saved: {name} ({len(content):,} bytes)\")\n\n    print(f\"\\nTotal files: {len(list(data_dir.iterdir()))}\")\n    print(\"Pipeline will process these into training data.\")\n\nelif UPLOAD_MODE == \"sample_data\":\n    # ====== Built-in sample data for testing ======\n    if CFG[\"lang\"] == \"zh\":\n        sample_text = \"\"\"\n第1章 黎明之前\n\n天还没有亮，整个村庄都笼罩在一片寂静之中。远处的山峦在薄雾中若隐若现，仿佛一幅淡墨山水画。\n李明站在院子里，深深地吸了一口清晨的空气。今天是个特别的日子，他已经等了整整三年。\n\n\"你真的要走吗？\"身后传来母亲苍老的声音。\n\n李明没有回头，他知道如果回头，自己可能就再也走不了了。\"妈，我会回来的。\"\n\n他的声音很轻，却在寂静的清晨显得格外清晰。母亲没有再说什么，只是默默地将一个包袱递到他手中。\n包袱不重，但李明知道里面装着母亲所有的心意——几件换洗的衣裳，几个烙饼，还有父亲留下的那把短刀。\n\n\"路上小心。\"母亲终于开口，声音有些颤抖。\n\n李明点了点头，背起包袱，向村口走去。晨雾渐渐散开，东方的天际泛起了一抹鱼肚白。\n他知道，从这一刻起，一切都将不同。\n\n村口的老槐树下，站着一个人。那是张叔，村里的铁匠，也是李明的师父。\n\"小子，过来。\"张叔的声音粗犷却不失温暖。\n\n李明走上前去，张叔从身后拿出一个长条形的布包，递给他。\n\"这是我打了三个月的剑，虽然比不上名家之作，但也算是我的心血。带着它，路上好歹有个防身的。\"\n\n李明双手接过，感觉到剑身的重量和温度。他深深鞠了一躬：\"谢谢师父。\"\n\n张叔拍了拍他的肩膀：\"去吧，长安城在等着你。记住，不管遇到什么，都别忘了你是谁。\"\n\n第2章 远行\n\n一路向西，李明走了整整七天。他穿过了无数个村庄和城镇，见识了各种各样的人和事。\n有热情好客的农家，有精明狡猾的商人，也有孤独的旅人。每个人都有自己的故事，每个故事都让他对这个世界有了新的认识。\n\n第三天的黄昏，他来到了一个叫做青石镇的地方。镇子不大，但街道整洁，店铺林立。\n最引人注目的是镇中央的一座酒楼，名叫\"醉仙居\"，三层高的木楼在夕阳下散发着暖黄色的光芒。\n\n\"年轻人，来住店吗？\"酒楼门口的小二热情地招呼道。\n\n李明摸了摸怀里所剩不多的铜钱，犹豫了一下。这七天来，他大多睡在路边的破庙或者好心人家的柴房里。\n一顿像样的饭菜和一张温暖的床，对他来说已经是奢侈品了。\n\n\"住一晚多少钱？\"他问道。\n\"上房一两银子，普通间三百文，通铺一百文。\"小二笑着回答。\n\"通铺吧。\"李明走了进去。\n\n酒楼里很热闹，各种各样的人聚在一起吃饭喝酒。李明找了个角落坐下，要了一碗面和一壶茶。\n邻桌坐着几个江湖模样的人，正在大声地讨论着什么。\n\n\"你们听说了吗？长安城最近出了大事！\"一个络腮胡子的大汉压低声音说道，但他的声音依然传遍了半个酒楼。\n\"什么大事？\"同桌的一个瘦高个急忙问道。\n\"据说天机阁的阁主失踪了，整个武林都在找他。\"络腮胡子神秘兮兮地说。\n\n李明的耳朵动了动。天机阁，那是他父亲生前常常提起的地方。\n\n月光从窗户洒进来，照在他年轻而坚毅的脸上。明天，他就要继续赶路了。\n长安，等着我。他在心中默念，然后闭上了眼睛。\n\"\"\"\n        for i in range(3):\n            (data_dir / f\"sample_novel_{i+1}.txt\").write_text(\n                sample_text.replace(\"李明\", [\"李明\", \"王刚\", \"赵云\"][i]),\n                encoding=\"utf-8\"\n            )\n    else:\n        sample_text = \"\"\"\nChapter 1: The Last Light\n\nThe old lighthouse stood at the edge of the world, or so it seemed to Thomas Gray.\nFor forty years he had climbed these stairs each evening, lit the great lamp, and watched\nits beam sweep across the dark Atlantic waters.\n\n\"One more night,\" he muttered to himself, a habit born of decades without anyone else\nto talk to. \"Just one more.\"\n\nThe lamp room at the top was warm despite the storm. Thomas had maintained the old\nFresnel lens with religious devotion. The Coast Guard had wanted to automate the light\nyears ago, replace him with sensors and timers. He had fought them tooth and nail.\n\nThomas struck the match and touched it to the wick. The flame caught, small at first,\nthen growing as the oil drew upward. He watched the light bloom and multiply through\nthe precision-cut prisms until it became something powerful, something that could reach\nacross miles of angry ocean to tell a sailor: you are not alone.\n\nHe settled into his chair and opened his logbook. \"November 17th,\" he wrote. \"Wind\nnortheast, 45 knots gusting to 60. Rain heavy. Visibility poor.\" He paused, pen\nhovering over the page. Then he added: \"Final entry.\"\n\nChapter 2: The Storm\n\nSarah Chen had not planned to be at sea tonight. The storm had come on fast, much\nfaster than the forecast predicted. So she did what sailors do: she shortened sail,\nlashed everything down, clipped her harness to the jackline, and held on.\n\nThrough the rain, through the spray, through the chaos of wind and wave, she saw\nit — a light. Sweeping across the water in a steady, ancient rhythm. The lighthouse.\n\n\"Thank God,\" she breathed, and for the first time in hours, she knew where she was.\n\"\"\"\n        for i in range(3):\n            names = [(\"Thomas Gray\", \"Sarah Chen\"), (\"James Walker\", \"Maria Santos\"), (\"Robert Kim\", \"Elena Volkov\")]\n            text = sample_text.replace(\"Thomas Gray\", names[i][0]).replace(\"Sarah Chen\", names[i][1])\n            (data_dir / f\"sample_novel_{i+1}.txt\").write_text(text, encoding=\"utf-8\")\n\n    print(f\"Created sample data in {data_dir}:\")\n    for f in sorted(data_dir.iterdir()):\n        if f.is_file():\n            print(f\"  {f.name} ({f.stat().st_size:,} bytes)\")\n\nprint(f\"\\nSkip pipeline: {SKIP_PIPELINE}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Step 4: Run Data Processing Pipeline\n\nRuns the full pipeline: **clean** > **format** (to JSONL) > **deduplicate** > **quality filter**\n\n*This step is automatically skipped if you uploaded `train.jsonl` directly.*"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if SKIP_PIPELINE:\n    print(\"✓ Skipping pipeline (train.jsonl was uploaded directly)\")\n    print(f\"  Using: {TRAIN_FILE}\")\nelse:\n    import yaml\n    from pathlib import Path\n\n    config = {\n        \"data\": {\n            \"input_dir\": \"data/raw\",\n            \"output_dir\": \"data/processed\",\n            \"temp_dir\": \"data/processed/temp_cleaned\",\n            \"chunk_size\": CHUNK_SIZE,\n            \"overlap\": 500,\n        },\n        \"log_level\": \"INFO\",\n    }\n\n    with open(\"/content/Novel_Writer/config.yaml\", \"w\") as f:\n        yaml.dump(config, f, default_flow_style=False)\n\n    print(\"Config written. Running pipeline...\\n\")\n\n    cmd = \"novel-writer -v pipeline --clean\"\n\n    ingest_exts = {\".epub\", \".html\", \".htm\", \".md\", \".mobi\"}\n    has_ingestable = any(f.suffix.lower() in ingest_exts for f in Path(\"data/raw\").iterdir() if f.is_file())\n    if has_ingestable:\n        cmd += \" --ingest\"\n    if RUN_DEDUP:\n        cmd += \" --deduplicate\"\n    if RUN_QUALITY_FILTER:\n        cmd += \" --filter\"\n\n    print(f\"Command: {cmd}\\n\")\n    !{cmd}"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import json\nfrom pathlib import Path\n\nif not SKIP_PIPELINE:\n    # Find the final JSONL file produced by pipeline\n    processed_dir = Path(\"data/processed\")\n    jsonl_files = sorted(processed_dir.glob(\"*.jsonl\"), key=lambda f: f.stat().st_mtime, reverse=True)\n\n    if not jsonl_files:\n        raise FileNotFoundError(\"No JSONL files produced! Check pipeline output above.\")\n\n    TRAIN_FILE = str(jsonl_files[0])\n\n# Validate the training file\nwith open(TRAIN_FILE, \"r\", encoding=\"utf-8\") as f:\n    lines = f.readlines()\n\nprint(f\"Training data: {TRAIN_FILE}\")\nprint(f\"Total entries: {len(lines)}\")\n\nif lines:\n    sample = json.loads(lines[0])\n    print(f\"Keys: {list(sample.keys())}\")\n    print(f\"Output preview: {sample.get('output', '')[:200]}...\")\nelse:\n    raise ValueError(\"Training file is empty!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: Load Model & Configure LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "print(f\"Loading model: {CFG['model_name']}\")\n",
    "print(f\"Max sequence length: {MAX_SEQ_LENGTH}\")\n",
    "print(f\"4-bit quantization: True\\n\")\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=CFG[\"model_name\"],\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    dtype=None,          # Auto-detect\n",
    "    load_in_4bit=True,   # QLoRA\n",
    ")\n",
    "\n",
    "print(f\"\\nGPU memory after loading: {torch.cuda.memory_allocated() / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=LORA_RANK,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha=LORA_RANK // 2,\n",
    "    lora_dropout=0,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=3407,\n",
    ")\n",
    "\n",
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Trainable parameters: {trainable:,} / {total:,} ({100 * trainable / total:.2f}%)\")\n",
    "print(f\"GPU memory with LoRA: {torch.cuda.memory_allocated() / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 6: Prepare Dataset for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import random\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"json\", data_files=TRAIN_FILE, split=\"train\")\n\n# Diversify instructions if they're all the same (common with pipeline-generated data)\nunique_instructions = set(dataset[\"instruction\"])\nif len(unique_instructions) <= 2:\n    print(f\"Found only {len(unique_instructions)} unique instruction(s) - diversifying...\")\n    instruction_pool = _ZH_INSTRUCTIONS if CFG[\"lang\"] == \"zh\" else _EN_INSTRUCTIONS\n\n    def diversify_instructions(examples):\n        new_instructions = [random.choice(instruction_pool) for _ in examples[\"instruction\"]]\n        return {\"instruction\": new_instructions}\n\n    dataset = dataset.map(diversify_instructions, batched=True)\n    new_unique = len(set(dataset[\"instruction\"]))\n    print(f\"  Diversified to {new_unique} unique instructions\")\nelse:\n    print(f\"Instructions already diverse ({len(unique_instructions)} unique)\")\n\n# Train/validation split\nif len(dataset) > 10:\n    split = dataset.train_test_split(test_size=0.1, seed=42)\n    train_dataset = split[\"train\"]\n    eval_dataset = split[\"test\"]\nelse:\n    train_dataset = dataset\n    eval_dataset = None\n    print(\"Dataset too small for validation split, training on all data.\")\n\nprint(f\"Training samples: {len(train_dataset)}\")\nif eval_dataset:\n    print(f\"Validation samples: {len(eval_dataset)}\")\n\n# Universal formatting using tokenizer's built-in chat template\n# This works for ALL models (Qwen, Llama, Mistral, Gemma, Phi, etc.)\ndef formatting_func(examples):\n    instructions = examples[\"instruction\"]\n    outputs = examples[\"output\"]\n    texts = []\n    for instruction, output in zip(instructions, outputs):\n        messages = [\n            {\"role\": \"system\", \"content\": CFG[\"system_prompt\"]},\n            {\"role\": \"user\", \"content\": instruction},\n            {\"role\": \"assistant\", \"content\": output},\n        ]\n        # apply_chat_template handles the correct format for each model family\n        try:\n            text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n        except Exception:\n            # Fallback for models without system role support\n            messages_no_sys = [\n                {\"role\": \"user\", \"content\": CFG[\"system_prompt\"] + \"\\n\\n\" + instruction},\n                {\"role\": \"assistant\", \"content\": output},\n            ]\n            text = tokenizer.apply_chat_template(messages_no_sys, tokenize=False, add_generation_prompt=False)\n        texts.append(text)\n    return {\"text\": texts}\n\ntrain_dataset = train_dataset.map(formatting_func, batched=True)\nif eval_dataset:\n    eval_dataset = eval_dataset.map(formatting_func, batched=True)\n\nprint(f\"\\n--- Sample formatted entry ---\")\nprint(train_dataset[0][\"text\"][:600])\nprint(\"...\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 7: Train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from trl import SFTTrainer\nfrom transformers import TrainingArguments, EarlyStoppingCallback\n\ntraining_args = TrainingArguments(\n    output_dir=f\"checkpoints_{CFG['output_name']}\",\n    num_train_epochs=NUM_EPOCHS,\n    per_device_train_batch_size=BATCH_SIZE,\n    gradient_accumulation_steps=GRADIENT_ACCUMULATION,\n    warmup_ratio=0.1,\n    learning_rate=LEARNING_RATE,\n    weight_decay=WEIGHT_DECAY,\n    lr_scheduler_type=\"cosine\",\n    fp16=not torch.cuda.is_bf16_supported(),\n    bf16=torch.cuda.is_bf16_supported(),\n    logging_steps=5,\n    save_strategy=\"steps\" if eval_dataset else \"epoch\",\n    save_steps=50 if eval_dataset else None,\n    save_total_limit=3,\n    seed=3407,\n)\n\n# Add eval settings if we have validation data\nif eval_dataset:\n    training_args.eval_strategy = \"steps\"\n    training_args.eval_steps = 50\n    training_args.load_best_model_at_end = True\n    training_args.metric_for_best_model = \"eval_loss\"\n    training_args.greater_is_better = False\n\ncallbacks = []\nif eval_dataset:\n    callbacks.append(EarlyStoppingCallback(early_stopping_patience=3))\n\ntrainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    dataset_text_field=\"text\",\n    max_seq_length=MAX_SEQ_LENGTH,\n    dataset_num_proc=2,\n    packing=False,\n    neftune_noise_alpha=NEFTUNE_ALPHA if NEFTUNE_ALPHA > 0 else None,\n    args=training_args,\n    callbacks=callbacks if callbacks else None,\n)\n\nnef_status = f\"NEFTune alpha={NEFTUNE_ALPHA}\" if NEFTUNE_ALPHA > 0 else \"NEFTune disabled\"\nprint(f\"Starting training: {NUM_EPOCHS} epochs, {len(train_dataset)} samples\")\nprint(f\"Effective batch size: {BATCH_SIZE * GRADIENT_ACCUMULATION}\")\nprint(f\"Estimated steps: {len(train_dataset) * NUM_EPOCHS // (BATCH_SIZE * GRADIENT_ACCUMULATION)}\")\nprint(f\"LR schedule: cosine, Weight decay: {WEIGHT_DECAY}, {nef_status}\")\nprint(\"=\"*60)\n\nstats = trainer.train()\n\nprint(\"=\"*60)\nprint(f\"Training complete!\")\nprint(f\"  Total steps: {stats.global_step}\")\nprint(f\"  Training loss: {stats.training_loss:.4f}\")\nprint(f\"  Runtime: {stats.metrics['train_runtime']:.0f} seconds\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 8: Test Generation\n",
    "\n",
    "Let's see what the fine-tuned model can do!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "FastLanguageModel.for_inference(model)\n\nprint(f\"Generating with {CFG['model_name']}...\\n\")\n\nfor i, prompt in enumerate(CFG[\"test_prompts\"]):\n    # Universal chat template approach - works for all models\n    messages = [\n        {\"role\": \"system\", \"content\": CFG[\"system_prompt\"]},\n        {\"role\": \"user\", \"content\": prompt},\n    ]\n    try:\n        inputs = tokenizer.apply_chat_template(\n            messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\"\n        ).to(\"cuda\")\n    except Exception:\n        # Fallback for models without system role\n        messages_no_sys = [\n            {\"role\": \"user\", \"content\": CFG[\"system_prompt\"] + \"\\n\\n\" + prompt},\n        ]\n        inputs = tokenizer.apply_chat_template(\n            messages_no_sys, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\"\n        ).to(\"cuda\")\n\n    input_len = inputs.shape[-1]\n\n    outputs = model.generate(\n        input_ids=inputs,\n        max_new_tokens=512,\n        temperature=0.8,\n        top_p=0.9,\n        top_k=50,\n        do_sample=True,\n        repetition_penalty=1.1,\n    )\n    response = tokenizer.decode(outputs[0][input_len:], skip_special_tokens=True)\n\n    print(f\"{'='*60}\")\n    print(f\"Prompt {i+1}: {prompt}\")\n    print(f\"{'='*60}\")\n    print(response)\n    print(f\"[{len(response)} chars]\\n\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 9: Save & Download Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_name = CFG[\"output_name\"]\n",
    "\n",
    "# Save LoRA adapters\n",
    "model.save_pretrained(output_name)\n",
    "tokenizer.save_pretrained(output_name)\n",
    "print(f\"Model saved to {output_name}/\")\n",
    "\n",
    "# Show saved files\n",
    "import os\n",
    "total_size = 0\n",
    "for f in sorted(Path(output_name).rglob(\"*\")):\n",
    "    if f.is_file():\n",
    "        size = f.stat().st_size\n",
    "        total_size += size\n",
    "        print(f\"  {f.name}: {size / 1024 / 1024:.1f} MB\")\n",
    "print(f\"\\nTotal size: {total_size / 1024 / 1024:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download as zip\n",
    "!zip -r {output_name}.zip {output_name}/\n",
    "\n",
    "from google.colab import files as colab_files\n",
    "colab_files.download(f\"{output_name}.zip\")\n",
    "print(f\"\\nDownloading {output_name}.zip ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 10 (Optional): Save to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment these lines to save to Google Drive\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount(\"/content/drive\")\n",
    "#\n",
    "# import shutil\n",
    "# drive_path = f\"/content/drive/MyDrive/{output_name}\"\n",
    "# shutil.copytree(output_name, drive_path, dirs_exist_ok=True)\n",
    "# print(f\"Saved to Google Drive: {drive_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 11 (Optional): Export to GGUF for Local Use\n",
    "\n",
    "Export your model to GGUF format for running locally with **Ollama** or **llama.cpp**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to export to GGUF (takes ~10-15 minutes)\n",
    "\n",
    "# gguf_name = f\"{output_name}_gguf\"\n",
    "# model.save_pretrained_gguf(\n",
    "#     gguf_name,\n",
    "#     tokenizer,\n",
    "#     quantization_method=\"q4_k_m\",  # Good balance of quality vs size\n",
    "# )\n",
    "#\n",
    "# from google.colab import files as colab_files\n",
    "# gguf_file = list(Path(gguf_name).glob(\"*.gguf\"))[0]\n",
    "# colab_files.download(str(gguf_file))\n",
    "# print(f\"GGUF exported! Run locally with:\")\n",
    "# print(f\"  ollama run ./{gguf_file.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Done!\n",
    "\n",
    "Your fine-tuned model has been saved. To use it locally with the Novel Writer CLI:\n",
    "\n",
    "```bash\n",
    "# Unzip your downloaded model\n",
    "unzip qwen3_chinese_novel_lora.zip  # or nemo_english_story_lora.zip\n",
    "\n",
    "# Generate text\n",
    "novel-writer generate --prompt \"Your prompt here...\" --model qwen3_chinese_novel_lora\n",
    "```\n",
    "\n",
    "Or start the API server:\n",
    "```bash\n",
    "python -m novel_writer.api\n",
    "# POST to http://localhost:8000/generate\n",
    "```"
   ]
  }
 ]
}