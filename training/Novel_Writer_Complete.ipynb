{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Novel Writer - Complete Pipeline & Training\n\nThis notebook runs **everything** end-to-end on Google Colab:\n\n1. Clone repo & install dependencies\n2. Upload your novels (or use built-in sample data)\n3. Run the full data processing pipeline\n4. Fine-tune your chosen model\n5. Generate sample text\n6. Download your trained model\n\n### Supported Models\n\n| Model | Params | Best For | Min GPU | Free Tier? |\n|-------|--------|----------|---------|------------|\n| Qwen3-4B | 4B | Chinese (lightweight) | 8GB (T4) | Yes |\n| Qwen3-8B | 8B | Chinese | 12GB (T4) | Yes |\n| Llama 3.1 8B | 8B | English | 12GB (T4) | Yes |\n| Gemma 2 9B | 9B | English | 12GB (T4) | Yes |\n| Mistral Nemo 12B | 12B | English creative writing | 12GB (T4) | Yes |\n| Phi-4 14B | 14B | English (reasoning + writing) | 24GB (L4/A10) | Kaggle 2xT4 |\n| Qwen3-14B | 14B | Chinese + English | 24GB (L4/A10) | Kaggle 2xT4 |\n| Qwen3-32B | 32B | Chinese + English (best quality) | 40GB (A100) | No |\n\n**Requirements:** Google Colab with T4 GPU (free tier works for 4B-12B models)\n\n---\n\n### How to use\n1. Pick your model in **Cell 1** below\n2. **Runtime > Run all**\n3. When prompted, upload your novel files (or skip to use sample data)\n4. Wait for training to complete (~1-3 hours depending on data size)\n5. Download your LoRA adapters at the end"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: Configuration\n",
    "\n",
    "**Change these settings before running!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#@title Configuration { display-mode: \"form\" }\n\n#@markdown ### Model Selection\n#@markdown > **Free tier (T4):** qwen3_4b, qwen3_8b, llama31_8b, gemma2_9b, mistral_nemo_12b\n#@markdown >\n#@markdown > **Paid/Kaggle (L4+):** phi4_14b, qwen3_14b\n#@markdown >\n#@markdown > **A100 only:** qwen3_32b\nMODEL_CHOICE = \"qwen3_8b\" #@param [\"qwen3_4b\", \"qwen3_8b\", \"llama31_8b\", \"gemma2_9b\", \"mistral_nemo_12b\", \"phi4_14b\", \"qwen3_14b\", \"qwen3_32b\"]\n\n#@markdown ### Training Settings\nNUM_EPOCHS = 2 #@param {type:\"slider\", min:1, max:5, step:1}\nLEARNING_RATE = 2e-4 #@param {type:\"number\"}\nMAX_SEQ_LENGTH = 4096 #@param [2048, 4096, 8192] {type:\"raw\"}\nLORA_RANK = 32 #@param [8, 16, 32, 64] {type:\"raw\"}\nBATCH_SIZE = 2 #@param [1, 2, 4] {type:\"raw\"}\nGRADIENT_ACCUMULATION = 4 #@param [2, 4, 8] {type:\"raw\"}\n\n#@markdown ### Data Settings\nUSE_SAMPLE_DATA = False #@param {type:\"boolean\"}\nCHUNK_SIZE = 4000 #@param {type:\"integer\"}\nRUN_DEDUP = True #@param {type:\"boolean\"}\nRUN_QUALITY_FILTER = True #@param {type:\"boolean\"}\n\n#@markdown ---\n\n# ===== Model configurations =====\n# Chinese test prompts\n_ZH_PROMPTS = [\n    \"续写以下故事：李明站在长安城门前，心中百感交集。三年前他离开家乡时还是个少年，如今\",\n    \"描写一个武侠场景：月光下，两位剑客在悬崖边对峙。\",\n    \"请以古风笔触描写一个春日清晨的集市。\",\n]\n# English test prompts\n_EN_PROMPTS = [\n    \"Continue the story: The old lighthouse keeper climbed the spiral stairs one last time. After forty years, tonight would be his final watch.\",\n    \"Write a scene: Two strangers meet in a rain-soaked cafe in Paris. One of them is hiding a secret.\",\n    \"Describe the moment a warrior returns to her village after a decade of war, only to find it completely changed.\",\n]\n\nMODEL_CONFIGS = {\n    # ---- Free tier models (T4 / 12-16 GB VRAM) ----\n    \"qwen3_4b\": {\n        \"model_name\": \"unsloth/Qwen3-4B\",\n        \"output_name\": \"qwen3_4b_novel_lora\",\n        \"system_prompt\": \"你是一位专业的中文小说作家。请根据指令，以优美流畅的中文续写故事内容。注意保持文风一致，人物性格鲜明，情节引人入胜。\",\n        \"test_prompts\": _ZH_PROMPTS,\n        \"lang\": \"zh\",\n        \"min_vram_gb\": 8,\n    },\n    \"qwen3_8b\": {\n        \"model_name\": \"unsloth/Qwen3-8B\",\n        \"output_name\": \"qwen3_8b_novel_lora\",\n        \"system_prompt\": \"你是一位专业的中文小说作家。请根据指令，以优美流畅的中文续写故事内容。注意保持文风一致，人物性格鲜明，情节引人入胜。\",\n        \"test_prompts\": _ZH_PROMPTS,\n        \"lang\": \"zh\",\n        \"min_vram_gb\": 12,\n    },\n    \"llama31_8b\": {\n        \"model_name\": \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n        \"output_name\": \"llama31_8b_novel_lora\",\n        \"system_prompt\": \"You are a talented fiction author. Write vivid, engaging prose with strong characters, sensory details, and natural dialogue. Continue the narrative in the established style.\",\n        \"test_prompts\": _EN_PROMPTS,\n        \"lang\": \"en\",\n        \"min_vram_gb\": 12,\n    },\n    \"gemma2_9b\": {\n        \"model_name\": \"unsloth/gemma-2-9b-it-bnb-4bit\",\n        \"output_name\": \"gemma2_9b_novel_lora\",\n        \"system_prompt\": \"You are a talented fiction author. Write vivid, engaging prose with strong characters, sensory details, and natural dialogue. Continue the narrative in the established style.\",\n        \"test_prompts\": _EN_PROMPTS,\n        \"lang\": \"en\",\n        \"min_vram_gb\": 12,\n    },\n    \"mistral_nemo_12b\": {\n        \"model_name\": \"unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit\",\n        \"output_name\": \"mistral_nemo_12b_novel_lora\",\n        \"system_prompt\": \"You are a talented fiction author. Write vivid, engaging prose with strong characters, sensory details, and natural dialogue.\",\n        \"test_prompts\": _EN_PROMPTS,\n        \"lang\": \"en\",\n        \"min_vram_gb\": 12,\n    },\n    # ---- Larger models (L4/A10 / 24+ GB VRAM) ----\n    \"phi4_14b\": {\n        \"model_name\": \"unsloth/Phi-4-bnb-4bit\",\n        \"output_name\": \"phi4_14b_novel_lora\",\n        \"system_prompt\": \"You are a talented fiction author. Write vivid, engaging prose with strong characters, sensory details, and natural dialogue. Continue the narrative in the established style.\",\n        \"test_prompts\": _EN_PROMPTS,\n        \"lang\": \"en\",\n        \"min_vram_gb\": 24,\n    },\n    \"qwen3_14b\": {\n        \"model_name\": \"unsloth/Qwen3-14B\",\n        \"output_name\": \"qwen3_14b_novel_lora\",\n        \"system_prompt\": \"你是一位专业的中文小说作家。请根据指令，以优美流畅的中文续写故事内容。注意保持文风一致，人物性格鲜明，情节引人入胜。\",\n        \"test_prompts\": _ZH_PROMPTS,\n        \"lang\": \"zh\",\n        \"min_vram_gb\": 24,\n    },\n    # ---- A100 models (40+ GB VRAM) ----\n    \"qwen3_32b\": {\n        \"model_name\": \"unsloth/Qwen3-32B\",\n        \"output_name\": \"qwen3_32b_novel_lora\",\n        \"system_prompt\": \"你是一位专业的中文小说作家。请根据指令，以优美流畅的中文续写故事内容。注意保持文风一致，人物性格鲜明，情节引人入胜。\",\n        \"test_prompts\": _ZH_PROMPTS,\n        \"lang\": \"zh\",\n        \"min_vram_gb\": 40,\n    },\n}\n\nCFG = MODEL_CONFIGS[MODEL_CHOICE]\nprint(f\"Model: {CFG['model_name']}\")\nprint(f\"Language: {'Chinese' if CFG['lang'] == 'zh' else 'English'}\")\nprint(f\"Min VRAM: {CFG['min_vram_gb']} GB\")\nprint(f\"Output: {CFG['output_name']}\")\nprint(f\"Epochs: {NUM_EPOCHS}, LR: {LEARNING_RATE}, Seq len: {MAX_SEQ_LENGTH}\")\nprint(f\"LoRA rank: {LORA_RANK}, Batch: {BATCH_SIZE}, Grad accum: {GRADIENT_ACCUMULATION}\")\nprint(f\"Sample data: {USE_SAMPLE_DATA}\")\n\n# VRAM warning\nimport subprocess\ntry:\n    result = subprocess.run([\"nvidia-smi\", \"--query-gpu=memory.total\", \"--format=csv,noheader,nounits\"],\n                          capture_output=True, text=True)\n    gpu_vram = int(result.stdout.strip()) / 1024  # Convert MiB to GiB\n    if gpu_vram < CFG[\"min_vram_gb\"]:\n        print(f\"\\n⚠️  WARNING: Your GPU has ~{gpu_vram:.0f} GB VRAM but {MODEL_CHOICE} needs {CFG['min_vram_gb']} GB.\")\n        print(f\"   Consider using a smaller model or upgrading your Colab runtime.\")\n    else:\n        print(f\"\\n✓ GPU VRAM: {gpu_vram:.0f} GB (requirement: {CFG['min_vram_gb']} GB)\")\nexcept Exception:\n    pass"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Install Unsloth (2x faster training, 70% less VRAM)\n",
    "!pip install unsloth\n",
    "!pip install --force-reinstall --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git\n",
    "\n",
    "# Clone Novel Writer repo\n",
    "!rm -rf /content/Novel_Writer\n",
    "!git clone https://github.com/LL-LLLu/Novel_Writer.git /content/Novel_Writer\n",
    "\n",
    "# Install Novel Writer\n",
    "%cd /content/Novel_Writer\n",
    "!pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify installation\n",
    "import torch\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"VRAM: {torch.cuda.get_device_properties(0).total_mem / 1024**3:.1f} GB\")\n",
    "print()\n",
    "\n",
    "!novel-writer --help | head -20\n",
    "print(\"\\nSetup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Upload Data (or Use Sample Data)\n",
    "\n",
    "**Supported formats:** `.txt`, `.pdf`, `.epub`, `.html`, `.htm`, `.md`, `.mobi`\n",
    "\n",
    "Upload your novel files when prompted, or check `USE_SAMPLE_DATA = True` in the config above to use built-in sample text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nfrom pathlib import Path\n\ndata_dir = Path(\"/content/Novel_Writer/data/raw\")\ndata_dir.mkdir(parents=True, exist_ok=True)\n\nif USE_SAMPLE_DATA:\n    # Create sample data so the pipeline has something to process\n    if CFG[\"lang\"] == \"zh\":\n        sample_text = \"\"\"\n第1章 黎明之前\n\n天还没有亮，整个村庄都笼罩在一片寂静之中。远处的山峦在薄雾中若隐若现，仿佛一幅淡墨山水画。\n李明站在院子里，深深地吸了一口清晨的空气。今天是个特别的日子，他已经等了整整三年。\n\n\"你真的要走吗？\"身后传来母亲苍老的声音。\n\n李明没有回头，他知道如果回头，自己可能就再也走不了了。\"妈，我会回来的。\"\n\n他的声音很轻，却在寂静的清晨显得格外清晰。母亲没有再说什么，只是默默地将一个包袱递到他手中。\n包袱不重，但李明知道里面装着母亲所有的心意——几件换洗的衣裳，几个烙饼，还有父亲留下的那把短刀。\n\n\"路上小心。\"母亲终于开口，声音有些颤抖。\n\n李明点了点头，背起包袱，向村口走去。晨雾渐渐散开，东方的天际泛起了一抹鱼肚白。\n他知道，从这一刻起，一切都将不同。\n\n村口的老槐树下，站着一个人。那是张叔，村里的铁匠，也是李明的师父。\n\"小子，过来。\"张叔的声音粗犷却不失温暖。\n\n李明走上前去，张叔从身后拿出一个长条形的布包，递给他。\n\"这是我打了三个月的剑，虽然比不上名家之作，但也算是我的心血。带着它，路上好歹有个防身的。\"\n\n李明双手接过，感觉到剑身的重量和温度。他深深鞠了一躬：\"谢谢师父。\"\n\n张叔拍了拍他的肩膀：\"去吧，长安城在等着你。记住，不管遇到什么，都别忘了你是谁。\"\n\n第2章 远行\n\n一路向西，李明走了整整七天。他穿过了无数个村庄和城镇，见识了各种各样的人和事。\n有热情好客的农家，有精明狡猾的商人，也有孤独的旅人。每个人都有自己的故事，每个故事都让他对这个世界有了新的认识。\n\n第三天的黄昏，他来到了一个叫做青石镇的地方。镇子不大，但街道整洁，店铺林立。\n最引人注目的是镇中央的一座酒楼，名叫\"醉仙居\"，三层高的木楼在夕阳下散发着暖黄色的光芒。\n\n\"年轻人，来住店吗？\"酒楼门口的小二热情地招呼道。\n\n李明摸了摸怀里所剩不多的铜钱，犹豫了一下。这七天来，他大多睡在路边的破庙或者好心人家的柴房里。\n一顿像样的饭菜和一张温暖的床，对他来说已经是奢侈品了。\n\n\"住一晚多少钱？\"他问道。\n\"上房一两银子，普通间三百文，通铺一百文。\"小二笑着回答。\n\"通铺吧。\"李明走了进去。\n\n酒楼里很热闹，各种各样的人聚在一起吃饭喝酒。李明找了个角落坐下，要了一碗面和一壶茶。\n邻桌坐着几个江湖模样的人，正在大声地讨论着什么。\n\n\"你们听说了吗？长安城最近出了大事！\"一个络腮胡子的大汉压低声音说道，但他的声音依然传遍了半个酒楼。\n\"什么大事？\"同桌的一个瘦高个急忙问道。\n\"据说天机阁的阁主失踪了，整个武林都在找他。\"络腮胡子神秘兮兮地说。\n\n李明的耳朵动了动。天机阁，那是他父亲生前常常提起的地方。\n\"天机阁掌控着天下武林的情报，阁主失踪，意味着很多秘密可能会被泄露。\"络腮胡子继续说道。\n\"各大门派都派了人去长安，明面上说是帮忙寻找，实际上嘛……\"他意味深长地笑了笑。\n\n李明默默地喝着面汤，心中却掀起了波澜。他去长安，本来只是为了完成父亲的遗愿。\n但现在看来，长安城远比他想象的要复杂得多。\n\n吃完饭，李明回到通铺。房间里已经躺了几个人，鼾声此起彼伏。\n他找了个靠墙的位置躺下，将包袱垫在头下，张叔给的剑放在伸手可及的地方。\n\n月光从窗户洒进来，照在他年轻而坚毅的脸上。明天，他就要继续赶路了。\n长安，等着我。他在心中默念，然后闭上了眼睛。\n\"\"\"\n        for i in range(3):  # Repeat to make dataset larger\n            (data_dir / f\"sample_novel_{i+1}.txt\").write_text(\n                sample_text.replace(\"李明\", [\"李明\", \"王刚\", \"赵云\"][i]),\n                encoding=\"utf-8\"\n            )\n    else:\n        sample_text = \"\"\"\nChapter 1: The Last Light\n\nThe old lighthouse stood at the edge of the world, or so it seemed to Thomas Gray.\nFor forty years he had climbed these stairs each evening, lit the great lamp, and watched\nits beam sweep across the dark Atlantic waters. Forty years of storms and calms, of ships\nsaved and ships lost, of loneliness so profound it had become a kind of companion.\n\nTonight the wind howled like a wounded animal, throwing sheets of rain against the\nwindows with enough force to rattle the thick glass. Thomas paused on the landing, one\nhand braced against the cold stone wall, and caught his breath. His knees weren't what\nthey used to be. None of him was what it used to be.\n\n\"One more night,\" he muttered to himself, a habit born of decades without anyone else\nto talk to. \"Just one more.\"\n\nThe lamp room at the top was warm despite the storm. Thomas had maintained the old\nFresnel lens with religious devotion — polishing the brass fittings until they gleamed,\nkeeping the clockwork mechanism oiled and true. The Coast Guard had wanted to automate\nthe light years ago, replace him with sensors and timers. He had fought them tooth and nail.\n\n\"A machine doesn't hear a ship in distress,\" he had argued. \"A machine doesn't notice\nwhen the fog rolls in different than usual.\"\n\nThey had let him stay. But tomorrow — tomorrow they were coming with their equipment.\nThe last manned lighthouse on the eastern seaboard would finally go dark. At least, his\nversion of it would.\n\nThomas struck the match and touched it to the wick. The flame caught, small at first,\nthen growing as the oil drew upward. He adjusted the mantle, watched the light bloom\nand multiply through the precision-cut prisms until it became something powerful,\nsomething that could reach across miles of angry ocean to tell a sailor: you are not alone.\n\nHe settled into his chair by the window and opened his logbook. The entries went back\ndecades, each one in his careful, slanting hand. Weather conditions. Visibility.\nShips observed. Incidents.\n\n\"November 17th,\" he wrote. \"Wind northeast, 45 knots gusting to 60. Rain heavy.\nVisibility poor. Seas rough, 15-foot swells.\"\n\nHe paused, pen hovering over the page. Then he added: \"Final entry.\"\n\nChapter 2: The Storm\n\nSarah Chen had not planned to be at sea tonight. Nobody with any sense would have\nchosen to sail through a November gale in a thirty-two-foot sloop. But plans, as her\ngrandmother used to say, were just stories you told yourself about a future that hadn't\nhappened yet.\n\nThe storm had come on fast, much faster than the forecast predicted. By the time the\nfirst squall line hit, she was already past the point of no return — too far from the\nharbor to turn back, too far from anywhere to find shelter.\n\nSo she did what sailors do: she shortened sail, lashed everything down, clipped her\nharness to the jackline, and held on.\n\nThe waves came like mountains, black and frothing, lifting the little sloop to\nimpossible heights before dropping her into troughs so deep the wind disappeared.\nEach time the boat climbed, Sarah's stomach fell. Each time it dropped, she braced\nfor the impact, the shuddering crash as the hull met the next wall of water.\n\nHer navigation instruments had failed an hour ago — the GPS screen flickered once\nand went dark, victim of a wave that had found its way below through a vent she\nthought was sealed. Without electronics, she was sailing blind.\n\nNo. Not quite blind.\n\nThrough the rain, through the spray, through the chaos of wind and wave, she saw\nit — a light. Sweeping across the water in a steady, ancient rhythm. The lighthouse.\n\n\"Thank God,\" she breathed, and for the first time in hours, she knew where she was.\n\nThe light was both salvation and warning. It told her the shore was near, which meant\nrocks were near. She adjusted her course, bearing off to give the headland a wide berth.\nThe light swept past again — reliable, unwavering, indifferent to the storm.\n\nUp in the lighthouse, Thomas Gray didn't know it yet, but his final watch was about\nto become the most important one of his life.\n\"\"\"\n        for i in range(3):\n            names = [(\"Thomas Gray\", \"Sarah Chen\"), (\"James Walker\", \"Maria Santos\"), (\"Robert Kim\", \"Elena Volkov\")]\n            text = sample_text.replace(\"Thomas Gray\", names[i][0]).replace(\"Sarah Chen\", names[i][1])\n            (data_dir / f\"sample_novel_{i+1}.txt\").write_text(text, encoding=\"utf-8\")\n\n    print(f\"Created sample data in {data_dir}:\")\n    for f in sorted(data_dir.iterdir()):\n        if f.is_file():\n            print(f\"  {f.name} ({f.stat().st_size:,} bytes)\")\n\nelse:\n    # Upload your own novels\n    from google.colab import files as colab_files\n    print(\"Upload your novel files (.txt, .pdf, .epub, .html, .md, .mobi):\")\n    print(\"(Click 'Choose Files' button below)\")\n    uploaded = colab_files.upload()\n\n    for name, content in uploaded.items():\n        target = data_dir / name\n        with open(target, \"wb\") as f:\n            f.write(content)\n        print(f\"  Saved: {name} ({len(content):,} bytes)\")\n\nprint(f\"\\nTotal files in {data_dir}: {len(list(data_dir.iterdir()))}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Run Data Processing Pipeline\n",
    "\n",
    "This runs the full pipeline: **ingest** (multi-format) > **clean** > **format** (to JSONL) > **deduplicate** > **quality filter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from pathlib import Path\n",
    "\n",
    "# Create config.yaml with our settings\n",
    "config = {\n",
    "    \"data\": {\n",
    "        \"input_dir\": \"data/raw\",\n",
    "        \"output_dir\": \"data/processed\",\n",
    "        \"temp_dir\": \"data/processed/temp_cleaned\",\n",
    "        \"chunk_size\": CHUNK_SIZE,\n",
    "        \"overlap\": 500,\n",
    "    },\n",
    "    \"log_level\": \"INFO\",\n",
    "}\n",
    "\n",
    "with open(\"/content/Novel_Writer/config.yaml\", \"w\") as f:\n",
    "    yaml.dump(config, f, default_flow_style=False)\n",
    "\n",
    "print(\"Config written. Running pipeline...\\n\")\n",
    "\n",
    "# Build pipeline command\n",
    "cmd = \"novel-writer -v pipeline --clean\"\n",
    "\n",
    "# Check if any ingestable files exist\n",
    "ingest_exts = {\".epub\", \".html\", \".htm\", \".md\", \".mobi\"}\n",
    "has_ingestable = any(f.suffix.lower() in ingest_exts for f in Path(\"data/raw\").iterdir() if f.is_file())\n",
    "if has_ingestable:\n",
    "    cmd += \" --ingest\"\n",
    "\n",
    "if RUN_DEDUP:\n",
    "    cmd += \" --deduplicate\"\n",
    "if RUN_QUALITY_FILTER:\n",
    "    cmd += \" --filter\"\n",
    "\n",
    "print(f\"Command: {cmd}\\n\")\n",
    "!{cmd}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check pipeline output\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Find the final JSONL file\n",
    "processed_dir = Path(\"data/processed\")\n",
    "jsonl_files = sorted(processed_dir.glob(\"*.jsonl\"), key=lambda f: f.stat().st_mtime, reverse=True)\n",
    "\n",
    "if not jsonl_files:\n",
    "    print(\"ERROR: No JSONL files produced! Check pipeline output above.\")\n",
    "    print(\"Files in processed dir:\")\n",
    "    for f in processed_dir.iterdir():\n",
    "        print(f\"  {f.name}\")\n",
    "else:\n",
    "    train_file = jsonl_files[0]\n",
    "    with open(train_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    print(f\"Training data: {train_file.name}\")\n",
    "    print(f\"Total entries: {len(lines)}\")\n",
    "\n",
    "    if lines:\n",
    "        sample = json.loads(lines[0])\n",
    "        print(f\"\\nSample entry keys: {list(sample.keys())}\")\n",
    "        print(f\"Output preview: {sample.get('output', '')[:200]}...\")\n",
    "\n",
    "    # Save path for training step\n",
    "    TRAIN_FILE = str(train_file)\n",
    "    print(f\"\\nUsing: {TRAIN_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: Load Model & Configure LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "print(f\"Loading model: {CFG['model_name']}\")\n",
    "print(f\"Max sequence length: {MAX_SEQ_LENGTH}\")\n",
    "print(f\"4-bit quantization: True\\n\")\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=CFG[\"model_name\"],\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    dtype=None,          # Auto-detect\n",
    "    load_in_4bit=True,   # QLoRA\n",
    ")\n",
    "\n",
    "print(f\"\\nGPU memory after loading: {torch.cuda.memory_allocated() / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=LORA_RANK,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha=LORA_RANK // 2,\n",
    "    lora_dropout=0,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=3407,\n",
    ")\n",
    "\n",
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Trainable parameters: {trainable:,} / {total:,} ({100 * trainable / total:.2f}%)\")\n",
    "print(f\"GPU memory with LoRA: {torch.cuda.memory_allocated() / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 6: Prepare Dataset for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from datasets import load_dataset\n\ndataset = load_dataset(\"json\", data_files=TRAIN_FILE, split=\"train\")\n\n# Train/validation split\nif len(dataset) > 10:\n    split = dataset.train_test_split(test_size=0.1, seed=42)\n    train_dataset = split[\"train\"]\n    eval_dataset = split[\"test\"]\nelse:\n    train_dataset = dataset\n    eval_dataset = None\n    print(\"Dataset too small for validation split, training on all data.\")\n\nprint(f\"Training samples: {len(train_dataset)}\")\nif eval_dataset:\n    print(f\"Validation samples: {len(eval_dataset)}\")\n\n# Universal formatting using tokenizer's built-in chat template\n# This works for ALL models (Qwen, Llama, Mistral, Gemma, Phi, etc.)\ndef formatting_func(examples):\n    instructions = examples[\"instruction\"]\n    outputs = examples[\"output\"]\n    texts = []\n    for instruction, output in zip(instructions, outputs):\n        messages = [\n            {\"role\": \"system\", \"content\": CFG[\"system_prompt\"]},\n            {\"role\": \"user\", \"content\": instruction},\n            {\"role\": \"assistant\", \"content\": output},\n        ]\n        # apply_chat_template handles the correct format for each model family\n        try:\n            text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n        except Exception:\n            # Fallback for models without system role support\n            messages_no_sys = [\n                {\"role\": \"user\", \"content\": CFG[\"system_prompt\"] + \"\\n\\n\" + instruction},\n                {\"role\": \"assistant\", \"content\": output},\n            ]\n            text = tokenizer.apply_chat_template(messages_no_sys, tokenize=False, add_generation_prompt=False)\n        texts.append(text)\n    return {\"text\": texts}\n\ntrain_dataset = train_dataset.map(formatting_func, batched=True)\nif eval_dataset:\n    eval_dataset = eval_dataset.map(formatting_func, batched=True)\n\nprint(f\"\\n--- Sample formatted entry ---\")\nprint(train_dataset[0][\"text\"][:600])\nprint(\"...\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 7: Train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, EarlyStoppingCallback\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"checkpoints_{CFG['output_name']}\",\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION,\n",
    "    warmup_ratio=0.1,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    fp16=not torch.cuda.is_bf16_supported(),\n",
    "    bf16=torch.cuda.is_bf16_supported(),\n",
    "    logging_steps=5,\n",
    "    save_strategy=\"steps\" if eval_dataset else \"epoch\",\n",
    "    save_steps=50 if eval_dataset else None,\n",
    "    save_total_limit=3,\n",
    "    seed=3407,\n",
    ")\n",
    "\n",
    "# Add eval settings if we have validation data\n",
    "if eval_dataset:\n",
    "    training_args.eval_strategy = \"steps\"\n",
    "    training_args.eval_steps = 50\n",
    "    training_args.load_best_model_at_end = True\n",
    "    training_args.metric_for_best_model = \"eval_loss\"\n",
    "    training_args.greater_is_better = False\n",
    "\n",
    "callbacks = []\n",
    "if eval_dataset:\n",
    "    callbacks.append(EarlyStoppingCallback(early_stopping_patience=3))\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    dataset_num_proc=2,\n",
    "    packing=False,\n",
    "    args=training_args,\n",
    "    callbacks=callbacks if callbacks else None,\n",
    ")\n",
    "\n",
    "print(f\"Starting training: {NUM_EPOCHS} epochs, {len(train_dataset)} samples\")\n",
    "print(f\"Effective batch size: {BATCH_SIZE * GRADIENT_ACCUMULATION}\")\n",
    "print(f\"Estimated steps: {len(train_dataset) * NUM_EPOCHS // (BATCH_SIZE * GRADIENT_ACCUMULATION)}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "stats = trainer.train()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(f\"Training complete!\")\n",
    "print(f\"  Total steps: {stats.global_step}\")\n",
    "print(f\"  Training loss: {stats.training_loss:.4f}\")\n",
    "print(f\"  Runtime: {stats.metrics['train_runtime']:.0f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 8: Test Generation\n",
    "\n",
    "Let's see what the fine-tuned model can do!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "FastLanguageModel.for_inference(model)\n\nprint(f\"Generating with {CFG['model_name']}...\\n\")\n\nfor i, prompt in enumerate(CFG[\"test_prompts\"]):\n    # Universal chat template approach - works for all models\n    messages = [\n        {\"role\": \"system\", \"content\": CFG[\"system_prompt\"]},\n        {\"role\": \"user\", \"content\": prompt},\n    ]\n    try:\n        inputs = tokenizer.apply_chat_template(\n            messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\"\n        ).to(\"cuda\")\n    except Exception:\n        # Fallback for models without system role\n        messages_no_sys = [\n            {\"role\": \"user\", \"content\": CFG[\"system_prompt\"] + \"\\n\\n\" + prompt},\n        ]\n        inputs = tokenizer.apply_chat_template(\n            messages_no_sys, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\"\n        ).to(\"cuda\")\n\n    input_len = inputs.shape[-1]\n\n    outputs = model.generate(\n        input_ids=inputs,\n        max_new_tokens=512,\n        temperature=0.8,\n        top_p=0.9,\n        top_k=50,\n        do_sample=True,\n        repetition_penalty=1.1,\n    )\n    response = tokenizer.decode(outputs[0][input_len:], skip_special_tokens=True)\n\n    print(f\"{'='*60}\")\n    print(f\"Prompt {i+1}: {prompt}\")\n    print(f\"{'='*60}\")\n    print(response)\n    print(f\"[{len(response)} chars]\\n\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 9: Save & Download Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_name = CFG[\"output_name\"]\n",
    "\n",
    "# Save LoRA adapters\n",
    "model.save_pretrained(output_name)\n",
    "tokenizer.save_pretrained(output_name)\n",
    "print(f\"Model saved to {output_name}/\")\n",
    "\n",
    "# Show saved files\n",
    "import os\n",
    "total_size = 0\n",
    "for f in sorted(Path(output_name).rglob(\"*\")):\n",
    "    if f.is_file():\n",
    "        size = f.stat().st_size\n",
    "        total_size += size\n",
    "        print(f\"  {f.name}: {size / 1024 / 1024:.1f} MB\")\n",
    "print(f\"\\nTotal size: {total_size / 1024 / 1024:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download as zip\n",
    "!zip -r {output_name}.zip {output_name}/\n",
    "\n",
    "from google.colab import files as colab_files\n",
    "colab_files.download(f\"{output_name}.zip\")\n",
    "print(f\"\\nDownloading {output_name}.zip ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 10 (Optional): Save to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment these lines to save to Google Drive\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount(\"/content/drive\")\n",
    "#\n",
    "# import shutil\n",
    "# drive_path = f\"/content/drive/MyDrive/{output_name}\"\n",
    "# shutil.copytree(output_name, drive_path, dirs_exist_ok=True)\n",
    "# print(f\"Saved to Google Drive: {drive_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 11 (Optional): Export to GGUF for Local Use\n",
    "\n",
    "Export your model to GGUF format for running locally with **Ollama** or **llama.cpp**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to export to GGUF (takes ~10-15 minutes)\n",
    "\n",
    "# gguf_name = f\"{output_name}_gguf\"\n",
    "# model.save_pretrained_gguf(\n",
    "#     gguf_name,\n",
    "#     tokenizer,\n",
    "#     quantization_method=\"q4_k_m\",  # Good balance of quality vs size\n",
    "# )\n",
    "#\n",
    "# from google.colab import files as colab_files\n",
    "# gguf_file = list(Path(gguf_name).glob(\"*.gguf\"))[0]\n",
    "# colab_files.download(str(gguf_file))\n",
    "# print(f\"GGUF exported! Run locally with:\")\n",
    "# print(f\"  ollama run ./{gguf_file.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Done!\n",
    "\n",
    "Your fine-tuned model has been saved. To use it locally with the Novel Writer CLI:\n",
    "\n",
    "```bash\n",
    "# Unzip your downloaded model\n",
    "unzip qwen3_chinese_novel_lora.zip  # or nemo_english_story_lora.zip\n",
    "\n",
    "# Generate text\n",
    "novel-writer generate --prompt \"Your prompt here...\" --model qwen3_chinese_novel_lora\n",
    "```\n",
    "\n",
    "Or start the API server:\n",
    "```bash\n",
    "python -m novel_writer.api\n",
    "# POST to http://localhost:8000/generate\n",
    "```"
   ]
  }
 ]
}