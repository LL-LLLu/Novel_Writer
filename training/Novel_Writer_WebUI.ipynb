{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "A100"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Novel Writer Studio - Web UI (Colab)\n\nRun a full story-writing web UI on Google Colab.\n\n**Two modes:**\n- **Local Model** (GPU required): Your fine-tuned LoRA model writes chapters with trained literary style. Needs Colab Pro for 32B model (A100), free tier works for 8B (T4).\n- **Cloud API** (no GPU needed): Gemini/GPT writes chapters. Works on **free Colab** — no GPU runtime required. Loses LoRA style but keeps all agent features.\n\n**Workflow:**\n1. **Cloud AI** (Gemini / GPT) develops your story idea into a detailed plot outline\n2. **Multi-agent system** (Mastermind + Story Tracker) orchestrates chapter generation\n3. **Your chosen writer** (local model or cloud API) generates the chapter prose\n\n### How to use\n1. Run cells in order (skip cells 3-4 if using Cloud API mode only)\n2. Click the Gradio public link to open the UI\n3. Enter your API key, pick your Writer Mode, and generate!"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "#@title Configuration { display-mode: \"form\" }\n\n#@markdown ### Writer Mode\n#@markdown - **local_model**: Use your fine-tuned LoRA model (requires GPU runtime)\n#@markdown - **cloud_api**: Use Gemini/GPT for writing (no GPU needed, skip cells 3-4)\nWRITER_MODE = \"cloud_api\" #@param [\"local_model\", \"cloud_api\"]\n\n#@markdown ---\n#@markdown ### Model Selection (only for local_model mode)\n#@markdown Choose the base model that matches your LoRA adapter.\nMODEL_CHOICE = \"qwen3_32b\" #@param [\"qwen3_4b\", \"qwen3_8b\", \"qwen3_14b\", \"qwen3_32b\", \"llama31_8b\", \"gemma2_9b\", \"mistral_nemo_12b\"]\n\n#@markdown ### LoRA Upload Mode\n#@markdown - **huggingface_hub**: Download from Hugging Face (recommended)\n#@markdown - **upload_zip**: Upload your LoRA adapter as a .zip file\n#@markdown - **google_drive**: Load from Google Drive path\nLORA_MODE = \"huggingface_hub\" #@param [\"huggingface_hub\", \"upload_zip\", \"google_drive\"]\n\n#@markdown ### Hugging Face repo ID (only if LORA_MODE = huggingface_hub)\nHF_REPO_ID = \"YOUR_USER/qwen3_32b_novel_lora\" #@param {type:\"string\"}\n\n#@markdown ### Google Drive path (only if LORA_MODE = google_drive)\nDRIVE_LORA_PATH = \"/content/drive/MyDrive/qwen3_32b_novel_lora\" #@param {type:\"string\"}\n\nMODEL_CONFIGS = {\n    'qwen3_4b': 'unsloth/Qwen3-4B',\n    'qwen3_8b': 'unsloth/Qwen3-8B',\n    'qwen3_14b': 'unsloth/Qwen3-14B',\n    'qwen3_32b': 'unsloth/Qwen3-32B',\n    'llama31_8b': 'unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit',\n    'gemma2_9b': 'unsloth/gemma-2-9b-it-bnb-4bit',\n    'mistral_nemo_12b': 'unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit',\n}\n\nBASE_MODEL = MODEL_CONFIGS[MODEL_CHOICE]\n\nprint(f'Writer mode: {WRITER_MODE}')\nif WRITER_MODE == 'local_model':\n    print(f'Base model: {BASE_MODEL}')\n    print(f'LoRA mode: {LORA_MODE}')\nelse:\n    print('Cloud API mode — no GPU or local model needed.')\n    print('You can skip cells 3 and 4, go straight to cell 5 (Install deps) then cell 6 (Launch UI).')",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "#@title Install dependencies\nimport subprocess, sys\n\n# Core deps (always needed)\nsubprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'gradio', 'google-genai', 'openai', 'huggingface_hub'])\n\nif WRITER_MODE == 'local_model':\n    # GPU deps\n    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'unsloth'])\n    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', '--force-reinstall',\n                           '--no-cache-dir', '--no-deps',\n                           'git+https://github.com/unslothai/unsloth.git'])\n    import torch\n    print(f'GPU: {torch.cuda.get_device_name(0)}')\n    print(f'VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')\nelse:\n    print('Cloud API mode — GPU dependencies skipped.')\n\nprint('Setup complete!')",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "#@title Upload / locate LoRA adapter (skip if Cloud API mode)\nimport os, zipfile\nfrom pathlib import Path\n\nLORA_PATH = None\n\nif WRITER_MODE == 'cloud_api':\n    print('Cloud API mode — no LoRA adapter needed. Skip this cell.')\n\nelif LORA_MODE == 'huggingface_hub':\n    from huggingface_hub import snapshot_download\n    print(f'Downloading LoRA from Hugging Face: {HF_REPO_ID}')\n    LORA_PATH = snapshot_download(\n        repo_id=HF_REPO_ID,\n        local_dir=f'/content/{HF_REPO_ID.split(\"/\")[-1]}',\n    )\n    print(f'Downloaded to: {LORA_PATH}')\n\nelif LORA_MODE == 'upload_zip':\n    from google.colab import files as colab_files\n    print('Upload your LoRA adapter zip file:')\n    uploaded = colab_files.upload()\n    for name in uploaded:\n        if name.endswith('.zip'):\n            with zipfile.ZipFile(name, 'r') as z:\n                z.extractall('/content/')\n            for d in Path('/content').iterdir():\n                if d.is_dir() and (d / 'adapter_config.json').exists():\n                    LORA_PATH = str(d)\n                    break\n        else:\n            os.makedirs('/content/lora_adapter', exist_ok=True)\n            os.rename(name, f'/content/lora_adapter/{name}')\n            LORA_PATH = '/content/lora_adapter'\n\nelif LORA_MODE == 'google_drive':\n    from google.colab import drive\n    drive.mount('/content/drive')\n    LORA_PATH = DRIVE_LORA_PATH\n\nif LORA_PATH and Path(LORA_PATH).exists():\n    print(f'LoRA adapter found: {LORA_PATH}')\n    for f in sorted(Path(LORA_PATH).iterdir()):\n        print(f'  {f.name} ({f.stat().st_size / 1024:.0f} KB)')\nelif WRITER_MODE == 'local_model':\n    print(f'ERROR: LoRA adapter not found at {LORA_PATH}')",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "#@title Load model + LoRA (skip if Cloud API mode)\n\nmodel = None\ntokenizer = None\n\nif WRITER_MODE == 'cloud_api':\n    print('Cloud API mode — no model to load. Skip this cell.')\nelse:\n    from unsloth import FastLanguageModel\n    import torch\n\n    print(f'Loading base model: {BASE_MODEL}')\n    model, tokenizer = FastLanguageModel.from_pretrained(\n        model_name=BASE_MODEL,\n        max_seq_length=4096,\n        dtype=None,\n        load_in_4bit=True,\n    )\n\n    print(f'Applying LoRA from: {LORA_PATH}')\n    from peft import PeftModel\n    model = PeftModel.from_pretrained(model, LORA_PATH)\n    FastLanguageModel.for_inference(model)\n\n    vram = torch.cuda.memory_allocated() / 1e9\n    print(f'\\nModel loaded! VRAM used: {vram:.1f} GB')",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "#@title Launch Web UI\nimport gradio as gr\nimport copy, re, time, json, random\nfrom pathlib import Path\n\n# ---- System prompts (matching training) ----\nZH_SYSTEM = (\n    '你是一位经验丰富的中文小说作家，擅长构建沉浸式的叙事场景。请根据给定的上下文续写故事，要求：\\n'\n    '1. 保持与原文一致的叙事视角和文风\\n'\n    '2. 通过具体的动作、对话和环境描写推动情节发展\\n'\n    '3. 角色的言行应符合其性格特征和当前情境\\n'\n    '4. 善用感官细节（视觉、听觉、触觉、嗅觉）营造氛围\\n'\n    '5. 对话要自然生动，符合角色身份和说话习惯\\n'\n    '6. 避免空洞的心理独白，用行动和细节展现人物内心'\n)\n\nEN_SYSTEM = (\n    'You are an accomplished fiction author with a gift for immersive storytelling. '\n    'Continue the narrative following these principles:\\n'\n    '1. Maintain the established point of view, voice, and tonal register\\n'\n    '2. Advance the plot through concrete action, dialogue, and environmental detail\\n'\n    '3. Show character emotion through behavior, body language, and subtext — not exposition\\n'\n    '4. Engage multiple senses (sight, sound, touch, smell, taste) to ground scenes\\n'\n    '5. Write dialogue that reveals character, creates tension, and sounds natural\\n'\n    '6. Vary sentence rhythm — mix short punchy lines with longer flowing passages'\n)\n\n# ---- Instruction pools (from training data) ----\n_ZH_INSTRUCTIONS = [\n    '续写这段叙事，保持原文的风格和节奏。',\n    '以相同的文风继续这个故事。',\n    '根据已有的情节和人物设定，续写下一段。',\n    '保持叙事视角不变，继续推进故事发展。',\n    '用生动的细节描写续写这个场景。',\n    '通过对话和动作描写推进下面的情节。',\n    '延续当前的叙事氛围，写出接下来发生的事。',\n    '以细腻的笔触续写这段文字。',\n    '按照原文的叙事节奏，写出故事的下一部分。',\n    '继续描绘这个场景中的人物和事件。',\n    '用符合原文风格的语言续写故事。',\n    '展开叙述，让故事自然地向前发展。',\n    '保持文风一致，续写接下来的情节。',\n    '以沉浸式的叙事方式继续这段故事。',\n    '描绘接下来的场景，注意环境和人物的刻画。',\n    '用简洁有力的文字续写这段叙事。',\n    '继续讲述这个故事，注意情感的表达。',\n    '以自然流畅的文笔续写下一段。',\n    '延续原文的基调，推进故事走向。',\n    '用丰富的感官描写续写这个场景。',\n]\n\n_EN_INSTRUCTIONS = [\n    'Continue the narrative in the established style.',\n    'Write the next passage, maintaining the existing voice and tone.',\n    'Advance the story using vivid sensory details.',\n    'Continue this scene with natural dialogue and action.',\n    'Extend the narrative, preserving the point of view and pacing.',\n    'Write what happens next, staying true to the characters.',\n    'Continue the story with concrete, immersive description.',\n    'Carry the narrative forward in the same literary register.',\n    'Write the next segment, matching the established rhythm.',\n    'Develop this scene further with authentic detail.',\n    'Push the story forward through action and dialogue.',\n    'Continue in the same voice, advancing the plot naturally.',\n    'Write the following passage in the style of the preceding text.',\n    'Extend this scene with attention to atmosphere and character.',\n    'Continue the narrative arc with engaging prose.',\n    'Write what comes next, maintaining tension and pacing.',\n    'Advance the story, weaving in environmental detail.',\n    'Continue with prose that matches the tone and texture of the original.',\n    'Develop the next beat of the story with precise language.',\n    'Carry the scene forward, balancing action with description.',\n]\n\n# ---- Token utilities ----\nMAX_SEQ_LENGTH = 4096\n\n\ndef count_tokens(text):\n    if tokenizer is not None:\n        return len(tokenizer.encode(text, add_special_tokens=False))\n    # Rough estimate for cloud-only mode (no tokenizer loaded)\n    cjk = sum(1 for c in text if '\\u4e00' <= c <= '\\u9fff')\n    return cjk + len(text.split()) - cjk // 2\n\n\ndef _find_sentence_boundary(text, max_chars, from_end=False):\n    sentence_ends_cjk = re.compile(r'[。！？…]+')\n    sentence_ends_en = re.compile(r'[.!?][\\u201c\\u201d\\u2018\\u2019\"\\')\\u300d\\uff09]*(?:\\s|\\n)')\n    if from_end:\n        search_region = text[-max_chars:] if len(text) > max_chars else text\n        offset = max(0, len(text) - max_chars)\n        best = 0\n        for m in sentence_ends_cjk.finditer(search_region):\n            best = m.end()\n            break\n        for m in sentence_ends_en.finditer(search_region):\n            if m.end() < best or best == 0:\n                best = m.end()\n            break\n        return offset + best if best > 0 else offset\n    else:\n        search_region = text[:max_chars]\n        best = max_chars\n        for m in sentence_ends_cjk.finditer(search_region):\n            best = m.end()\n        for m in sentence_ends_en.finditer(search_region):\n            best = m.end()\n        return best\n\n\ndef trim_to_token_budget(text, max_tokens, keep_end=True):\n    current = count_tokens(text)\n    if current <= max_tokens:\n        return text\n    ratio = max_tokens / max(current, 1)\n    target_chars = int(len(text) * ratio * 0.95)\n    if keep_end:\n        start = _find_sentence_boundary(text, len(text) - target_chars, from_end=True)\n        trimmed = text[start:]\n    else:\n        end = _find_sentence_boundary(text, target_chars, from_end=False)\n        trimmed = text[:end]\n    if count_tokens(trimmed) > max_tokens:\n        trimmed = text[-target_chars:] if keep_end else text[:target_chars]\n    return trimmed.strip()\n\n\ndef detect_language(text):\n    cjk = sum(1 for c in text[:300] if '\\u4e00' <= c <= '\\u9fff')\n    return 'zh' if cjk > len(text[:300]) * 0.15 else 'en'\n\n\n# ---- Cloud API helpers ----\ndef _build_plot_prompt(idea, num_chapters, lang):\n    if lang == 'zh':\n        system = (\n            '你是一位资深的小说策划编辑和故事架构师。你擅长从简单的故事构思中发展出完整、'\n            '引人入胜的小说大纲。你的大纲应该包含极其丰富的细节，足以直接指导AI模型逐章生成高质量的小说内容。'\n            '每个章节的大纲都应该详细到可以独立作为写作指南。'\n        )\n        prompt = f'请基于以下故事构思，创作一个非常详细的小说大纲：\\n\\n故事构思：{idea}\\n\\n请严格按照以下格式输出（使用中文）：\\n\\n## 小说标题\\n[一个引人入胜的标题]\\n\\n## 故事背景\\n[详细的世界观和背景设定，至少200字。包括：时代背景、地理环境、社会体制、文化风俗、特殊设定]\\n\\n## 主要人物\\n（至少4个主要角色，每个角色需包含详细信息）\\n- **[角色全名]**（[年龄/外貌简述]）：[性格特点——至少3个性格关键词]，[身份背景]，[核心动机]，[人物弧光]，[与其他角色的关键关系]\\n\\n## 核心冲突\\n[故事的主要矛盾和驱动力，包括外部冲突和内部冲突，至少100字]\\n\\n## 章节大纲\\n（共{num_chapters}章，每章需要非常具体的情节描述）\\n'\n        for i in range(1, num_chapters + 1):\n            prompt += f'\\n### 第{i}章：[章节标题]\\n- **开场场景**：[具体的时间、地点、氛围描写]\\n- **主要事件**：[本章发生的1-3个关键事件]\\n- **人物互动**：[哪些角色出场，对话和冲突要点]\\n- **情感节奏**：[情感基调变化]\\n- **关键细节**：[需要着重描写的场景细节]\\n- **章末转折**：[悬念、伏笔或转折点]\\n'\n        prompt += '\\n## 伏笔与线索\\n[列出3-5个贯穿全文的伏笔和线索]\\n\\n## 写作风格指导\\n[对本小说整体风格的建议]\\n\\n请确保章节之间有清晰的因果关系，整体故事有完整的起承转合，每章大纲足够详细。'\n    else:\n        system = (\n            'You are a senior fiction editor and story architect. You excel at developing '\n            'simple story concepts into complete, compelling novel outlines with rich detail.'\n        )\n        prompt = f'Based on the following story idea, create a very detailed novel outline:\\n\\nStory idea: {idea}\\n\\n## Title\\n[A compelling title]\\n\\n## Setting\\n[Detailed world-building, at least 200 words]\\n\\n## Main Characters\\n(At least 4, each with detailed profiles)\\n\\n## Central Conflict\\n[Main tension, at least 100 words]\\n\\n## Chapter Outline\\n({num_chapters} chapters, each with specific plot details)\\n'\n        for i in range(1, num_chapters + 1):\n            prompt += f'\\n### Chapter {i}: [Title]\\n- **Opening scene**: [Time, place, atmosphere]\\n- **Key events**: [1-3 major events]\\n- **Character interactions**: [Dialogue and conflict points]\\n- **Emotional rhythm**: [Tone arc]\\n- **Key details**: [Important elements to emphasize]\\n- **Chapter-end hook**: [Cliffhanger or turning point]\\n'\n        prompt += '\\n## Foreshadowing & Threads\\n[3-5 narrative threads]\\n\\n## Style Guide\\n[Recommendations for style]\\n\\nEnsure clear progression and complete narrative arc.'\n    return system, prompt\n\n\ndef generate_plot_gemini(api_key, idea, num_chapters, lang):\n    from google import genai\n    from google.genai import types\n    client = genai.Client(api_key=api_key)\n    system, prompt = _build_plot_prompt(idea, num_chapters, lang)\n    response = client.models.generate_content(\n        model='gemini-3-pro-preview', contents=prompt,\n        config=types.GenerateContentConfig(system_instruction=system, temperature=0.9, max_output_tokens=8192),\n    )\n    return response.text\n\n\ndef generate_plot_gpt(api_key, idea, num_chapters, lang):\n    from openai import OpenAI\n    client = OpenAI(api_key=api_key)\n    system, prompt = _build_plot_prompt(idea, num_chapters, lang)\n    response = client.chat.completions.create(\n        model='gpt-4o', messages=[{'role': 'system', 'content': system}, {'role': 'user', 'content': prompt}],\n        temperature=0.9, max_tokens=8192,\n    )\n    return response.choices[0].message.content\n\n\ndef develop_plot_api(idea, num_chapters, provider, api_key):\n    if not idea.strip():\n        return 'Please enter a story idea first.'\n    if not api_key.strip():\n        return f'Please enter your {provider} API key above.'\n    lang = detect_language(idea)\n    try:\n        if provider == 'Gemini':\n            return generate_plot_gemini(api_key, idea, int(num_chapters), lang)\n        else:\n            return generate_plot_gpt(api_key, idea, int(num_chapters), lang)\n    except Exception as e:\n        return f'API Error ({provider}): {e}'\n\n\ndef call_cloud_api(system, prompt, provider, api_key, temperature=0.7, max_tokens=2048):\n    if provider == 'Gemini':\n        from google import genai\n        from google.genai import types\n        client = genai.Client(api_key=api_key)\n        response = client.models.generate_content(\n            model='gemini-3-pro-preview', contents=prompt,\n            config=types.GenerateContentConfig(\n                system_instruction=system, temperature=temperature, max_output_tokens=max_tokens),\n        )\n        return response.text\n    elif provider == 'GPT':\n        from openai import OpenAI\n        client = OpenAI(api_key=api_key)\n        response = client.chat.completions.create(\n            model='gpt-4o',\n            messages=[{'role': 'system', 'content': system}, {'role': 'user', 'content': prompt}],\n            temperature=temperature, max_tokens=max_tokens,\n        )\n        return response.choices[0].message.content\n    else:\n        raise ValueError(f'Unknown provider: {provider}')\n\n\ndef _parse_json_response(text):\n    m = re.search(r'```(?:json)?\\s*\\n(.*?)\\n```', text, re.DOTALL)\n    if m:\n        return json.loads(m.group(1))\n    cleaned = text.strip()\n    if cleaned.startswith('{'):\n        return json.loads(cleaned)\n    start = cleaned.find('{')\n    end = cleaned.rfind('}')\n    if start != -1 and end != -1:\n        return json.loads(cleaned[start:end + 1])\n    raise ValueError(f'Could not parse JSON from response: {text[:200]}...')\n\n\n# ---- Cloud text generation ----\ndef generate_text_cloud(prompt, system_prompt='', max_new_tokens=2048, temperature=0.8,\n                        provider='Gemini', api_key=''):\n    \"\"\"Generate text using cloud API instead of a local model.\"\"\"\n    if not api_key.strip():\n        return 'Please enter your API key in Settings above.'\n    if not system_prompt:\n        system_prompt = ZH_SYSTEM if detect_language(prompt) == 'zh' else EN_SYSTEM\n    return call_cloud_api(system_prompt, prompt, provider, api_key,\n                          temperature=temperature, max_tokens=max_new_tokens)\n\n\n# ---- Local model generation ----\ndef generate_text(prompt, system_prompt='', max_new_tokens=2048, temperature=0.8,\n                  top_p=0.9, top_k=50, repetition_penalty=1.0):\n    if model is None or tokenizer is None:\n        return 'No local model loaded. Switch Writer Mode to Cloud API or load a model first.'\n\n    if not system_prompt:\n        system_prompt = ZH_SYSTEM if detect_language(prompt) == 'zh' else EN_SYSTEM\n\n    import torch\n    messages = [\n        {'role': 'system', 'content': system_prompt},\n        {'role': 'user', 'content': prompt},\n    ]\n    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n    inputs = tokenizer(text, return_tensors='pt').to(model.device)\n\n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs, max_new_tokens=max_new_tokens,\n            temperature=max(temperature, 0.01), top_p=top_p, top_k=top_k,\n            repetition_penalty=repetition_penalty, do_sample=True,\n        )\n    new_tokens = outputs[0][inputs['input_ids'].shape[1]:]\n    return tokenizer.decode(new_tokens, skip_special_tokens=True)\n\n\n# ---- Story Tracker ----\nclass StoryTracker:\n    @staticmethod\n    def empty_bible():\n        return {'characters': {}, 'plot_threads': [], 'chapter_summaries': [], 'style_notes': ''}\n\n    @staticmethod\n    def initialize_from_outline(outline, provider, api_key, lang):\n        if lang == 'zh':\n            system = '你是一位小说编辑助手。请从小说大纲中提取结构化信息，以JSON格式输出。'\n            prompt = f'请分析以下小说大纲，提取关键信息并以JSON格式输出：\\n\\n{outline}\\n\\n输出格式（必须是合法JSON）：\\n```json\\n{{\"characters\": {{\"角色名\": {{\"description\": \"外貌和身份\", \"personality\": \"性格\", \"motivation\": \"动机\", \"relationships\": {{}}, \"location\": \"位置\", \"emotional_state\": \"状态\"}}}}, \"plot_threads\": [{{\"name\": \"线索\", \"status\": \"active\", \"description\": \"描述\"}}], \"style_notes\": \"风格\"}}\\n```'\n        else:\n            system = 'You are a fiction editor assistant. Extract structured information from the novel outline as JSON.'\n            prompt = f'Analyze the following novel outline and extract key information as JSON:\\n\\n{outline}\\n\\nOutput format (must be valid JSON):\\n```json\\n{{\"characters\": {{\"Name\": {{\"description\": \"role\", \"personality\": \"traits\", \"motivation\": \"motivation\", \"relationships\": {{}}, \"location\": \"location\", \"emotional_state\": \"state\"}}}}, \"plot_threads\": [{{\"name\": \"thread\", \"status\": \"active\", \"description\": \"desc\"}}], \"style_notes\": \"style\"}}\\n```'\n        try:\n            response = call_cloud_api(system, prompt, provider, api_key, temperature=0.3)\n            parsed = _parse_json_response(response)\n            bible = StoryTracker.empty_bible()\n            bible['characters'] = parsed.get('characters', {})\n            bible['plot_threads'] = parsed.get('plot_threads', [])\n            bible['style_notes'] = parsed.get('style_notes', '')\n            return bible\n        except Exception as e:\n            bible = StoryTracker.empty_bible()\n            bible['style_notes'] = f'(Bible init failed: {e})'\n            return bible\n\n    @staticmethod\n    def apply_updates(bible, updates):\n        bible = copy.deepcopy(bible)\n        for name, changes in updates.get('characters', {}).items():\n            if name in bible['characters']:\n                bible['characters'][name].update(changes)\n            else:\n                bible['characters'][name] = changes\n        existing_names = {t['name'] for t in bible['plot_threads']}\n        for thread in updates.get('plot_threads', []):\n            if thread['name'] in existing_names:\n                for i, t in enumerate(bible['plot_threads']):\n                    if t['name'] == thread['name']:\n                        bible['plot_threads'][i].update(thread)\n                        break\n            else:\n                bible['plot_threads'].append(thread)\n        if 'chapter_summary' in updates:\n            bible.setdefault('chapter_summaries', []).append(updates['chapter_summary'])\n        return bible\n\n    @staticmethod\n    def get_context_summary(bible, ch_num, lang):\n        if not bible or not bible.get('characters'):\n            return ''\n        parts = []\n        if lang == 'zh':\n            chars = []\n            for name, info in bible.get('characters', {}).items():\n                loc = info.get('location', '')\n                emo = info.get('emotional_state', '')\n                chars.append(f'{name}：{loc}，{emo}')\n            if chars:\n                parts.append('【人物状态】' + '；'.join(chars))\n            active = [t for t in bible.get('plot_threads', []) if t.get('status') == 'active']\n            if active:\n                parts.append(f\"【活跃线索】{'、'.join(t['name'] for t in active[:5])}\")\n            for s in bible.get('chapter_summaries', [])[-3:]:\n                parts.append(f\"【第{s.get('chapter', '?')}章】{s.get('summary', '')}\")\n        else:\n            chars = []\n            for name, info in bible.get('characters', {}).items():\n                loc = info.get('location', '')\n                emo = info.get('emotional_state', '')\n                chars.append(f'{name}: {loc}, {emo}')\n            if chars:\n                parts.append('[Characters] ' + '; '.join(chars))\n            active = [t for t in bible.get('plot_threads', []) if t.get('status') == 'active']\n            if active:\n                parts.append(f\"[Active threads] {', '.join(t['name'] for t in active[:5])}\")\n            for s in bible.get('chapter_summaries', [])[-3:]:\n                parts.append(f\"[Ch.{s.get('chapter', '?')}] {s.get('summary', '')}\")\n        return '\\n'.join(parts)\n\n\n# ---- Mastermind ----\nclass Mastermind:\n    @staticmethod\n    def _extract_chapter_outline(outline, ch_num):\n        pattern = rf'(?:###?\\s*(?:第{ch_num}章|Chapter\\s*{ch_num})[：:\\s]*)(.+?)(?=(?:###?\\s*(?:第\\d+章|Chapter\\s*\\d+))|$)'\n        match = re.search(pattern, outline, re.DOTALL)\n        return match.group(0).strip() if match else f'Chapter {ch_num}'\n\n    @staticmethod\n    def plan_chapter(outline, ch_num, bible, prev_ending, provider, api_key, lang):\n        chapter_outline = Mastermind._extract_chapter_outline(outline, ch_num)\n        bible_summary = StoryTracker.get_context_summary(bible, ch_num, lang)\n        if lang == 'zh':\n            system = '你是一位资深小说编辑。你的任务是将结构化的章节大纲转化为散文体的场景引子，供AI写手作为写作起点。场景引子必须是纯散文，不能包含任何标题、列表、或Markdown格式。'\n            prompt = f'请为第{ch_num}章创建写作计划。\\n\\n【本章大纲】\\n{chapter_outline}\\n\\n【故事圣经】\\n{bible_summary if bible_summary else \"（首章，无历史记录）\"}\\n\\n【上一章结尾】\\n{prev_ending[-800:] if prev_ending else \"（首章）\"}\\n\\n请输出JSON格式（必须合法JSON）：\\n```json\\n{{\"scene_primer\": \"用散文体写一段场景引子（200-400字），描述本章开场的环境、氛围和人物状态。必须是纯叙事散文。\", \"key_events\": [\"事件1\", \"事件2\", \"事件3\"], \"guidance\": \"写作指导\"}}\\n```'\n        else:\n            system = 'You are a senior fiction editor. Convert structured chapter outlines into prose scene primers for an AI writer. The scene primer MUST be pure prose — no headings, bullet points, or markdown.'\n            prompt = f'Create a writing plan for Chapter {ch_num}.\\n\\n[Chapter outline]\\n{chapter_outline}\\n\\n[Story bible]\\n{bible_summary if bible_summary else \"(First chapter, no history)\"}\\n\\n[End of previous chapter]\\n{prev_ending[-800:] if prev_ending else \"(First chapter)\"}\\n\\nOutput as JSON (must be valid JSON):\\n```json\\n{{\"scene_primer\": \"Write a prose scene primer (150-300 words). Must be pure narrative prose.\", \"key_events\": [\"Event 1\", \"Event 2\", \"Event 3\"], \"guidance\": \"Writing guidance\"}}\\n```'\n        try:\n            response = call_cloud_api(system, prompt, provider, api_key, temperature=0.7)\n            plan = _parse_json_response(response)\n            plan.setdefault('scene_primer', chapter_outline)\n            plan.setdefault('key_events', [])\n            plan.setdefault('guidance', '')\n            return plan\n        except Exception as e:\n            return {'scene_primer': chapter_outline, 'key_events': [], 'guidance': f'(Planning failed: {e})'}\n\n    @staticmethod\n    def review_and_update(plan, text, bible, ch_num, provider, api_key, lang):\n        key_events = ', '.join(plan.get('key_events', []))\n        bible_summary = StoryTracker.get_context_summary(bible, ch_num, lang)\n        if lang == 'zh':\n            system = '你同时扮演两个角色：1）小说编辑——审核章节质量；2）故事记录员——更新故事圣经。请以JSON格式输出。'\n            prompt = f'请审核以下生成的第{ch_num}章，并更新故事圣经。\\n\\n【写作计划的关键事件】\\n{key_events}\\n\\n【生成的章节文本】（前2000字）\\n{text[:3000]}\\n\\n【当前故事圣经】\\n{bible_summary if bible_summary else \"（空）\"}\\n\\n请输出JSON格式：\\n```json\\n{{\"review\": {{\"approved\": true, \"scores\": {{\"plot_adherence\": 8, \"prose_quality\": 7, \"character_consistency\": 8, \"engagement\": 7}}, \"issues\": [], \"regen_guidance\": \"\"}}, \"bible_updates\": {{\"characters\": {{}}, \"plot_threads\": [], \"chapter_summary\": {{\"chapter\": {ch_num}, \"summary\": \"概要\"}}}}}}\\n```'\n        else:\n            system = 'You play two roles: 1) Fiction editor — review chapter quality; 2) Story recorder — update the story bible. Output as JSON.'\n            prompt = f'Review Chapter {ch_num} and update the story bible.\\n\\n[Key events]\\n{key_events}\\n\\n[Generated text] (first 2000 words)\\n{text[:3000]}\\n\\n[Current bible]\\n{bible_summary if bible_summary else \"(empty)\"}\\n\\nOutput as JSON:\\n```json\\n{{\"review\": {{\"approved\": true, \"scores\": {{\"plot_adherence\": 8, \"prose_quality\": 7, \"character_consistency\": 8, \"engagement\": 7}}, \"issues\": [], \"regen_guidance\": \"\"}}, \"bible_updates\": {{\"characters\": {{}}, \"plot_threads\": [], \"chapter_summary\": {{\"chapter\": {ch_num}, \"summary\": \"summary\"}}}}}}\\n```'\n        try:\n            response = call_cloud_api(system, prompt, provider, api_key, temperature=0.3)\n            result = _parse_json_response(response)\n            result.setdefault('review', {'approved': True, 'scores': {}, 'issues': [], 'regen_guidance': ''})\n            result.setdefault('bible_updates', {})\n            return result\n        except Exception as e:\n            return {\n                'review': {'approved': True, 'scores': {}, 'issues': [f'Review failed: {e}'], 'regen_guidance': ''},\n                'bible_updates': {'chapter_summary': {'chapter': ch_num, 'summary': text[:100] + '...'}},\n            }\n\n\n# ---- Prompt Builder ----\nclass PromptBuilder:\n    @staticmethod\n    def build_chapter_prompt(instruction, scene_primer, prev_text, max_new_tokens, lang):\n        system = ZH_SYSTEM if lang == 'zh' else EN_SYSTEM\n        system_tokens = count_tokens(system)\n        instruction_tokens = count_tokens(instruction)\n        template_overhead = 15\n        # Sanitize primer: strip any markdown the cloud API may have included\n        scene_primer = re.sub(r'^#+\\s.*$', '', scene_primer, flags=re.MULTILINE)\n        scene_primer = re.sub(r'^\\s*[-*]\\s+', '', scene_primer, flags=re.MULTILINE)\n        scene_primer = re.sub(r'\\*\\*([^*]+)\\*\\*', r'\\1', scene_primer)\n        scene_primer = scene_primer.strip()\n        available = MAX_SEQ_LENGTH - system_tokens - instruction_tokens - template_overhead - max_new_tokens\n        available = max(available, 100)\n        primer_budget = min(count_tokens(scene_primer), available // 3)\n        context_budget = available - primer_budget\n        trimmed_primer = trim_to_token_budget(scene_primer, primer_budget, keep_end=False)\n        trimmed_context = trim_to_token_budget(prev_text, context_budget, keep_end=True)\n        parts = [instruction]\n        if trimmed_primer:\n            parts.append(trimmed_primer)\n        if trimmed_context:\n            parts.append(trimmed_context)\n        user_content = '\\n\\n'.join(parts)\n        user_tokens = count_tokens(user_content)\n        total = system_tokens + user_tokens + template_overhead + max_new_tokens\n        return system, user_content, {\n            'system_tokens': system_tokens, 'user_tokens': user_tokens,\n            'max_new_tokens': max_new_tokens, 'total_estimated': total,\n            'within_budget': total <= MAX_SEQ_LENGTH,\n        }\n\n\n# ---- Multi-pass generation ----\ndef generate_chapter_multipass(scene_primer, prev_text, num_passes, tokens_per_pass,\n                                lang, temperature, top_p,\n                                use_cloud=False, cloud_provider='', cloud_api_key=''):\n    instructions = _ZH_INSTRUCTIONS if lang == 'zh' else _EN_INSTRUCTIONS\n    builder = PromptBuilder()\n    accumulated = ''\n    pass_log = []\n    for i in range(num_passes):\n        instruction = random.choice(instructions)\n        if i == 0:\n            context = prev_text[-1200:] if prev_text else ''\n            primer = scene_primer\n        else:\n            context = accumulated[-1000:]\n            primer = ''\n        system, user_content, token_info = builder.build_chapter_prompt(\n            instruction, primer, context, tokens_per_pass, lang)\n        if use_cloud:\n            result = generate_text_cloud(user_content, system_prompt=system, max_new_tokens=tokens_per_pass,\n                                         temperature=temperature, provider=cloud_provider, api_key=cloud_api_key)\n        else:\n            result = generate_text(user_content, system_prompt=system, max_new_tokens=tokens_per_pass,\n                                   temperature=temperature, top_p=top_p, repetition_penalty=1.0)\n        accumulated += result\n        mode_label = 'cloud' if use_cloud else 'local'\n        pass_log.append(f'Pass {i+1}/{num_passes} ({mode_label}): +{len(result)} chars ({token_info[\"total_estimated\"]}/{MAX_SEQ_LENGTH} tokens)')\n    return accumulated, pass_log\n\n\n# ---- Legacy chapter generation ----\ndef generate_chapter_legacy(plot_outline, chapter_num, previous_text, style_notes,\n                            max_tokens, temperature, top_p, rep_penalty):\n    lang = detect_language(plot_outline)\n    ch = int(chapter_num)\n    pattern = rf'(?:###?\\s*(?:第{ch}章|Chapter\\s*{ch})[：:\\s]*)(.+?)(?=(?:###?\\s*(?:第\\d+章|Chapter\\s*\\d+))|$)'\n    match = re.search(pattern, plot_outline, re.DOTALL)\n    chapter_outline = match.group(0).strip() if match else f'Chapter {ch}'\n    if lang == 'zh':\n        prompt = f'## 小说大纲\\n{plot_outline}\\n\\n'\n        if previous_text:\n            ctx = previous_text[-2000:] if len(previous_text) > 2000 else previous_text\n            prompt += f'## 上一章结尾\\n{ctx}\\n\\n'\n        prompt += f'## 当前任务\\n请根据以上大纲，撰写第{ch}章的完整内容。\\n本章大纲：{chapter_outline}\\n\\n'\n        if style_notes:\n            prompt += f'风格要求：{style_notes}\\n\\n'\n        prompt += f'要求：\\n1. 以具体的场景描写开头\\n2. 通过对话和动作推动情节\\n3. 注意人物性格的一致性\\n4. 章节结尾要有悬念或转折\\n5. 写出完整的章节内容\\n\\n第{ch}章正文：\\n'\n    else:\n        prompt = f'## Novel Outline\\n{plot_outline}\\n\\n'\n        if previous_text:\n            ctx = previous_text[-2000:] if len(previous_text) > 2000 else previous_text\n            prompt += f'## End of Previous Chapter\\n{ctx}\\n\\n'\n        prompt += f'## Current Task\\nWrite the complete text of Chapter {ch}.\\nChapter outline: {chapter_outline}\\n\\n'\n        if style_notes:\n            prompt += f'Style notes: {style_notes}\\n\\n'\n        prompt += f'Requirements:\\n1. Open with vivid scene-setting\\n2. Drive the plot through dialogue and action\\n3. Maintain consistent characterization\\n4. End with a hook or turning point\\n5. Write the complete chapter\\n\\nChapter {ch}:\\n'\n    system = ZH_SYSTEM if lang == 'zh' else EN_SYSTEM\n    return generate_text(prompt, system_prompt=system, max_new_tokens=int(max_tokens),\n                         temperature=temperature, top_p=top_p, repetition_penalty=rep_penalty)\n\n\n# ---- Full pipeline ----\ndef generate_chapter_pipeline(outline, ch_num, chapters_state, style_notes,\n                              max_tokens, temperature, top_p, rep_penalty,\n                              enable_agents, num_passes, provider, api_key, bible_state,\n                              writer_mode='Local Model'):\n    use_cloud = writer_mode == 'Cloud API'\n    ch_num = int(ch_num)\n    lang = detect_language(outline)\n\n    if not use_cloud and model is None:\n        msg = \"Please load a local model first, or switch Writer Mode to 'Cloud API'.\"\n        return msg, chapters_state, bible_state, msg, ''\n\n    if use_cloud and not api_key.strip():\n        msg = 'Cloud writer mode requires an API key. Please enter it in API Settings.'\n        return msg, chapters_state, bible_state, msg, ''\n\n    # Legacy mode (local only, no agents)\n    if not enable_agents:\n        if use_cloud:\n            msg = 'Legacy mode requires a local model. Please enable agents for cloud-only writing.'\n            return msg, chapters_state, bible_state, msg, ''\n        result = generate_chapter_legacy(outline, ch_num, chapters_state, style_notes,\n                                         max_tokens, temperature, top_p, rep_penalty)\n        sep = f'\\n\\n{\"=\"*40}\\n第{ch_num}章 / Chapter {ch_num}\\n{\"=\"*40}\\n\\n'\n        new_acc = chapters_state + sep + result if chapters_state else result\n        return result, new_acc, bible_state, 'Legacy mode (agents disabled).', ''\n\n    # Agents require API key\n    if not api_key.strip():\n        msg = 'Agent mode requires an API key. Please enter it in API Settings.'\n        return msg, chapters_state, bible_state, msg, ''\n\n    # Agent mode\n    log_lines = []\n    def log(msg):\n        log_lines.append(f'[{time.strftime(\"%H:%M:%S\")}] {msg}')\n\n    mode_label = 'cloud' if use_cloud else 'local'\n    log(f'Writer mode: {mode_label}')\n\n    if not bible_state or not bible_state.get('characters'):\n        log('Initializing story bible from outline...')\n        bible_state = StoryTracker.initialize_from_outline(outline, provider, api_key, lang)\n        log(f'Bible initialized: {len(bible_state.get(\"characters\", {}))} characters')\n\n    log(f'Mastermind planning chapter {ch_num}...')\n    prev_ending = chapters_state[-1200:] if chapters_state else ''\n    plan = Mastermind.plan_chapter(outline, ch_num, bible_state, prev_ending, provider, api_key, lang)\n    log(f'Scene primer: {plan[\"scene_primer\"][:150].replace(chr(10), \" \")}...')\n    if plan['key_events']:\n        log(f'Key events: {\", \".join(plan[\"key_events\"][:3])}')\n\n    tokens_per_pass = min(int(max_tokens) // int(num_passes), 1500)\n    if tokens_per_pass < 512:\n        log(f'Warning: tokens_per_pass ({tokens_per_pass}) below minimum 512, clamping up')\n        tokens_per_pass = 512\n    log(f'Generating chapter ({int(num_passes)} passes, {tokens_per_pass} tokens/pass, {mode_label})...')\n    result, pass_log = generate_chapter_multipass(\n        plan['scene_primer'], prev_ending, int(num_passes), tokens_per_pass, lang, temperature, top_p,\n        use_cloud=use_cloud, cloud_provider=provider, cloud_api_key=api_key)\n    for pl in pass_log:\n        log(pl)\n    log(f'Total generated: {len(result)} chars')\n\n    log('Reviewing chapter + updating story bible...')\n    review_result = Mastermind.review_and_update(plan, result, bible_state, ch_num, provider, api_key, lang)\n    review = review_result.get('review', {})\n    bible_updates = review_result.get('bible_updates', {})\n    bible_state = StoryTracker.apply_updates(bible_state, bible_updates)\n\n    approved = review.get('approved', True)\n    scores = review.get('scores', {})\n    issues = review.get('issues', [])\n    status = 'APPROVED' if approved else 'NEEDS REVISION'\n    log(f'Review: {status}')\n    if scores:\n        log(f'Scores: {\", \".join(f\"{k}: {v}/10\" for k, v in scores.items())}')\n\n    review_text = f'Status: {status}\\n'\n    if scores:\n        review_text += 'Scores:\\n' + ''.join(f'  {k}: {v}/10\\n' for k, v in scores.items())\n    if issues:\n        review_text += 'Issues:\\n' + ''.join(f'  - {issue}\\n' for issue in issues)\n    if not approved and review.get('regen_guidance'):\n        review_text += f'\\nRevision guidance: {review[\"regen_guidance\"]}\\n'\n\n    sep = f'\\n\\n{\"=\"*40}\\n第{ch_num}章 / Chapter {ch_num}\\n{\"=\"*40}\\n\\n'\n    new_acc = chapters_state + sep + result if chapters_state else result\n    return result, new_acc, bible_state, '\\n'.join(log_lines), review_text\n\n\ndef continue_writing(existing_text, instruction, max_tokens, temperature, top_p, rep_penalty,\n                     writer_mode='Local Model', api_provider='Gemini', api_key=''):\n    use_cloud = writer_mode == 'Cloud API'\n    if not use_cloud and model is None:\n        return \"Please load a local model first, or switch Writer Mode to 'Cloud API'.\"\n    if use_cloud and not api_key.strip():\n        return 'Cloud writer mode requires an API key. Please enter it in API Settings.'\n\n    lang = detect_language(existing_text)\n    if not instruction:\n        instruction = '续写这段叙事，保持原文的风格和节奏。' if lang == 'zh' else 'Continue the narrative in the established style.'\n    prompt = instruction + '\\n\\n' + existing_text\n    system = ZH_SYSTEM if lang == 'zh' else EN_SYSTEM\n\n    if use_cloud:\n        return generate_text_cloud(prompt, system_prompt=system, max_new_tokens=int(max_tokens),\n                                   temperature=temperature, provider=api_provider, api_key=api_key)\n    else:\n        return generate_text(prompt, system_prompt=system, max_new_tokens=int(max_tokens),\n                             temperature=temperature, top_p=top_p, repetition_penalty=rep_penalty)\n\n\n# ---- Determine default writer mode from config ----\n_default_writer_mode = 'Cloud API' if WRITER_MODE == 'cloud_api' else 'Local Model'\n\n# ---- Build Gradio UI ----\nwith gr.Blocks(title='Novel Writer Studio') as app:\n    gr.Markdown('# Novel Writer Studio (Colab)')\n    gr.Markdown(\n        '*Cloud AI develops your plot + orchestrates agents. '\n        'Chapters can be written by your fine-tuned local model OR entirely via cloud API (no GPU needed).*'\n    )\n\n    # Shared state\n    all_chapters = gr.State('')\n    bible_state = gr.State({})\n\n    # API Settings\n    with gr.Accordion('API Settings', open=True):\n        gr.Markdown('Enter your API key. Used for plot generation, agent orchestration, and cloud writing.')\n        with gr.Row():\n            api_provider = gr.Radio(['Gemini', 'GPT'], value='Gemini', label='Provider')\n            api_key_input = gr.Textbox(label='API Key', type='password', placeholder='Paste your API key...', scale=3)\n\n    # Writer Mode\n    with gr.Accordion('Writer Mode', open=True):\n        gr.Markdown(\n            '**Local Model**: Uses your fine-tuned LoRA model (requires GPU). Best quality for style-specific prose.\\n\\n'\n            '**Cloud API**: Uses Gemini/GPT for chapter writing too. No GPU needed. '\n            'Loses LoRA style but keeps all agent features (planning, review, story bible).'\n        )\n        writer_mode = gr.Radio(\n            ['Local Model', 'Cloud API'], value=_default_writer_mode,\n            label='Chapter Writer', info='Choose what writes the chapter prose')\n\n    # Agent Settings\n    with gr.Accordion('Agent Settings', open=False):\n        gr.Markdown(\n            '**Multi-Agent System**: Mastermind + Story Tracker orchestrate chapter generation.\\n'\n            '- **Mastermind**: Converts outline to prose scene primers, reviews output\\n'\n            '- **Story Tracker**: Maintains character states, plot threads, summaries\\n'\n            '- Uses 2-3 cloud API calls per chapter'\n        )\n        with gr.Row():\n            enable_agents = gr.Checkbox(value=True, label='Enable Agents',\n                                        info='Use Mastermind + Story Tracker')\n            num_passes = gr.Slider(1, 5, value=3, step=1, label='Generation Passes',\n                                   info='More passes = longer chapters (~1000-1500 chars each)')\n\n    # Generation settings\n    with gr.Accordion('Chapter Generation Settings', open=False):\n        with gr.Row():\n            max_tokens_slider = gr.Slider(128, 4096, value=2048, step=128, label='Max New Tokens')\n            temperature_slider = gr.Slider(0.1, 2.0, value=0.8, step=0.05, label='Temperature')\n        with gr.Row():\n            top_p_slider = gr.Slider(0.1, 1.0, value=0.9, step=0.05, label='Top-P')\n            rep_penalty_slider = gr.Slider(1.0, 2.0, value=1.0, step=0.05, label='Repetition Penalty',\n                                           info='1.0 recommended (matches training)')\n\n    with gr.Tabs():\n        # Tab 1: Story Workshop\n        with gr.Tab('Story Workshop'):\n            gr.Markdown('### Step 1: Your Story Idea')\n            with gr.Row():\n                with gr.Column(scale=3):\n                    story_idea = gr.Textbox(\n                        label='Story Idea', lines=4,\n                        placeholder='例：在一个武林高手辈出的时代，一个失忆的少年在雪山醒来...\\n\\nOr: In a world where magic is fueled by music...',\n                    )\n                with gr.Column(scale=1):\n                    num_chapters = gr.Slider(3, 20, value=8, step=1, label='Chapters')\n                    develop_btn = gr.Button('Develop Plot (Cloud AI)', variant='primary', size='lg')\n\n            gr.Markdown('### Step 2: Plot Outline')\n            gr.Markdown('Generated by Gemini/GPT. Review and edit freely.')\n            plot_outline = gr.Textbox(label='Plot Outline (editable)', lines=25,\n                                     placeholder='Click Develop Plot to generate...')\n            develop_btn.click(develop_plot_api, [story_idea, num_chapters, api_provider, api_key_input], plot_outline)\n\n            gr.Markdown('---')\n            gr.Markdown('### Step 3: Generate Chapters')\n            gr.Markdown('Writes each chapter using your chosen Writer Mode. Enable agents for multi-pass generation with plot tracking and review.')\n            with gr.Row():\n                with gr.Column(scale=1):\n                    chapter_num = gr.Slider(1, 20, value=1, step=1, label='Chapter Number')\n                    style_notes = gr.Textbox(label='Style Notes (optional)', lines=2, placeholder='e.g., 多用对话')\n                    gen_chapter_btn = gr.Button('Generate Chapter', variant='primary', size='lg')\n                with gr.Column(scale=3):\n                    chapter_output = gr.Textbox(label='Generated Chapter', lines=25)\n\n            with gr.Accordion('Generation Log', open=False):\n                gen_log_display = gr.Textbox(label='Agent Activity Log', lines=12, interactive=False)\n\n            with gr.Accordion('Chapter Review', open=False):\n                review_display = gr.Textbox(label='Mastermind Review', lines=8, interactive=False)\n\n            gen_chapter_btn.click(\n                generate_chapter_pipeline,\n                [plot_outline, chapter_num, all_chapters, style_notes,\n                 max_tokens_slider, temperature_slider, top_p_slider, rep_penalty_slider,\n                 enable_agents, num_passes, api_provider, api_key_input, bible_state,\n                 writer_mode],\n                [chapter_output, all_chapters, bible_state, gen_log_display, review_display],\n            )\n\n            with gr.Accordion('All Generated Chapters', open=False):\n                all_chapters_display = gr.Textbox(label='Full Story So Far', lines=30, interactive=False)\n                refresh_btn = gr.Button('Refresh')\n                refresh_btn.click(lambda x: x, all_chapters, all_chapters_display)\n\n                export_btn = gr.Button('Export Story to File')\n                export_file = gr.File(label='Download')\n\n                def export_story(text):\n                    if not text:\n                        return None\n                    fpath = f'/content/story_{time.strftime(\"%Y%m%d_%H%M%S\")}.txt'\n                    Path(fpath).write_text(text, encoding='utf-8')\n                    return fpath\n\n                export_btn.click(export_story, all_chapters, export_file)\n\n        # Tab 2: Story Bible\n        with gr.Tab('Story Bible'):\n            gr.Markdown('### Story Bible\\nTracks character states, plot threads, and chapter summaries.')\n            bible_display = gr.JSON(label='Current Story Bible')\n            with gr.Row():\n                refresh_bible_btn = gr.Button('Refresh Display')\n                init_bible_btn = gr.Button('Initialize Bible from Outline', variant='primary')\n                clear_bible_btn = gr.Button('Clear Bible', variant='stop')\n\n            bible_manual_notes = gr.Textbox(\n                label='Manual Notes (added to style_notes)', lines=3,\n                placeholder=\"Add your own notes (e.g., 'the sword is named Frostbite')\")\n            add_notes_btn = gr.Button('Add Notes to Bible')\n            bible_status = gr.Textbox(label='Status', interactive=False, lines=1)\n\n            def refresh_bible(bible):\n                return bible\n\n            def init_bible_from_outline(outline, provider, key, bible):\n                if not outline.strip():\n                    return bible, bible, 'No outline to initialize from.'\n                if not key.strip():\n                    return bible, bible, f'Please enter your {provider} API key.'\n                lang = detect_language(outline)\n                new_bible = StoryTracker.initialize_from_outline(outline, provider, key, lang)\n                n_chars = len(new_bible.get('characters', {}))\n                return new_bible, new_bible, f'Bible initialized: {n_chars} characters found.'\n\n            def clear_bible():\n                return StoryTracker.empty_bible()\n\n            def add_manual_notes(bible, notes):\n                if not notes.strip():\n                    return bible\n                bible = copy.deepcopy(bible)\n                existing = bible.get('style_notes', '')\n                bible['style_notes'] = (existing + '\\n' + notes).strip() if existing else notes\n                return bible\n\n            refresh_bible_btn.click(refresh_bible, bible_state, bible_display)\n            init_bible_btn.click(init_bible_from_outline,\n                                 [plot_outline, api_provider, api_key_input, bible_state],\n                                 [bible_state, bible_display, bible_status])\n            clear_bible_btn.click(clear_bible, outputs=[bible_state])\n            add_notes_btn.click(add_manual_notes, [bible_state, bible_manual_notes], bible_state)\n\n        # Tab 3: Free Write\n        with gr.Tab('Free Write'):\n            gr.Markdown('### Direct Generation')\n            gr.Markdown('Write freely — uses your chosen Writer Mode.')\n            with gr.Row():\n                with gr.Column():\n                    free_context = gr.Textbox(label='Context / Previous Text', lines=10,\n                                             placeholder='Paste existing text for continuation...')\n                    free_instruction = gr.Textbox(label='Instruction (optional)', lines=2,\n                                                 placeholder='e.g., 续写这段战斗场景')\n                    free_gen_btn = gr.Button('Generate', variant='primary', size='lg')\n                with gr.Column():\n                    free_output = gr.Textbox(label='Generated Text', lines=20)\n                    append_btn = gr.Button('Append to Context')\n\n            free_gen_btn.click(\n                continue_writing,\n                [free_context, free_instruction, max_tokens_slider, temperature_slider, top_p_slider, rep_penalty_slider,\n                 writer_mode, api_provider, api_key_input],\n                free_output,\n            )\n            append_btn.click(lambda ctx, out: ctx + '\\n' + out if ctx else out, [free_context, free_output], free_context)\n\n        # Tab 4: Quick Test\n        with gr.Tab('Quick Test'):\n            gr.Markdown('### Test with a single prompt')\n            gr.Markdown('Uses your chosen Writer Mode.')\n            test_prompt = gr.Textbox(label='Prompt', lines=4,\n                                    placeholder='月色如霜，照在悬崖边两道对峙的身影上...')\n            test_btn = gr.Button('Generate', variant='primary')\n            test_output = gr.Textbox(label='Output', lines=15)\n\n            def run_test(prompt, max_t, temp, tp, rp, wmode, provider, key):\n                if wmode == 'Cloud API':\n                    return generate_text_cloud(prompt, max_new_tokens=int(max_t), temperature=temp,\n                                               provider=provider, api_key=key)\n                return generate_text(prompt, max_new_tokens=int(max_t), temperature=temp, top_p=tp, repetition_penalty=rp)\n\n            test_btn.click(run_test,\n                           [test_prompt, max_tokens_slider, temperature_slider, top_p_slider, rep_penalty_slider,\n                            writer_mode, api_provider, api_key_input],\n                           test_output)\n\nprint('Launching Web UI...')\napp.launch(share=True)",
   "execution_count": null,
   "outputs": []
  }
 ]
}