{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "A100"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Novel Writer Studio - Web UI (Colab)\n\nRun a full story-writing web UI on Google Colab.\n\n**Two modes:**\n- **Local Model** (GPU required): Your fine-tuned LoRA model writes chapters with trained literary style. Needs Colab Pro for 32B model (A100), free tier works for 8B (T4).\n- **Cloud API** (no GPU needed): Gemini/GPT writes chapters. Works on **free Colab** — no GPU runtime required. Loses LoRA style but keeps all agent features.\n\n**Workflow:**\n1. **Cloud AI** (Gemini / GPT) develops your story idea into a detailed plot outline\n2. **Multi-agent system** (Mastermind + Story Tracker) orchestrates chapter generation\n3. **Your chosen writer** (local model or cloud API) generates the chapter prose\n\n### How to use\n1. Run cells in order (skip cells 3-4 if using Cloud API mode only)\n2. Click the Gradio public link to open the UI\n3. Enter your API key, pick your Writer Mode, and generate!"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "#@title Configuration { display-mode: \"form\" }\n\n#@markdown ### Writer Mode\n#@markdown - **local_model**: Use your fine-tuned LoRA model (requires GPU runtime)\n#@markdown - **cloud_api**: Use Gemini/GPT for writing (no GPU needed, skip cells 3-4)\nWRITER_MODE = \"cloud_api\" #@param [\"local_model\", \"cloud_api\"]\n\n#@markdown ---\n#@markdown ### Model Selection (only for local_model mode)\n#@markdown Choose the base model that matches your LoRA adapter.\nMODEL_CHOICE = \"qwen3_32b\" #@param [\"qwen3_4b\", \"qwen3_8b\", \"qwen3_14b\", \"qwen3_32b\", \"llama31_8b\", \"gemma2_9b\", \"mistral_nemo_12b\"]\n\n#@markdown ### LoRA Upload Mode\n#@markdown - **huggingface_hub**: Download from Hugging Face (recommended)\n#@markdown - **upload_zip**: Upload your LoRA adapter as a .zip file\n#@markdown - **google_drive**: Load from Google Drive path\nLORA_MODE = \"huggingface_hub\" #@param [\"huggingface_hub\", \"upload_zip\", \"google_drive\"]\n\n#@markdown ### Hugging Face repo ID (only if LORA_MODE = huggingface_hub)\nHF_REPO_ID = \"YOUR_USER/qwen3_32b_novel_lora\" #@param {type:\"string\"}\n\n#@markdown ### Google Drive path (only if LORA_MODE = google_drive)\nDRIVE_LORA_PATH = \"/content/drive/MyDrive/qwen3_32b_novel_lora\" #@param {type:\"string\"}\n\nMODEL_CONFIGS = {\n    'qwen3_4b': 'unsloth/Qwen3-4B',\n    'qwen3_8b': 'unsloth/Qwen3-8B',\n    'qwen3_14b': 'unsloth/Qwen3-14B',\n    'qwen3_32b': 'unsloth/Qwen3-32B',\n    'llama31_8b': 'unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit',\n    'gemma2_9b': 'unsloth/gemma-2-9b-it-bnb-4bit',\n    'mistral_nemo_12b': 'unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit',\n}\n\nBASE_MODEL = MODEL_CONFIGS[MODEL_CHOICE]\n\nprint(f'Writer mode: {WRITER_MODE}')\nif WRITER_MODE == 'local_model':\n    print(f'Base model: {BASE_MODEL}')\n    print(f'LoRA mode: {LORA_MODE}')\nelse:\n    print('Cloud API mode — no GPU or local model needed.')\n    print('You can skip cells 3 and 4, go straight to cell 5 (Install deps) then cell 6 (Launch UI).')",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "#@title Install dependencies\nimport subprocess, sys\n\n# Core deps (always needed)\nsubprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'gradio', 'google-genai', 'openai', 'huggingface_hub'])\n\nif WRITER_MODE == 'local_model':\n    # GPU deps\n    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'unsloth'])\n    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', '--force-reinstall',\n                           '--no-cache-dir', '--no-deps',\n                           'git+https://github.com/unslothai/unsloth.git'])\n    import torch\n    print(f'GPU: {torch.cuda.get_device_name(0)}')\n    print(f'VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')\nelse:\n    print('Cloud API mode — GPU dependencies skipped.')\n\nprint('Setup complete!')",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "#@title Upload / locate LoRA adapter (skip if Cloud API mode)\nimport os, zipfile\nfrom pathlib import Path\n\nLORA_PATH = None\n\nif WRITER_MODE == 'cloud_api':\n    print('Cloud API mode — no LoRA adapter needed. Skip this cell.')\n\nelif LORA_MODE == 'huggingface_hub':\n    from huggingface_hub import snapshot_download\n    print(f'Downloading LoRA from Hugging Face: {HF_REPO_ID}')\n    LORA_PATH = snapshot_download(\n        repo_id=HF_REPO_ID,\n        local_dir=f'/content/{HF_REPO_ID.split(\"/\")[-1]}',\n    )\n    print(f'Downloaded to: {LORA_PATH}')\n\nelif LORA_MODE == 'upload_zip':\n    from google.colab import files as colab_files\n    print('Upload your LoRA adapter zip file:')\n    uploaded = colab_files.upload()\n    for name in uploaded:\n        if name.endswith('.zip'):\n            with zipfile.ZipFile(name, 'r') as z:\n                z.extractall('/content/')\n            for d in Path('/content').iterdir():\n                if d.is_dir() and (d / 'adapter_config.json').exists():\n                    LORA_PATH = str(d)\n                    break\n        else:\n            os.makedirs('/content/lora_adapter', exist_ok=True)\n            os.rename(name, f'/content/lora_adapter/{name}')\n            LORA_PATH = '/content/lora_adapter'\n\nelif LORA_MODE == 'google_drive':\n    from google.colab import drive\n    drive.mount('/content/drive')\n    LORA_PATH = DRIVE_LORA_PATH\n\nif LORA_PATH and Path(LORA_PATH).exists():\n    print(f'LoRA adapter found: {LORA_PATH}')\n    for f in sorted(Path(LORA_PATH).iterdir()):\n        print(f'  {f.name} ({f.stat().st_size / 1024:.0f} KB)')\nelif WRITER_MODE == 'local_model':\n    print(f'ERROR: LoRA adapter not found at {LORA_PATH}')",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "#@title Load model + LoRA (skip if Cloud API mode)\n\nmodel = None\ntokenizer = None\n\nif WRITER_MODE == 'cloud_api':\n    print('Cloud API mode — no model to load. Skip this cell.')\nelse:\n    from unsloth import FastLanguageModel\n    import torch\n\n    print(f'Loading base model: {BASE_MODEL}')\n    model, tokenizer = FastLanguageModel.from_pretrained(\n        model_name=BASE_MODEL,\n        max_seq_length=4096,\n        dtype=None,\n        load_in_4bit=True,\n    )\n\n    print(f'Applying LoRA from: {LORA_PATH}')\n    from peft import PeftModel\n    model = PeftModel.from_pretrained(model, LORA_PATH)\n    FastLanguageModel.for_inference(model)\n\n    vram = torch.cuda.memory_allocated() / 1e9\n    print(f'\\nModel loaded! VRAM used: {vram:.1f} GB')",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "#@title Launch Web UI\nimport gradio as gr\nimport copy, re, time, json, random\nfrom pathlib import Path\n\n# ---- System prompts (matching training) ----\nZH_SYSTEM = (\n    '你是一位经验丰富的中文小说作家，擅长构建沉浸式的叙事场景。请根据给定的上下文续写故事，要求：\\n'\n    '1. 保持与原文一致的叙事视角和文风\\n'\n    '2. 通过具体的动作、对话和环境描写推动情节发展\\n'\n    '3. 角色的言行应符合其性格特征和当前情境\\n'\n    '4. 善用感官细节（视觉、听觉、触觉、嗅觉）营造氛围\\n'\n    '5. 对话要自然生动，符合角色身份和说话习惯\\n'\n    '6. 避免空洞的心理独白，用行动和细节展现人物内心'\n)\n\nEN_SYSTEM = (\n    'You are an accomplished fiction author with a gift for immersive storytelling. '\n    'Continue the narrative following these principles:\\n'\n    '1. Maintain the established point of view, voice, and tonal register\\n'\n    '2. Advance the plot through concrete action, dialogue, and environmental detail\\n'\n    '3. Show character emotion through behavior, body language, and subtext — not exposition\\n'\n    '4. Engage multiple senses (sight, sound, touch, smell, taste) to ground scenes\\n'\n    '5. Write dialogue that reveals character, creates tension, and sounds natural\\n'\n    '6. Vary sentence rhythm — mix short punchy lines with longer flowing passages'\n)\n\n# ---- Instruction pools (from training data) ----\n_ZH_INSTRUCTIONS = [\n    '续写这段叙事，保持原文的风格和节奏。',\n    '以相同的文风继续这个故事。',\n    '根据已有的情节和人物设定，续写下一段。',\n    '保持叙事视角不变，继续推进故事发展。',\n    '用生动的细节描写续写这个场景。',\n    '通过对话和动作描写推进下面的情节。',\n    '延续当前的叙事氛围，写出接下来发生的事。',\n    '以细腻的笔触续写这段文字。',\n    '按照原文的叙事节奏，写出故事的下一部分。',\n    '继续描绘这个场景中的人物和事件。',\n    '用符合原文风格的语言续写故事。',\n    '展开叙述，让故事自然地向前发展。',\n    '保持文风一致，续写接下来的情节。',\n    '以沉浸式的叙事方式继续这段故事。',\n    '描绘接下来的场景，注意环境和人物的刻画。',\n    '用简洁有力的文字续写这段叙事。',\n    '继续讲述这个故事，注意情感的表达。',\n    '以自然流畅的文笔续写下一段。',\n    '延续原文的基调，推进故事走向。',\n    '用丰富的感官描写续写这个场景。',\n]\n\n_EN_INSTRUCTIONS = [\n    'Continue the narrative in the established style.',\n    'Write the next passage, maintaining the existing voice and tone.',\n    'Advance the story using vivid sensory details.',\n    'Continue this scene with natural dialogue and action.',\n    'Extend the narrative, preserving the point of view and pacing.',\n    'Write what happens next, staying true to the characters.',\n    'Continue the story with concrete, immersive description.',\n    'Carry the narrative forward in the same literary register.',\n    'Write the next segment, matching the established rhythm.',\n    'Develop this scene further with authentic detail.',\n    'Push the story forward through action and dialogue.',\n    'Continue in the same voice, advancing the plot naturally.',\n    'Write the following passage in the style of the preceding text.',\n    'Extend this scene with attention to atmosphere and character.',\n    'Continue the narrative arc with engaging prose.',\n    'Write what comes next, maintaining tension and pacing.',\n    'Advance the story, weaving in environmental detail.',\n    'Continue with prose that matches the tone and texture of the original.',\n    'Develop the next beat of the story with precise language.',\n    'Carry the scene forward, balancing action with description.',\n]\n\n# ---- Token utilities ----\nMAX_SEQ_LENGTH = 4096\n\n\ndef count_tokens(text):\n    if tokenizer is not None:\n        return len(tokenizer.encode(text, add_special_tokens=False))\n    cjk = sum(1 for c in text if '\\u4e00' <= c <= '\\u9fff')\n    return cjk + len(text.split()) - cjk // 2\n\n\ndef _find_sentence_boundary(text, max_chars, from_end=False):\n    sentence_ends_cjk = re.compile(r'[。！？…]+')\n    sentence_ends_en = re.compile(r'[.!?][\\u201c\\u201d\\u2018\\u2019\"\\')\\u300d\\uff09]*(?:\\s|\\n)')\n    if from_end:\n        search_region = text[-max_chars:] if len(text) > max_chars else text\n        offset = max(0, len(text) - max_chars)\n        best = 0\n        for m in sentence_ends_cjk.finditer(search_region):\n            best = m.end()\n            break\n        for m in sentence_ends_en.finditer(search_region):\n            if m.end() < best or best == 0:\n                best = m.end()\n            break\n        return offset + best if best > 0 else offset\n    else:\n        search_region = text[:max_chars]\n        best = max_chars\n        for m in sentence_ends_cjk.finditer(search_region):\n            best = m.end()\n        for m in sentence_ends_en.finditer(search_region):\n            best = m.end()\n        return best\n\n\ndef trim_to_token_budget(text, max_tokens, keep_end=True):\n    current = count_tokens(text)\n    if current <= max_tokens:\n        return text\n    ratio = max_tokens / max(current, 1)\n    target_chars = int(len(text) * ratio * 0.95)\n    if keep_end:\n        start = _find_sentence_boundary(text, len(text) - target_chars, from_end=True)\n        trimmed = text[start:]\n    else:\n        end = _find_sentence_boundary(text, target_chars, from_end=False)\n        trimmed = text[:end]\n    if count_tokens(trimmed) > max_tokens:\n        trimmed = text[-target_chars:] if keep_end else text[:target_chars]\n    return trimmed.strip()\n\n\ndef detect_language(text):\n    cjk = sum(1 for c in text[:300] if '\\u4e00' <= c <= '\\u9fff')\n    return 'zh' if cjk > len(text[:300]) * 0.15 else 'en'\n\n\n# ---- Cloud API helpers ----\ndef _build_plot_prompt(idea, target_chapters, lang):\n    \"\"\"Build plot prompt with high-level story arcs (not chapter-by-chapter).\"\"\"\n    if lang == 'zh':\n        system = (\n            '你是一位资深的小说策划编辑和故事架构师。你擅长从简单的故事构思中发展出宏大、'\n            '引人入胜的长篇小说大纲。你的大纲以故事弧线和幕（Act）为结构单位——不要逐章列举。'\n            '大纲应包含极其丰富的细节：人物关系网、世界观设定、核心冲突的多层递进、'\n            '以及每个故事弧线中的关键转折点。这份大纲将作为AI逐章生成小说的总蓝图。'\n        )\n        prompt = f\"\"\"请基于以下故事构思，创作一个宏大而详细的长篇小说大纲。\n目标篇幅：约{target_chapters}章的长篇故事。\n\n故事构思：{idea}\n\n请严格按照以下格式输出（使用中文）：\n\n## 小说标题\n[一个引人入胜的标题]\n\n## 故事背景\n[详细的世界观和背景设定，至少300字。包括：时代背景、地理环境、社会体制、文化风俗、历史渊源、特殊设定。要有层次感——大世界背景、故事发生的具体区域、以及主角的生活圈]\n\n## 主要人物\n（至少6个角色：主角、对手、盟友、导师、情感关联角色、关键配角）\n- **[角色全名]**（[年龄/外貌简述]）：[性格特点——至少3个关键词]，[身份背景]，[核心动机]，[人物弧光——从故事开始到结束的完整变化轨迹]，[秘密或隐藏面]，[关键人际关系]\n\n## 人物关系网\n[用文字描述主要人物之间的关系图谱：谁和谁是盟友/对手/师徒/恋人/亲人，关系如何随故事发展变化]\n\n## 核心冲突\n[故事的多层矛盾结构，至少200字：\n- 外部冲突：主角面对的外在威胁或障碍\n- 内部冲突：主角的内心挣扎和道德困境\n- 社会冲突：更宏大的社会/世界层面的矛盾\n- 关系冲突：人物之间的矛盾和张力]\n\n## 故事弧线\n\n### 第一幕：起（约占全书前20%）\n**核心目标**：[这一幕要完成什么——建立世界、引入角色、设置悬念]\n**起始状态**：[故事开始时主角和世界的状态]\n**关键场景**：\n1. [开篇场景：具体描述]\n2. [引发事件：打破日常的事件]\n3. [第一个转折：推动主角踏上旅程的催化剂]\n**情感基调**：[从平静到不安，或从混乱到希望等]\n**埋下的伏笔**：[在这一幕中埋下哪些伏笔]\n\n### 第二幕上半：承（约占全书20%-50%）\n**核心目标**：[展开冒险、加深冲突、发展关系]\n**关键场景**：\n1. [重要事件1]\n2. [重要事件2]\n3. [重要事件3]\n4. [中点转折：故事中点的重大转折或启示]\n**人物发展**：[各角色在这一段的成长和变化]\n**情感基调**：[逐渐升温的紧张感]\n**关键谜团/悬念**：[让读者欲罢不能的谜团]\n\n### 第二幕下半：转（约占全书50%-75%）\n**核心目标**：[加剧危机、黑暗时刻、联盟瓦解或重组]\n**关键场景**：\n1. [危机加剧的关键事件]\n2. [背叛/揭秘/重大失败]\n3. [最低谷：主角面临最大的困境]\n**人物发展**：[角色弧光的关键转变]\n**情感基调**：[从希望到绝望的转变]\n**伏笔揭示**：[哪些之前埋下的伏笔在这里揭示]\n\n### 第三幕：合（约占全书后25%）\n**核心目标**：[走向高潮、解决冲突、完成角色弧光]\n**关键场景**：\n1. [重整旗鼓/获得关键力量或信息]\n2. [最终对决/高潮场景的详细描述]\n3. [结局：各人物的命运，故事的收束]\n**人物发展**：[角色弧光的完成]\n**情感基调**：[从绝境到高潮的爆发]\n**主题升华**：[故事的核心主题如何在结尾得到升华]\n\n## 伏笔与线索网络\n[列出5-8个贯穿全文的伏笔和线索，说明它们在哪个弧线中出现和揭示]\n\n## 主题与象征\n[故事的深层主题和反复出现的象征物、意象]\n\n## 写作风格指导\n[叙事视角、语言风格、节奏控制、对话风格、描写偏重]\n\n请确保：\n1. 故事弧线之间有清晰的因果递进和张力升级\n2. 人物发展有合理的成长曲线\n3. 伏笔和悬念形成网络，前后呼应\n4. 大纲足够详细，能支撑{target_chapters}章的长篇创作\n5. 每个弧线的关键场景描写具体到可以直接指导写作\"\"\"\n    else:\n        system = (\n            'You are a senior fiction editor and story architect. You excel at developing '\n            'simple story concepts into expansive, compelling novel outlines. Your outlines '\n            'use story arcs and acts as structural units — NOT individual chapters. '\n            'Include exceptionally rich detail: character relationship webs, world-building, '\n            'multi-layered conflict escalation, and key turning points in each arc. '\n            'This outline will serve as the master blueprint for AI chapter-by-chapter generation.'\n        )\n        prompt = f\"\"\"Based on the following story idea, create an expansive and detailed novel outline.\nTarget length: approximately {target_chapters} chapters.\n\nStory idea: {idea}\n\nPlease use this exact format:\n\n## Title\n[A compelling title]\n\n## Setting\n[Detailed world-building, at least 300 words. Layer it — broad world, specific region, protagonist's immediate circle]\n\n## Main Characters\n(At least 6 characters: protagonist, antagonist, ally, mentor, love interest, key supporting)\n- **[Full Name]** ([Age/Appearance]): [Personality — at least 3 key traits], [Background], [Core motivation], [Character arc — complete transformation], [Secret or hidden side], [Key relationships]\n\n## Relationship Web\n[Describe the relationship map between characters and how it evolves]\n\n## Central Conflict\n[Multi-layered conflict structure, at least 200 words:\n- External conflict\n- Internal conflict\n- Social conflict\n- Relationship conflict]\n\n## Story Arcs\n\n### Act I: Setup (roughly the first 20%)\n**Core goal**: [What this act must accomplish]\n**Starting state**: [Status quo]\n**Key scenes**:\n1. [Opening scene: specific description]\n2. [Inciting incident]\n3. [First turning point]\n**Emotional tone**: [e.g., calm to uneasy]\n**Foreshadowing planted**: [Seeds sown for later]\n\n### Act II-A: Rising Action (roughly 20%-50%)\n**Core goal**: [Expand adventure, deepen conflicts]\n**Key scenes**:\n1. [Major event 1]\n2. [Major event 2]\n3. [Major event 3]\n4. [Midpoint twist]\n**Character development**: [Growth in this section]\n**Emotional tone**: [Building tension]\n**Key mysteries/suspense**: [What keeps readers engaged]\n\n### Act II-B: Complications (roughly 50%-75%)\n**Core goal**: [Escalate crisis, dark moment]\n**Key scenes**:\n1. [Key event escalating crisis]\n2. [Betrayal/revelation/major failure]\n3. [All-is-lost moment]\n**Character development**: [Critical arc turning points]\n**Emotional tone**: [Hope to despair]\n**Foreshadowing revealed**: [Earlier seeds pay off]\n\n### Act III: Resolution (roughly the final 25%)\n**Core goal**: [Climax, resolve conflicts, complete arcs]\n**Key scenes**:\n1. [Rallying/gaining crucial power]\n2. [Final confrontation/climax]\n3. [Resolution: fates and closure]\n**Character development**: [Arcs completed]\n**Emotional tone**: [Desperation to catharsis]\n**Thematic payoff**: [Core theme crystallized]\n\n## Foreshadowing & Thread Network\n[5-8 narrative threads, noting which arc they appear in and resolve]\n\n## Themes & Symbolism\n[Deeper themes and recurring symbols/imagery]\n\n## Style Guide\n[Narrative POV, prose register, pacing, dialogue style]\n\nEnsure:\n1. Clear cause-and-effect escalation between arcs\n2. Believable character growth\n3. Interconnected foreshadowing web\n4. Enough detail to support {target_chapters} chapters\n5. Key scenes specific enough to guide prose generation\"\"\"\n\n    return system, prompt\n\n\ndef generate_plot_gemini(api_key, idea, num_chapters, lang):\n    from google import genai\n    from google.genai import types\n    client = genai.Client(api_key=api_key)\n    system, prompt = _build_plot_prompt(idea, num_chapters, lang)\n    response = client.models.generate_content(\n        model='gemini-3-pro-preview', contents=prompt,\n        config=types.GenerateContentConfig(system_instruction=system, temperature=0.9, max_output_tokens=8192),\n    )\n    return response.text\n\n\ndef generate_plot_gpt(api_key, idea, num_chapters, lang):\n    from openai import OpenAI\n    client = OpenAI(api_key=api_key)\n    system, prompt = _build_plot_prompt(idea, num_chapters, lang)\n    response = client.chat.completions.create(\n        model='gpt-4o', messages=[{'role': 'system', 'content': system}, {'role': 'user', 'content': prompt}],\n        temperature=0.9, max_tokens=8192,\n    )\n    return response.choices[0].message.content\n\n\ndef develop_plot_api(idea, num_chapters, provider, api_key):\n    if not idea.strip():\n        return 'Please enter a story idea first.'\n    if not api_key.strip():\n        return f'Please enter your {provider} API key above.'\n    lang = detect_language(idea)\n    try:\n        if provider == 'Gemini':\n            return generate_plot_gemini(api_key, idea, int(num_chapters), lang)\n        else:\n            return generate_plot_gpt(api_key, idea, int(num_chapters), lang)\n    except Exception as e:\n        return f'API Error ({provider}): {e}'\n\n\ndef call_cloud_api(system, prompt, provider, api_key, temperature=0.7, max_tokens=2048):\n    if provider == 'Gemini':\n        from google import genai\n        from google.genai import types\n        client = genai.Client(api_key=api_key)\n        response = client.models.generate_content(\n            model='gemini-3-pro-preview', contents=prompt,\n            config=types.GenerateContentConfig(\n                system_instruction=system, temperature=temperature, max_output_tokens=max_tokens),\n        )\n        return response.text\n    elif provider == 'GPT':\n        from openai import OpenAI\n        client = OpenAI(api_key=api_key)\n        response = client.chat.completions.create(\n            model='gpt-4o',\n            messages=[{'role': 'system', 'content': system}, {'role': 'user', 'content': prompt}],\n            temperature=temperature, max_tokens=max_tokens,\n        )\n        return response.choices[0].message.content\n    else:\n        raise ValueError(f'Unknown provider: {provider}')\n\n\ndef _parse_json_response(text):\n    m = re.search(r'```(?:json)?\\s*\\n(.*?)\\n```', text, re.DOTALL)\n    if m:\n        return json.loads(m.group(1))\n    cleaned = text.strip()\n    if cleaned.startswith('{'):\n        return json.loads(cleaned)\n    start = cleaned.find('{')\n    end = cleaned.rfind('}')\n    if start != -1 and end != -1:\n        return json.loads(cleaned[start:end + 1])\n    raise ValueError(f'Could not parse JSON from response: {text[:200]}...')\n\n\n# ---- Cloud text generation ----\ndef generate_text_cloud(prompt, system_prompt='', max_new_tokens=2048, temperature=0.8,\n                        provider='Gemini', api_key=''):\n    \"\"\"Generate text using cloud API instead of a local model.\"\"\"\n    if not api_key.strip():\n        return 'Please enter your API key in Settings above.'\n    if not system_prompt:\n        system_prompt = ZH_SYSTEM if detect_language(prompt) == 'zh' else EN_SYSTEM\n    try:\n        return call_cloud_api(system_prompt, prompt, provider, api_key,\n                              temperature=temperature, max_tokens=max_new_tokens)\n    except Exception as e:\n        return f'Cloud API Error ({provider}): {e}'\n\n\n# ---- Local model generation ----\ndef generate_text(prompt, system_prompt='', max_new_tokens=2048, temperature=0.8,\n                  top_p=0.9, top_k=50, repetition_penalty=1.0):\n    if model is None or tokenizer is None:\n        return 'No local model loaded. Switch Writer Mode to Cloud API or load a model first.'\n\n    if not system_prompt:\n        system_prompt = ZH_SYSTEM if detect_language(prompt) == 'zh' else EN_SYSTEM\n\n    import torch\n    messages = [\n        {'role': 'system', 'content': system_prompt},\n        {'role': 'user', 'content': prompt},\n    ]\n    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n    inputs = tokenizer(text, return_tensors='pt').to(model.device)\n\n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs, max_new_tokens=max_new_tokens,\n            temperature=max(temperature, 0.01), top_p=top_p, top_k=top_k,\n            repetition_penalty=repetition_penalty, do_sample=True,\n        )\n    new_tokens = outputs[0][inputs['input_ids'].shape[1]:]\n    return tokenizer.decode(new_tokens, skip_special_tokens=True)\n\n\n# ---- Story Tracker ----\nclass StoryTracker:\n    @staticmethod\n    def empty_bible():\n        return {'characters': {}, 'plot_threads': [], 'chapter_summaries': [], 'style_notes': ''}\n\n    @staticmethod\n    def initialize_from_outline(outline, provider, api_key, lang):\n        if lang == 'zh':\n            system = '你是一位小说编辑助手。请从小说大纲中提取结构化信息，以JSON格式输出。'\n            prompt = f'请分析以下小说大纲，提取关键信息并以JSON格式输出：\\n\\n{outline}\\n\\n输出格式（必须是合法JSON）：\\n```json\\n{{\"characters\": {{\"角色名\": {{\"description\": \"外貌和身份\", \"personality\": \"性格\", \"motivation\": \"动机\", \"relationships\": {{}}, \"location\": \"位置\", \"emotional_state\": \"状态\"}}}}, \"plot_threads\": [{{\"name\": \"线索\", \"status\": \"active\", \"description\": \"描述\"}}], \"style_notes\": \"风格\"}}\\n```'\n        else:\n            system = 'You are a fiction editor assistant. Extract structured information from the novel outline as JSON.'\n            prompt = f'Analyze the following novel outline and extract key information as JSON:\\n\\n{outline}\\n\\nOutput format (must be valid JSON):\\n```json\\n{{\"characters\": {{\"Name\": {{\"description\": \"role\", \"personality\": \"traits\", \"motivation\": \"motivation\", \"relationships\": {{}}, \"location\": \"location\", \"emotional_state\": \"state\"}}}}, \"plot_threads\": [{{\"name\": \"thread\", \"status\": \"active\", \"description\": \"desc\"}}], \"style_notes\": \"style\"}}\\n```'\n        try:\n            response = call_cloud_api(system, prompt, provider, api_key, temperature=0.3)\n            parsed = _parse_json_response(response)\n            bible = StoryTracker.empty_bible()\n            bible['characters'] = parsed.get('characters', {})\n            bible['plot_threads'] = parsed.get('plot_threads', [])\n            bible['style_notes'] = parsed.get('style_notes', '')\n            return bible\n        except Exception as e:\n            bible = StoryTracker.empty_bible()\n            bible['style_notes'] = f'(Bible init failed: {e})'\n            return bible\n\n    @staticmethod\n    def apply_updates(bible, updates):\n        bible = copy.deepcopy(bible)\n        for name, changes in updates.get('characters', {}).items():\n            if name in bible['characters']:\n                bible['characters'][name].update(changes)\n            else:\n                bible['characters'][name] = changes\n        existing_names = {t['name'] for t in bible['plot_threads']}\n        for thread in updates.get('plot_threads', []):\n            if thread['name'] in existing_names:\n                for i, t in enumerate(bible['plot_threads']):\n                    if t['name'] == thread['name']:\n                        bible['plot_threads'][i].update(thread)\n                        break\n            else:\n                bible['plot_threads'].append(thread)\n        if 'chapter_summary' in updates:\n            bible.setdefault('chapter_summaries', []).append(updates['chapter_summary'])\n        return bible\n\n    @staticmethod\n    def get_context_summary(bible, ch_num, lang):\n        if not bible or not bible.get('characters'):\n            return ''\n        parts = []\n        if lang == 'zh':\n            chars = []\n            for name, info in bible.get('characters', {}).items():\n                loc = info.get('location', '')\n                emo = info.get('emotional_state', '')\n                chars.append(f'{name}：{loc}，{emo}')\n            if chars:\n                parts.append('【人物状态】' + '；'.join(chars))\n            active = [t for t in bible.get('plot_threads', []) if t.get('status') == 'active']\n            if active:\n                parts.append(f\"【活跃线索】{'、'.join(t['name'] for t in active[:5])}\")\n            for s in bible.get('chapter_summaries', [])[-3:]:\n                parts.append(f\"【第{s.get('chapter', '?')}章】{s.get('summary', '')}\")\n        else:\n            chars = []\n            for name, info in bible.get('characters', {}).items():\n                loc = info.get('location', '')\n                emo = info.get('emotional_state', '')\n                chars.append(f'{name}: {loc}, {emo}')\n            if chars:\n                parts.append('[Characters] ' + '; '.join(chars))\n            active = [t for t in bible.get('plot_threads', []) if t.get('status') == 'active']\n            if active:\n                parts.append(f\"[Active threads] {', '.join(t['name'] for t in active[:5])}\")\n            for s in bible.get('chapter_summaries', [])[-3:]:\n                parts.append(f\"[Ch.{s.get('chapter', '?')}] {s.get('summary', '')}\")\n        return '\\n'.join(parts)\n\n\n# ---- Mastermind ----\nclass Mastermind:\n    @staticmethod\n    def _extract_chapter_outline(outline, ch_num):\n        \"\"\"Extract relevant context from the outline for a given chapter.\n        If chapter-specific sections exist, extract that section.\n        Otherwise (arc-based outline), return the full outline.\"\"\"\n        pattern = rf'(?:###?\\s*(?:第{ch_num}章|Chapter\\s*{ch_num})[：:\\s]*)(.+?)(?=(?:###?\\s*(?:第\\d+章|Chapter\\s*\\d+))|$)'\n        match = re.search(pattern, outline, re.DOTALL)\n        if match:\n            return match.group(0).strip()\n        # Arc-based outline — return full outline (cloud API will interpret it)\n        if len(outline) > 12000:\n            return outline[:12000] + '\\n\\n[...outline truncated...]'\n        return outline\n\n    @staticmethod\n    def plan_chapter(outline, ch_num, bible, prev_ending, provider, api_key, lang):\n        chapter_outline = Mastermind._extract_chapter_outline(outline, ch_num)\n        bible_summary = StoryTracker.get_context_summary(bible, ch_num, lang)\n        if lang == 'zh':\n            system = (\n                '你是一位资深小说编辑和连载小说策划。你的任务是根据故事总大纲和已有进展，'\n                '为下一章制定详细的写作计划，并生成散文体的场景引子供AI写手使用。\\n'\n                '关键要求：\\n'\n                '1. 场景引子必须是纯散文，不能包含标题、列表或Markdown格式\\n'\n                '2. 必须确保与前文的连续性——人物位置、情感状态、已发生的事件\\n'\n                '3. 根据章节在全书中的位置，从大纲弧线中选择合适的情节推进\\n'\n                '4. 维持故事节奏——不要跳跃太快，也不要原地踏步'\n            )\n            prompt = f\"\"\"请为第{ch_num}章创建写作计划。\n\n【故事大纲】\n{chapter_outline}\n\n【故事圣经——人物状态和剧情进展】\n{bible_summary if bible_summary else \"（首章，无历史记录）\"}\n\n【上一章结尾】\n{prev_ending[-800:] if prev_ending else \"（首章——从故事的起点开始）\"}\n\n【任务】\n这是全书的第{ch_num}章。请根据大纲中的故事弧线，判断当前应该处于哪个阶段，\n然后为本章制定具体的写作计划。确保：\n- 与上一章的结尾自然衔接\n- 人物的言行与故事圣经中记录的状态一致\n- 情节推进符合大纲中对应弧线的方向\n- 每章有明确的情节推进，不要重复之前已写过的内容\n\n请输出JSON格式（必须合法JSON）：\n```json\n{{\"scene_primer\": \"用散文体写一段场景引子（300-500字），描述本章开场的具体场景——时间、地点、环境细节、在场人物的状态和动作。必须与上一章结尾自然衔接。必须是纯叙事散文。\", \"key_events\": [\"事件1\", \"事件2\", \"事件3\"], \"guidance\": \"本章的情感基调、节奏建议、需要重点刻画的人物互动、应避免的情节方向\"}}\n```\"\"\"\n        else:\n            system = (\n                'You are a senior fiction editor and serial novel planner. Your task is to take '\n                'the story\\'s master outline and existing progress, then create a detailed writing '\n                'plan for the next chapter with a prose scene primer for the AI writer.\\n'\n                'Key requirements:\\n'\n                '1. The scene primer MUST be pure prose — no headings, bullet points, or markdown\\n'\n                '2. Ensure continuity with previous chapters — character locations, emotional states, events\\n'\n                '3. Based on the chapter\\'s position in the overall story, select appropriate plot progression from the outline arcs\\n'\n                '4. Maintain story pacing — don\\'t jump too fast or stall'\n            )\n            prompt = f\"\"\"Create a writing plan for Chapter {ch_num}.\n\n[Story outline]\n{chapter_outline}\n\n[Story bible — character states and plot progress]\n{bible_summary if bible_summary else \"(First chapter, no history)\"}\n\n[End of previous chapter]\n{prev_ending[-800:] if prev_ending else \"(First chapter — start from the story's beginning)\"}\n\n[Task]\nThis is Chapter {ch_num} of the full novel. Based on the story arcs in the outline,\ndetermine what stage the story should be at, then create a specific writing plan.\nEnsure:\n- Natural continuation from the previous chapter's ending\n- Character behavior consistent with the story bible's recorded states\n- Plot progression aligned with the corresponding arc in the outline\n- Clear plot advancement — do NOT repeat content already written\n\nOutput as JSON (must be valid JSON):\n```json\n{{\"scene_primer\": \"Write a prose scene primer (200-400 words) describing the chapter's opening scene. Must naturally follow from the previous chapter. Must be pure narrative prose.\", \"key_events\": [\"Event 1\", \"Event 2\", \"Event 3\"], \"guidance\": \"Emotional tone, pacing, character interactions to emphasize, plot directions to avoid\"}}\n```\"\"\"\n        try:\n            response = call_cloud_api(system, prompt, provider, api_key, temperature=0.7)\n            plan = _parse_json_response(response)\n            plan.setdefault('scene_primer', chapter_outline[:500])\n            plan.setdefault('key_events', [])\n            plan.setdefault('guidance', '')\n            return plan\n        except Exception as e:\n            return {'scene_primer': chapter_outline[:500], 'key_events': [], 'guidance': f'(Planning failed: {e})'}\n\n    @staticmethod\n    def review_and_update(plan, text, bible, ch_num, provider, api_key, lang):\n        key_events = ', '.join(plan.get('key_events', []))\n        bible_summary = StoryTracker.get_context_summary(bible, ch_num, lang)\n        if lang == 'zh':\n            system = '你同时扮演两个角色：1）小说编辑——审核章节质量；2）故事记录员——更新故事圣经。请以JSON格式输出。'\n            prompt = f'请审核以下生成的第{ch_num}章，并更新故事圣经。\\n\\n【写作计划的关键事件】\\n{key_events}\\n\\n【生成的章节文本】（前2000字）\\n{text[:3000]}\\n\\n【当前故事圣经】\\n{bible_summary if bible_summary else \"（空）\"}\\n\\n请输出JSON格式：\\n```json\\n{{\"review\": {{\"approved\": true, \"scores\": {{\"plot_adherence\": 8, \"prose_quality\": 7, \"character_consistency\": 8, \"engagement\": 7}}, \"issues\": [], \"regen_guidance\": \"\"}}, \"bible_updates\": {{\"characters\": {{}}, \"plot_threads\": [], \"chapter_summary\": {{\"chapter\": {ch_num}, \"summary\": \"概要\"}}}}}}\\n```'\n        else:\n            system = 'You play two roles: 1) Fiction editor — review chapter quality; 2) Story recorder — update the story bible. Output as JSON.'\n            prompt = f'Review Chapter {ch_num} and update the story bible.\\n\\n[Key events]\\n{key_events}\\n\\n[Generated text] (first 2000 words)\\n{text[:3000]}\\n\\n[Current bible]\\n{bible_summary if bible_summary else \"(empty)\"}\\n\\nOutput as JSON:\\n```json\\n{{\"review\": {{\"approved\": true, \"scores\": {{\"plot_adherence\": 8, \"prose_quality\": 7, \"character_consistency\": 8, \"engagement\": 7}}, \"issues\": [], \"regen_guidance\": \"\"}}, \"bible_updates\": {{\"characters\": {{}}, \"plot_threads\": [], \"chapter_summary\": {{\"chapter\": {ch_num}, \"summary\": \"summary\"}}}}}}\\n```'\n        try:\n            response = call_cloud_api(system, prompt, provider, api_key, temperature=0.3)\n            result = _parse_json_response(response)\n            result.setdefault('review', {'approved': True, 'scores': {}, 'issues': [], 'regen_guidance': ''})\n            result.setdefault('bible_updates', {})\n            return result\n        except Exception as e:\n            return {\n                'review': {'approved': True, 'scores': {}, 'issues': [f'Review failed: {e}'], 'regen_guidance': ''},\n                'bible_updates': {'chapter_summary': {'chapter': ch_num, 'summary': text[:100] + '...'}},\n            }\n\n\n# ---- Prompt Builder ----\nclass PromptBuilder:\n    @staticmethod\n    def build_chapter_prompt(instruction, scene_primer, prev_text, max_new_tokens, lang):\n        system = ZH_SYSTEM if lang == 'zh' else EN_SYSTEM\n        system_tokens = count_tokens(system)\n        instruction_tokens = count_tokens(instruction)\n        template_overhead = 15\n        # Sanitize primer: strip any markdown the cloud API may have included\n        scene_primer = re.sub(r'^#+\\s.*$', '', scene_primer, flags=re.MULTILINE)\n        scene_primer = re.sub(r'^\\s*[-*]\\s+', '', scene_primer, flags=re.MULTILINE)\n        scene_primer = re.sub(r'\\*\\*([^*]+)\\*\\*', r'\\1', scene_primer)\n        scene_primer = scene_primer.strip()\n        available = MAX_SEQ_LENGTH - system_tokens - instruction_tokens - template_overhead - max_new_tokens\n        available = max(available, 100)\n        primer_budget = min(count_tokens(scene_primer), available // 3)\n        context_budget = available - primer_budget\n        trimmed_primer = trim_to_token_budget(scene_primer, primer_budget, keep_end=False)\n        trimmed_context = trim_to_token_budget(prev_text, context_budget, keep_end=True)\n        parts = [instruction]\n        if trimmed_primer:\n            parts.append(trimmed_primer)\n        if trimmed_context:\n            parts.append(trimmed_context)\n        user_content = '\\n\\n'.join(parts)\n        user_tokens = count_tokens(user_content)\n        total = system_tokens + user_tokens + template_overhead + max_new_tokens\n        return system, user_content, {\n            'system_tokens': system_tokens, 'user_tokens': user_tokens,\n            'max_new_tokens': max_new_tokens, 'total_estimated': total,\n            'within_budget': total <= MAX_SEQ_LENGTH,\n        }\n\n\n# ---- Multi-pass generation ----\ndef generate_chapter_multipass(scene_primer, prev_text, num_passes, tokens_per_pass,\n                                lang, temperature, top_p,\n                                use_cloud=False, cloud_provider='', cloud_api_key=''):\n    instructions = _ZH_INSTRUCTIONS if lang == 'zh' else _EN_INSTRUCTIONS\n    builder = PromptBuilder()\n    accumulated = ''\n    pass_log = []\n    for i in range(num_passes):\n        instruction = random.choice(instructions)\n        if i == 0:\n            context = prev_text[-1200:] if prev_text else ''\n            primer = scene_primer\n        else:\n            context = accumulated[-1000:]\n            primer = ''\n        system, user_content, token_info = builder.build_chapter_prompt(\n            instruction, primer, context, tokens_per_pass, lang)\n        if use_cloud:\n            result = generate_text_cloud(user_content, system_prompt=system, max_new_tokens=tokens_per_pass,\n                                         temperature=temperature, provider=cloud_provider, api_key=cloud_api_key)\n        else:\n            result = generate_text(user_content, system_prompt=system, max_new_tokens=tokens_per_pass,\n                                   temperature=temperature, top_p=top_p, repetition_penalty=1.0)\n        accumulated += result\n        mode_label = 'cloud' if use_cloud else 'local'\n        pass_log.append(f'Pass {i+1}/{num_passes} ({mode_label}): +{len(result)} chars ({token_info[\"total_estimated\"]}/{MAX_SEQ_LENGTH} tokens)')\n    return accumulated, pass_log\n\n\n# ---- Legacy chapter generation ----\ndef generate_chapter_legacy(plot_outline, chapter_num, previous_text, style_notes,\n                            max_tokens, temperature, top_p, rep_penalty):\n    lang = detect_language(plot_outline)\n    ch = int(chapter_num)\n    pattern = rf'(?:###?\\s*(?:第{ch}章|Chapter\\s*{ch})[：:\\s]*)(.+?)(?=(?:###?\\s*(?:第\\d+章|Chapter\\s*\\d+))|$)'\n    match = re.search(pattern, plot_outline, re.DOTALL)\n    chapter_outline = match.group(0).strip() if match else f'Chapter {ch}'\n    if lang == 'zh':\n        prompt = f'## 小说大纲\\n{plot_outline}\\n\\n'\n        if previous_text:\n            ctx = previous_text[-2000:] if len(previous_text) > 2000 else previous_text\n            prompt += f'## 上一章结尾\\n{ctx}\\n\\n'\n        prompt += f'## 当前任务\\n请根据以上大纲，撰写第{ch}章的完整内容。\\n本章大纲：{chapter_outline}\\n\\n'\n        if style_notes:\n            prompt += f'风格要求：{style_notes}\\n\\n'\n        prompt += f'要求：\\n1. 以具体的场景描写开头\\n2. 通过对话和动作推动情节\\n3. 注意人物性格的一致性\\n4. 章节结尾要有悬念或转折\\n5. 写出完整的章节内容\\n\\n第{ch}章正文：\\n'\n    else:\n        prompt = f'## Novel Outline\\n{plot_outline}\\n\\n'\n        if previous_text:\n            ctx = previous_text[-2000:] if len(previous_text) > 2000 else previous_text\n            prompt += f'## End of Previous Chapter\\n{ctx}\\n\\n'\n        prompt += f'## Current Task\\nWrite the complete text of Chapter {ch}.\\nChapter outline: {chapter_outline}\\n\\n'\n        if style_notes:\n            prompt += f'Style notes: {style_notes}\\n\\n'\n        prompt += f'Requirements:\\n1. Open with vivid scene-setting\\n2. Drive the plot through dialogue and action\\n3. Maintain consistent characterization\\n4. End with a hook or turning point\\n5. Write the complete chapter\\n\\nChapter {ch}:\\n'\n    system = ZH_SYSTEM if lang == 'zh' else EN_SYSTEM\n    return generate_text(prompt, system_prompt=system, max_new_tokens=int(max_tokens),\n                         temperature=temperature, top_p=top_p, repetition_penalty=rep_penalty)\n\n\n# ---- Full pipeline ----\ndef generate_chapter_pipeline(outline, ch_num, chapters_state, style_notes,\n                              max_tokens, temperature, top_p, rep_penalty,\n                              enable_agents, num_passes, provider, api_key, bible_state,\n                              writer_mode='Local Model'):\n    use_cloud = writer_mode == 'Cloud API'\n    ch_num = int(ch_num)\n    lang = detect_language(outline)\n\n    if not use_cloud and model is None:\n        msg = \"Please load a local model first, or switch Writer Mode to 'Cloud API'.\"\n        return msg, chapters_state, bible_state, msg, ''\n\n    if use_cloud and not api_key.strip():\n        msg = 'Cloud writer mode requires an API key. Please enter it in API Settings.'\n        return msg, chapters_state, bible_state, msg, ''\n\n    # Legacy mode (local only, no agents)\n    if not enable_agents:\n        if use_cloud:\n            msg = 'Legacy mode requires a local model. Please enable agents for cloud-only writing.'\n            return msg, chapters_state, bible_state, msg, ''\n        result = generate_chapter_legacy(outline, ch_num, chapters_state, style_notes,\n                                         max_tokens, temperature, top_p, rep_penalty)\n        sep = f'\\n\\n{\"=\"*40}\\n第{ch_num}章 / Chapter {ch_num}\\n{\"=\"*40}\\n\\n'\n        new_acc = chapters_state + sep + result if chapters_state else result\n        return result, new_acc, bible_state, 'Legacy mode (agents disabled).', ''\n\n    # Agents require API key\n    if not api_key.strip():\n        msg = 'Agent mode requires an API key. Please enter it in API Settings.'\n        return msg, chapters_state, bible_state, msg, ''\n\n    # Agent mode\n    log_lines = []\n    def log(msg):\n        log_lines.append(f'[{time.strftime(\"%H:%M:%S\")}] {msg}')\n\n    mode_label = 'cloud' if use_cloud else 'local'\n    log(f'Writer mode: {mode_label}')\n\n    try:\n        if not bible_state or not bible_state.get('characters'):\n            log('Initializing story bible from outline...')\n            bible_state = StoryTracker.initialize_from_outline(outline, provider, api_key, lang)\n            log(f'Bible initialized: {len(bible_state.get(\"characters\", {}))} characters')\n\n        log(f'Mastermind planning chapter {ch_num}...')\n        prev_ending = chapters_state[-1200:] if chapters_state else ''\n        plan = Mastermind.plan_chapter(outline, ch_num, bible_state, prev_ending, provider, api_key, lang)\n        log(f'Scene primer: {plan[\"scene_primer\"][:150].replace(chr(10), \" \")}...')\n        if plan['key_events']:\n            log(f'Key events: {\", \".join(plan[\"key_events\"][:3])}')\n\n        tokens_per_pass = min(int(max_tokens) // int(num_passes), 1500)\n        if tokens_per_pass < 512:\n            log(f'Warning: tokens_per_pass ({tokens_per_pass}) below minimum 512, clamping up')\n            tokens_per_pass = 512\n        log(f'Generating chapter ({int(num_passes)} passes, {tokens_per_pass} tokens/pass, {mode_label})...')\n\n        try:\n            result, pass_log = generate_chapter_multipass(\n                plan['scene_primer'], prev_ending, int(num_passes), tokens_per_pass, lang, temperature, top_p,\n                use_cloud=use_cloud, cloud_provider=provider, cloud_api_key=api_key)\n        except Exception as e:\n            log(f'ERROR in chapter generation: {e}')\n            error_msg = f'Chapter generation failed: {e}\\nCheck API key and provider settings.'\n            return error_msg, chapters_state, bible_state, '\\n'.join(log_lines), ''\n\n        for pl in pass_log:\n            log(pl)\n        log(f'Total generated: {len(result)} chars')\n\n        if not result.strip():\n            log('WARNING: Generation produced empty text')\n            return 'Generation produced empty text. Try again or check your API settings.', \\\n                chapters_state, bible_state, '\\n'.join(log_lines), ''\n\n        log('Reviewing chapter + updating story bible...')\n        review_result = Mastermind.review_and_update(plan, result, bible_state, ch_num, provider, api_key, lang)\n        review = review_result.get('review', {})\n        bible_updates = review_result.get('bible_updates', {})\n        bible_state = StoryTracker.apply_updates(bible_state, bible_updates)\n\n        approved = review.get('approved', True)\n        scores = review.get('scores', {})\n        issues = review.get('issues', [])\n        status = 'APPROVED' if approved else 'NEEDS REVISION'\n        log(f'Review: {status}')\n        if scores:\n            log(f'Scores: {\", \".join(f\"{k}: {v}/10\" for k, v in scores.items())}')\n\n        review_text = f'Status: {status}\\n'\n        if scores:\n            review_text += 'Scores:\\n' + ''.join(f'  {k}: {v}/10\\n' for k, v in scores.items())\n        if issues:\n            review_text += 'Issues:\\n' + ''.join(f'  - {issue}\\n' for issue in issues)\n        if not approved and review.get('regen_guidance'):\n            review_text += f'\\nRevision guidance: {review[\"regen_guidance\"]}\\n'\n\n        sep = f'\\n\\n{\"=\"*40}\\n第{ch_num}章 / Chapter {ch_num}\\n{\"=\"*40}\\n\\n'\n        new_acc = chapters_state + sep + result if chapters_state else result\n        return result, new_acc, bible_state, '\\n'.join(log_lines), review_text\n\n    except Exception as e:\n        log(f'PIPELINE ERROR: {e}')\n        error_msg = f'Pipeline error: {e}\\nSee Generation Log for details.'\n        return error_msg, chapters_state, bible_state, '\\n'.join(log_lines), ''\n\n\ndef continue_writing(existing_text, instruction, max_tokens, temperature, top_p, rep_penalty,\n                     writer_mode='Local Model', api_provider='Gemini', api_key=''):\n    use_cloud = writer_mode == 'Cloud API'\n    if not use_cloud and model is None:\n        return \"Please load a local model first, or switch Writer Mode to 'Cloud API'.\"\n    if use_cloud and not api_key.strip():\n        return 'Cloud writer mode requires an API key. Please enter it in API Settings.'\n\n    lang = detect_language(existing_text)\n    if not instruction:\n        instruction = '续写这段叙事，保持原文的风格和节奏。' if lang == 'zh' else 'Continue the narrative in the established style.'\n    prompt = instruction + '\\n\\n' + existing_text\n    system = ZH_SYSTEM if lang == 'zh' else EN_SYSTEM\n\n    if use_cloud:\n        return generate_text_cloud(prompt, system_prompt=system, max_new_tokens=int(max_tokens),\n                                   temperature=temperature, provider=api_provider, api_key=api_key)\n    else:\n        return generate_text(prompt, system_prompt=system, max_new_tokens=int(max_tokens),\n                             temperature=temperature, top_p=top_p, repetition_penalty=rep_penalty)\n\n\n# ---- Determine default writer mode from config ----\n_default_writer_mode = 'Cloud API' if WRITER_MODE == 'cloud_api' else 'Local Model'\n\n# ---- Build Gradio UI ----\nwith gr.Blocks(title='Novel Writer Studio') as app:\n    gr.Markdown('# Novel Writer Studio (Colab)')\n    gr.Markdown(\n        '*Cloud AI develops your plot (as high-level story arcs) + orchestrates agents. '\n        'Chapters can be written by your fine-tuned local model OR entirely via cloud API (no GPU needed).*'\n    )\n\n    # Shared state\n    all_chapters = gr.State('')\n    bible_state = gr.State({})\n\n    # API Settings\n    with gr.Accordion('API Settings', open=True):\n        gr.Markdown('Enter your API key. Used for plot generation, agent orchestration, and cloud writing.')\n        with gr.Row():\n            api_provider = gr.Radio(['Gemini', 'GPT'], value='Gemini', label='Provider')\n            api_key_input = gr.Textbox(label='API Key', type='password', placeholder='Paste your API key...', scale=3)\n\n    # Writer Mode\n    with gr.Accordion('Writer Mode', open=True):\n        gr.Markdown(\n            '**Local Model**: Uses your fine-tuned LoRA model (requires GPU). Best quality for style-specific prose.\\n\\n'\n            '**Cloud API**: Uses Gemini/GPT for chapter writing too. No GPU needed. '\n            'Loses LoRA style but keeps all agent features (planning, review, story bible).'\n        )\n        writer_mode = gr.Radio(\n            ['Local Model', 'Cloud API'], value=_default_writer_mode,\n            label='Chapter Writer', info='Choose what writes the chapter prose')\n\n    # Agent Settings\n    with gr.Accordion('Agent Settings', open=False):\n        gr.Markdown(\n            '**Multi-Agent System**: Mastermind + Story Tracker orchestrate chapter generation.\\n'\n            '- **Mastermind**: Converts outline to prose scene primers, reviews output\\n'\n            '- **Story Tracker**: Maintains character states, plot threads, summaries\\n'\n            '- Uses 2-3 cloud API calls per chapter'\n        )\n        with gr.Row():\n            enable_agents = gr.Checkbox(value=True, label='Enable Agents',\n                                        info='Use Mastermind + Story Tracker')\n            num_passes = gr.Slider(1, 5, value=3, step=1, label='Generation Passes',\n                                   info='More passes = longer chapters (~1000-1500 chars each)')\n\n    # Generation settings\n    with gr.Accordion('Chapter Generation Settings', open=False):\n        with gr.Row():\n            max_tokens_slider = gr.Slider(128, 4096, value=2048, step=128, label='Max New Tokens')\n            temperature_slider = gr.Slider(0.1, 2.0, value=0.8, step=0.05, label='Temperature')\n        with gr.Row():\n            top_p_slider = gr.Slider(0.1, 1.0, value=0.9, step=0.05, label='Top-P')\n            rep_penalty_slider = gr.Slider(1.0, 2.0, value=1.0, step=0.05, label='Repetition Penalty',\n                                           info='1.0 recommended (matches training)')\n\n    with gr.Tabs():\n        # Tab 1: Story Workshop\n        with gr.Tab('Story Workshop'):\n            gr.Markdown('### Step 1: Your Story Idea')\n            with gr.Row():\n                with gr.Column(scale=3):\n                    story_idea = gr.Textbox(\n                        label='Story Idea', lines=4,\n                        placeholder='例：在一个武林高手辈出的时代，一个失忆的少年在雪山醒来...\\n\\nOr: In a world where magic is fueled by music...',\n                    )\n                with gr.Column(scale=1):\n                    num_chapters = gr.Slider(3, 200, value=30, step=1, label='Target Chapters',\n                                            info='Approximate story length — outline uses arcs, not individual chapters')\n                    develop_btn = gr.Button('Develop Plot (Cloud AI)', variant='primary', size='lg')\n\n            gr.Markdown('### Step 2: Plot Outline')\n            gr.Markdown(\n                'Generated by Gemini/GPT as a high-level story arc structure (not chapter-by-chapter). '\n                'Review and edit freely — add detail, change plot points, extend arcs. '\n                'The Mastermind agent will plan each chapter from this outline.'\n            )\n            plot_outline = gr.Textbox(label='Plot Outline (editable)', lines=25,\n                                     placeholder='Click Develop Plot to generate...')\n            develop_btn.click(develop_plot_api, [story_idea, num_chapters, api_provider, api_key_input], plot_outline)\n\n            gr.Markdown('---')\n            gr.Markdown('### Step 3: Generate Chapters')\n            gr.Markdown(\n                'Writes each chapter using your chosen Writer Mode. Enable agents for multi-pass generation '\n                'with plot tracking and review. The Mastermind plans each chapter from your outline arcs.'\n            )\n            with gr.Row():\n                with gr.Column(scale=1):\n                    chapter_num = gr.Slider(1, 200, value=1, step=1, label='Chapter Number')\n                    style_notes = gr.Textbox(label='Style Notes (optional)', lines=2, placeholder='e.g., 多用对话')\n                    gen_chapter_btn = gr.Button('Generate Chapter', variant='primary', size='lg')\n                with gr.Column(scale=3):\n                    chapter_output = gr.Textbox(label='Generated Chapter', lines=25)\n\n            with gr.Accordion('Generation Log', open=False):\n                gen_log_display = gr.Textbox(label='Agent Activity Log', lines=12, interactive=False)\n\n            with gr.Accordion('Chapter Review', open=False):\n                review_display = gr.Textbox(label='Mastermind Review', lines=8, interactive=False)\n\n            gen_chapter_btn.click(\n                generate_chapter_pipeline,\n                [plot_outline, chapter_num, all_chapters, style_notes,\n                 max_tokens_slider, temperature_slider, top_p_slider, rep_penalty_slider,\n                 enable_agents, num_passes, api_provider, api_key_input, bible_state,\n                 writer_mode],\n                [chapter_output, all_chapters, bible_state, gen_log_display, review_display],\n            )\n\n            with gr.Accordion('All Generated Chapters', open=False):\n                all_chapters_display = gr.Textbox(label='Full Story So Far', lines=30, interactive=False)\n                refresh_btn = gr.Button('Refresh')\n                refresh_btn.click(lambda x: x, all_chapters, all_chapters_display)\n\n                export_btn = gr.Button('Export Story to File')\n                export_file = gr.File(label='Download')\n\n                def export_story(text):\n                    if not text:\n                        return None\n                    fpath = f'/content/story_{time.strftime(\"%Y%m%d_%H%M%S\")}.txt'\n                    Path(fpath).write_text(text, encoding='utf-8')\n                    return fpath\n\n                export_btn.click(export_story, all_chapters, export_file)\n\n        # Tab 2: Story Bible\n        with gr.Tab('Story Bible'):\n            gr.Markdown('### Story Bible\\nTracks character states, plot threads, and chapter summaries.')\n            bible_display = gr.JSON(label='Current Story Bible')\n            with gr.Row():\n                refresh_bible_btn = gr.Button('Refresh Display')\n                init_bible_btn = gr.Button('Initialize Bible from Outline', variant='primary')\n                clear_bible_btn = gr.Button('Clear Bible', variant='stop')\n\n            bible_manual_notes = gr.Textbox(\n                label='Manual Notes (added to style_notes)', lines=3,\n                placeholder=\"Add your own notes (e.g., 'the sword is named Frostbite')\")\n            add_notes_btn = gr.Button('Add Notes to Bible')\n            bible_status = gr.Textbox(label='Status', interactive=False, lines=1)\n\n            def refresh_bible(bible):\n                return bible\n\n            def init_bible_from_outline(outline, provider, key, bible):\n                if not outline.strip():\n                    return bible, bible, 'No outline to initialize from.'\n                if not key.strip():\n                    return bible, bible, f'Please enter your {provider} API key.'\n                lang = detect_language(outline)\n                new_bible = StoryTracker.initialize_from_outline(outline, provider, key, lang)\n                n_chars = len(new_bible.get('characters', {}))\n                return new_bible, new_bible, f'Bible initialized: {n_chars} characters found.'\n\n            def clear_bible():\n                return StoryTracker.empty_bible()\n\n            def add_manual_notes(bible, notes):\n                if not notes.strip():\n                    return bible\n                bible = copy.deepcopy(bible)\n                existing = bible.get('style_notes', '')\n                bible['style_notes'] = (existing + '\\n' + notes).strip() if existing else notes\n                return bible\n\n            refresh_bible_btn.click(refresh_bible, bible_state, bible_display)\n            init_bible_btn.click(init_bible_from_outline,\n                                 [plot_outline, api_provider, api_key_input, bible_state],\n                                 [bible_state, bible_display, bible_status])\n            clear_bible_btn.click(clear_bible, outputs=[bible_state])\n            add_notes_btn.click(add_manual_notes, [bible_state, bible_manual_notes], bible_state)\n\n        # Tab 3: Free Write\n        with gr.Tab('Free Write'):\n            gr.Markdown('### Direct Generation')\n            gr.Markdown('Write freely — uses your chosen Writer Mode.')\n            with gr.Row():\n                with gr.Column():\n                    free_context = gr.Textbox(label='Context / Previous Text', lines=10,\n                                             placeholder='Paste existing text for continuation...')\n                    free_instruction = gr.Textbox(label='Instruction (optional)', lines=2,\n                                                 placeholder='e.g., 续写这段战斗场景')\n                    free_gen_btn = gr.Button('Generate', variant='primary', size='lg')\n                with gr.Column():\n                    free_output = gr.Textbox(label='Generated Text', lines=20)\n                    append_btn = gr.Button('Append to Context')\n\n            free_gen_btn.click(\n                continue_writing,\n                [free_context, free_instruction, max_tokens_slider, temperature_slider, top_p_slider, rep_penalty_slider,\n                 writer_mode, api_provider, api_key_input],\n                free_output,\n            )\n            append_btn.click(lambda ctx, out: ctx + '\\n' + out if ctx else out, [free_context, free_output], free_context)\n\n        # Tab 4: Quick Test\n        with gr.Tab('Quick Test'):\n            gr.Markdown('### Test with a single prompt')\n            gr.Markdown('Uses your chosen Writer Mode.')\n            test_prompt = gr.Textbox(label='Prompt', lines=4,\n                                    placeholder='月色如霜，照在悬崖边两道对峙的身影上...')\n            test_btn = gr.Button('Generate', variant='primary')\n            test_output = gr.Textbox(label='Output', lines=15)\n\n            def run_test(prompt, max_t, temp, tp, rp, wmode, provider, key):\n                if wmode == 'Cloud API':\n                    return generate_text_cloud(prompt, max_new_tokens=int(max_t), temperature=temp,\n                                               provider=provider, api_key=key)\n                return generate_text(prompt, max_new_tokens=int(max_t), temperature=temp, top_p=tp, repetition_penalty=rp)\n\n            test_btn.click(run_test,\n                           [test_prompt, max_tokens_slider, temperature_slider, top_p_slider, rep_penalty_slider,\n                            writer_mode, api_provider, api_key_input],\n                           test_output)\n\nprint('Launching Web UI...')\napp.launch(share=True)",
   "execution_count": null,
   "outputs": []
  }
 ]
}